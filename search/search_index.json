{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 When designing and building APIs, there are a lot of things to consider. Among these are security , media types , documentation , and versioning . These, and a lot of other concerns, are what we address here, in order to offer comprehensible guidelines on how to design APIs. Motivation \u00b6 The motivation behind the API Guidelines is to define standards or \"best practices\" in order to: design robust and long-lasting APIs of high quality design APIs that are easy to understand and use have a consistent API \"look and feel\" across the board facilitate APIs as a Product About These Guidelines \u00b6 These guidelines are a work in progress, and is maintained by the API Enablement Team Now defunct - who then? during their initial work. Later teams are responsible for adhering to these guidelines during API design and development, and to continue to evolve these guidelines. What should we do when the guideline changes? Do existing APIs have to be changed as well? Conventions Used in These Guidelines \u00b6 In these guidelines when referring to an API, we implicitly mean a web API delivered over HTTP. The terms: API, web API, and public API may be used interchangeably throughout these guidelines. The following typographical conventions are used in these guidelines: Constant width program listings, as well as within paragraphs to refer to URLs, mime types, HTTP methods, HTTP headers, and HTTP status codes. Each section will usually be comprised of one or more lists of recommendations: DO to signify that you should do this. CONSIDER to signify that you should consider doing this. AVOID to signify that you should avoid doing this. DO NOT to signify that you should not do this. Should we consider using the requirement level keywords \"MUST\", \"SHOULD\", etc., from RFC 2119 instead? Admonitions \u00b6 Note these sections hold additional information Tip these sections hold tips Warning these sections hold cautions Larger blocks of code and examples curl https://example.org/api/resource | jq","title":"Introduction"},{"location":"#introduction","text":"When designing and building APIs, there are a lot of things to consider. Among these are security , media types , documentation , and versioning . These, and a lot of other concerns, are what we address here, in order to offer comprehensible guidelines on how to design APIs.","title":"Introduction"},{"location":"#motivation","text":"The motivation behind the API Guidelines is to define standards or \"best practices\" in order to: design robust and long-lasting APIs of high quality design APIs that are easy to understand and use have a consistent API \"look and feel\" across the board facilitate APIs as a Product","title":"Motivation"},{"location":"#about-these-guidelines","text":"These guidelines are a work in progress, and is maintained by the API Enablement Team Now defunct - who then? during their initial work. Later teams are responsible for adhering to these guidelines during API design and development, and to continue to evolve these guidelines. What should we do when the guideline changes? Do existing APIs have to be changed as well?","title":"About These Guidelines"},{"location":"#conventions-used-in-these-guidelines","text":"In these guidelines when referring to an API, we implicitly mean a web API delivered over HTTP. The terms: API, web API, and public API may be used interchangeably throughout these guidelines. The following typographical conventions are used in these guidelines: Constant width program listings, as well as within paragraphs to refer to URLs, mime types, HTTP methods, HTTP headers, and HTTP status codes. Each section will usually be comprised of one or more lists of recommendations: DO to signify that you should do this. CONSIDER to signify that you should consider doing this. AVOID to signify that you should avoid doing this. DO NOT to signify that you should not do this. Should we consider using the requirement level keywords \"MUST\", \"SHOULD\", etc., from RFC 2119 instead?","title":"Conventions Used in These Guidelines"},{"location":"#admonitions","text":"Note these sections hold additional information Tip these sections hold tips Warning these sections hold cautions Larger blocks of code and examples curl https://example.org/api/resource | jq","title":"Admonitions"},{"location":"api-design/","text":"API Design \u00b6 Building Services API First \u00b6 API first means that what you are building is an API to be consumed by client applications and services. Built into every decision you make and every line of code you write is the notion that every functional requirement of your application will be met through the consumption of an API. Even a user interface, be it web or mobile, is really nothing more than a consumer of an API. By designing your API first, you are able to facilitate discussion with your stakeholders (your internal team, customers, or possibly other teams within your organization who want to consume your API) well before you might have coded yourself past the point of no return. This collaboration then allows you to build user stories, mock your API, and generate documentation that can be used to further socialize the intent and functionality of the service you\u2019re building. All of this can be done to vet (and test!) your direction and plans without investing too much in the plumbing that supports a given API. These days, you\u2019ll find that there are myriad tools and standards to support API-first development. There is a standard format for API specification that uses a markdown-like syntax called API Blueprint. This format is far more human readable than JSON (or WSDL, a relic that belongs in a museum) and can be used by code to generate documentation and even server mocks, which are invaluable in testing service ecosystems. Tool suites like Apiary provide things like GitHub integration and server mocks. If someone wants to build a client to your API, all you have to do is give her a link to your application on Apiary, where she can read your API Blueprint, see example code for consuming your service, and even execute requests against a running server mock. In other words, there is absolutely no excuse for claiming that API first is a difficult or unsupported path. This is a pattern that can be applied to noncloud software development, but it is particularly well suited to cloud development in its ability to allow rapid prototyping, support a services ecosystem, and facilitate the automated deployment testing and continuous delivery pipelines that are some of the hallmarks of modern { cloud-native/Kubernetes/\"cloud-friendly\" } application development. This pattern is an extension of the contract-first development pattern, where developers concentrate on building the edges or seams of their application first. With the integration points tested continuously via CI servers, 2 teams can work on their own services and still maintain reasonable assurance that everything will work together properly. API first frees organizations from the waterfall, deliberately engineered system that follows a preplanned orchestration pattern, and allows products to evolve into organic, self-organizing ecosystems that can grow to handle new and unforeseen demands. If you\u2019ve built a monolith, or even an ecosystem of monoliths, that all interact in tightly coupled ways, then your ability to adapt to new needs or create new consumers of existing functionality is hindered. On the other hand, if you adopt the mentality that all applications are just backing services (more on those later in the book), and that they should be designed API-first, then your system is free to grow, adapt to new load and demand, and accommodate new consumers of existing services without having to stop the world to re-architect yet another closed system. Live, eat, and breathe the API-first 3 lifestyle, and your investment will pay off exponentially. The overall point of view on API design is to approach API design from the 'outside-in' perspective. The orientation for APIs is to think about design choices from the application developer's point of view. The goal is to make the developers as successful as possible. The primary design principle when crafting your API should be to maximize developer productivity and success. This is what we call pragmatic REST. You have to get the design right, because design communicates how something will be used. The question becomes - what is the design with optimal benefit for the app developer? The developer point of view is the guiding principle for all the specific tips and best practices we've compiled We call our point of view in API design \"pragmatic REST\", because it places the success of the developer over and above any other design principle. The developer is the customer for the Web API. The success of an API design is measured by how quickly developers can get up to speed and start enjoying success using your API. Any type of design requires taking well-informed decisions, in order to make a better product. API design is no different. Generally API design decisions can be grouped into four categories: Architectural design decisions . How to gather, structure, format, deliver, secure and protect data. It is also a choice between a RESTful or RPC architectural style. Our focus is REST - should we mention that? API Frontend Design Decisions is about what is visible to the API consumers. This is quite important for the success of an API, and what most of these guidelines are about, and especially this page. API Backend Design Decisions . How to leverage backend systems regarding e.g., integration, transformation, aggregation and security. Non-functional Design Decisions . Topics such as security, performance, availability, and evolvability should not be an afterthought. Architectural Design Decisions \u00b6 DO use Richardson Maturity Model Level 2. Strive for a good implementation of Level 2 as it enables building resource-oriented APIs that make full use of HTTP verbs and status codes. Although this is not HATEOAS, it should not prevent you from designing proper link relationships in your APIs. What is the difference between API Design and API Architecture? API architecture may refer to the architecture of the complete solution, consisting not only of the API itself. When building, running and exposing not only one, but several APIs, it becomes clear that certain building blocks of the API, runtime functionality and management functionality for the API need to be used over and over again. An API platform provides an infrastructure for developing, running and managing APIs. The API portfolio contains all APIs of the enterprise and needs to be managed like a product. API architecture may refer to the design decisions for a particular API. This is what we usually refer to as API proxy architecture. API design is one aspect of API architecture. API architecture has a wider scope, considering also the API solution, API platform and API portfolio. the API architecture defines the frame, in which API design can take place and does make sense. API Frontend Design Decisions \u00b6 Remember API Frontend Design Decisions will be based on the Architectural design decisions. Using the following overall API design guidelines provides a practical way to develop a RESTful API: DO do build a Consumer-Oriented API by finding out, how the majority of consumers would want to use the new API. DO use a Contract-First approach when designing a new API. DO design the API, so it fits into the API portfolio. DO choose REST as the architectural style (for this guide to make sense). DO design a blueprint of the API using an API description language . Consumer-Oriented APIs \u00b6 Investing time and energy in building an API that no one wants is a waste of resources. Therefore, engage with potential API consumers before and during the design of the API. DO identify the target consumers, and engage with them to get to know their needs and learn about the problems they are trying to solve with the API. Ideally you have a pilot group of API consumers that can act as a sounding board. DO aim to develop a reusable API that can be used by many consumers in many different scenarios and applications. Distinguish between the needs and requirements of a particular consumer and the needs and requirements that the majority of consumers will have. DO start by analyzing what is needed by the consumers. What would the consumer want to achieve by using the API? How can I make it easy for the consumer to find the API? How can I help the consumer to build apps with my API and make it convenient for the consumer to use the API? DO use the outside-in approach by analysing what is needed by the consumers. If the consumers are outside the organization, the design process needs to start outside organization. It certainly means more work on the side of the API provider, but there is a bigger chance that the consumer gets an API, she actually like to use. DO collect the feedback as early as possible, when changes to the API are still possible, relatively simple and can be implemented with low risk, low effort and low costs. CONSIDER using a classical requirements engineering process. CONSIDER using more interactive techniques such as workshops or design thinking, to quickly uncover hidden needs, to allow for more creativity and to find unexpected solutions. Outside-in vs. inside-out There are two basic approaches for API design: Inside-out , where the starting point is using already existing backend systems as a basis for defining the API. An API could be built just by forwarding calls to backends, even though building the API may be simple from the perspective of the API provider, it might be quite complex to use the API for the API consumers, since they are confronted with the complex data structures of the backends. Outside-in is the opposite where you start with the contract and where the consumers are in focus. This is the approach we recommend . Contract-First \u00b6 Using a contract-first approach to API design, allows for a clear separation of the responsibility between API and client. DO create a central artifact as an explicit contract between API provider and API consumer. DO use a contract to provide guarantees to the API consumer, exactly as they were specified in the contract. DO use an API description language to create a contract. DO use these guidelines and a lightweight API review process to get early review feedback from peers and client developers. Peer review is important to get high quality APIs, to enable architectural and design alignment and to supported development of client applications decoupled from service provider engineering life cycle. Agile Process \u00b6 It is important to know that contract-first is not in conflict with the agile development principles. Contract-first does not mean that you must have complete domain and requirement understanding, and an agile approach can help to navigate in situations with unclear or vague requirements. DO use an agile approach until the API is published for the first time. DO allow for breaking changes, of not yet published features, during the contract development life cycle. Warning Once an API has been published, changes need to be controlled more strictly and agility is confined to new versions of the API. New versions may need to be created for each API change that is not backward compatible. See Versioning and extensibility for more info. Design Approach \u00b6 The API design approach can be organized in the following phases: Analysis Prototyping Implementation Publishing Maintenance Analysis \u00b6 The goal of the first phase is to analyze the \u201cproblem domain\u201d, identify resources and sketch a simple API description for each resource. The first step of a domain analysis phase is gaining some clarity on the needs of the consumer and possible usage scenarios. Start by asking yourself: Who are the consumers of the API? What is the purpose of the API? Which API solutions do the consumers plan to build with the API? Which other API solutions would be possible with the API? not only the usage scenario at hand, or the obvious usage scenario should be sketched. Ideally, a broad set of usage scenarios for the API should be sketched. The next step is to build a resource taxonomy for the given usage scenarios. Think from a consumer\u2019s perspective about the usage scenario, try not to have the tinted view of some existing backend structure or existing database tables, since those provide an internal view. To create a taxonomy, write down the usage scenario, then select the nouns in the text. Shortlist the nouns that would make sense as resources, i.e. nouns that describe data objects, which the operations create, read, update or delete can be performed on. The resources in the taxonomy have states and during the execution of the app, the resources may change their states and transition into new states. Express the states and transitions in a state diagram. The transitions in the diagram provide an indicator for the HTTP methods that need to be supported. questions, a first, low-fidelity API prototype should be built. the API prototype should not be implemented manually, but it should be constructed automatically by generating a simulation based on the API description. it should be avoided to reinvent the wheel for common APIs. Instead, published or company-specific API templates should be used. APIs should also be designed as a part of the API portfolio, meaning that the designed API should be consistent with the other APIs in the same portfolio. The simplest demo app would be a little bit more than a curl call. The demo app created in the previous phase can be reused for integration testing of the simulated API. Prototyping \u00b6 There are thus two goals for proper prototyping: practical insights into critical implementation and usability issues and a low effort for the creation of the prototype. DO let the API prototype should conform to the API description DO use real data from real backends, if possible. If the real backends are available, they may be integrated, otherwise a simulation of the backend is used. input and output need to be validated and security needs to be implemented and configured, To maximize the learnings from prototyping, one should focus on implementing the aspects, which are most critical. Pilot consumers need to be API consumers, who are willing to work with unfinished APIs with changing interfaces, broken clients, frequent updates, unavailability and low performance of the API. In short: a pilot customer must be able to bear some pain. This is why pilot consumers are typically recruited from inside the organization of the API provider, for example from a department of the API provider. Such a test should include JSON wellformedness checks and JSON schema validation of the results. Both of these tests can be generated from the API description. Implementation \u00b6 When an API has reached this phase of the approach, the API description has been properly designed, has gone through several feedback loops has been verified from several perspectives. The API description should thus be stable. Publishing \u00b6 the API and its documentation become publicly available, API consumers will start building API clients and start using the API. Publishing the API also means freezing its interface specification. After publishing, there is no agility in the development process any longer. Changes on published APIs require a traditional change management process. For each published API, the API provider has an implicit contract with all its API consumers, This is why once an API gets published, its interface can never change and the API needs to be maintained for a long time. Publishing an API requires an appropriate documentation for consumers. This can be avoided by generating both the implementation skeleton and the documentation from the same single source of truth, from the API description. To be able to find out if and how this expectation is actually fulfilled by the API, usage of the API has to be monitored, measured and analyzed. Some of the metrics, which might be interesting in this context are: the total number of API calls per time frame, the number of API calls per consumer, and how many API calls resulted in an error vs in success. Not only quantitative analysis is relevant, but also some qualitative analysis. Qualitative analysis includes for example understanding and categorizing the solutions, which the API consumers build with the API. Which of the usage scenarios were correctly predicted and which new usage scenarios evolved? portfolio. A relevant metric is the number successful API calls, especially if compared to the number of unsuccessful API calls. The performance and availability metrics such as status reports, up-time and response time should not only be available to the API provider, but also to the API consumers. Maintenance \u00b6 The simple rule for API evolution is, that the externally observable behavior of an API (from the perspective of the clients) cannot be changed, once the API has been published. longevity and stability are important aspects of published APIs. APIs need to stay backward- (and forward-) compatible, so that old clients do not break and new clients can use the new and improved features. API Description Languages \u00b6 Design a blueprint of the API using an API description language Design the visible frontend interface Build a prototype of the API and simulate its behavior. Collect feedback on the design, check that the design fulfills the non-functional properties. Based on the feedback adapt and iterate. Iteratively the API design is improved. Use a generative API design approach (the generative techniques are only used as far as possible, at some point some code might still need to be written). One of the quickest ways to get feedback on your API is to define it using a specification language. An API specification language allows for building APIs in a consistent manner, utilizing pattern design and code reuse to help ensure that the APIs remains uniform across the full interface, keeping resources and methods standardized. API description languages are domain specific languages, which are especially suited for describing APIs. They are both human readable and machine readable languages, They are intuitive languages that can be easily written, read and understood by API developers and API designers alike. API description languages are also precise, leave little room for ambiguity. They are very expressive and powerful. And they have a well-defined syntax, which makes it possible to process them automatically by software. API description languages use a higher level of abstraction and a declarative paradigm. This means that they can be used to express the \u201cwhat\u201d instead of the \u201chow\u201d. Be aware that none of the popular specification frameworks support Hypermedia Links (let alone HATEOS). API description languages can serve as the single source of truth and as the main reference for all aspects of API design and development. API descriptions also enable the creation of an interactive documentation. Interactive documentation is not only meant to be read like regular documentation, it also includes a testing bed for the APIs. API consumers can make test calls to the real API or to a simulation of the API directly from the documentation page. The API description can be used by the API provider to automatically generate API skeletons. An API skeleton contains some important pieces of the implementation, it is, however, not complete. The skeleton needs to be extended and filled with manual implementation before the API can be used. When the first iteration of the API implementation is generated from the API description, the API implementation is created from scratch. There is no prior implementation to take care of and the API implementation initially only consists of the API skeleton. A challenge for automated code generation are updates to the API description. If a previous implementation already exists, the newly generated code needs to be merged with the existing code. Depending on the code generation framework, this might be supported by specific code markers, which are used to separate the generated code skeleton from the API implementation. OpenAPI/Swagger \u00b6 Especially the frontend design decisions and architectural design decisions, which are visible to the consumer, can be captured by API description languages. they can be used for code generation of the API, code generation of the API clients, generation of documentation, generation of tests, and the generation of mocks. DO define Your API in a flexible, but standard specification language (e.g. RAML, Swagger, JSON-API, etc.). DO NOT not Code to the Spec... and Don't Deviate. If you have taken advantage of the above steps and carefully laid out your API, carefully designed your spec, user tested, and perfected your API \u2013 there is nothing worse than throwing all that work away by deviating from the spec and doing one-off things. DO Follow API First Principle. DO define APIs outside the code first using a standard specification language. By defining APIs outside the code, we want to facilitate early review feedback and also a development discipline that focus service interface design on a profound understanding of the domain and required functionality DO get early review feedback from peers and client DO generalize business entities / resources, i.e. avoidance of use case specific APIs DO a clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects \u2014 APIs should be stable even if we replace complete service implementation including its underlying technology stack Moreover, API definitions with standardized specification format also facilitate... DO use a single source of truth for the API specification; it is a crucial part of a contract between service provider and client users DO infrastructure tooling for API discovery, API GUIs, API documents, automated quality checks DO write APIs in U.S. English. CONSIDER omitting further description of the following status codes from API specifications (includes but is not limited to): 401 Unauthorized 403 Forbidden 404 Not Found unless it has some additional semantics 405 Method Not Allowed 406 Not Acceptable 408 Request Timeout 413 Payload Too Large 414 URI Too Long 415 Unsupported Media Type 500 Internal Server Error 503 Service Unavailable As they are generally self explanatory. DO Provide Online Access to OpenAPI Reference Definition DO Dog-food your own API AVOID Chatty APIs. Some API designs become very chatty - meaning just to build a simple UI or app, you have dozens or hundreds of API calls back to the server. The API team can sometimes decide not to deal with creating a nice, resource-oriented RESTful API, and just fall back to a mode where they create the 3 or 4 Java-style getter and setter methods they know they need to power a particular user interface. We don't recommend this. You can design a RESTful API and still mitigate the chattiness. Be complete and RESTful and provide shortcuts. First design your API and its resources according to pragmatic RESTful design principles and then provide shortcuts. What kind of shortcut? Say you know that 80% of all your apps are going to need some sort of composite response, then build the kind of request that gives them what they need. Take advantage of the partial response syntax. API Backend Design Decisions \u00b6 It is beyond the scope of these guidelines to go into details about API backend design decision, however, the following high-level advice should be taken into account: CONSIDER mocking backends when developing APIs, if the real backend is not available yet, a critical production system, etc. See Mocking for further details. AVOID letting the API Backend Design Decisions influence too heavily the API Frontend Design Decisions. Non-functional Design Decisions \u00b6 Some of the most important non-functional aspects of APIs are security, performance, and availability. Tools \u00b6 The demand for flexibility and extensibility has driven the development of APIs and tools alike, and in many regards it has never been easier to create an API than it is today with multitudes of frameworks (such as JAX-RS, Apigility, Django REST Framework, Grape), specs (RAML, Swagger, API Blueprint, IO Docs), and tools (API Designer, API Science, APImatic) available. MAY: Use REST Maturity Level 3 - HATEOAS \u00b6 We do not generally recommend to implement REST Maturity Level 3. HATEOAS comes with additional API complexity without real value in our SOA context where client and server interact via REST APIs and provide complex business functions as part of our e-commerce SaaS platform. Our major concerns regarding the promised advantages of HATEOAS (see also RESTistential Crisis over Hypermedia APIs, Why I Hate HATEOAS and others): We follow the API First principle with APIs explicitly defined outside the code with standard specification language. HATEOAS does not really add value for SOA client engineers in terms of API self-descriptiveness: a client engineer finds necessary links and usage description (depending on resource state) in the API reference definition anyway. Generic HATEOAS clients which need no prior knowledge about APIs and explore API capabilities based on hypermedia information provided, is a theoretical concept that we haven't seen working in practise and does not fit to our SOA set-up. The OpenAPI description format (and tooling based on OpenAPI) doesn't provide sufficient support for HATEOAS either. In practice relevant HATEOAS approximations (e.g. following specifications like HAL or JSON API) support API navigation by abstracting from URL endpoint and HTTP method aspects via link types. So, Hypermedia does not prevent clients from required manual changes when domain model changes over time. Hypermedia make sense for humans, less for SOA machine clients. We would expect use cases where it may provide value more likely in the frontend and human facing service domain. Hypermedia does not prevent API clients to implement shortcuts and directly target resources without 'discovering' them However, we do not forbid HATEOAS; you could use it, if you checked its limitations and still see clear value for your usage scenario that justifies its additional complexity. If you use HATEOAS please share experience and present your findings in the API Guild [internal link]. A traditional dependency graph looks very hierarchical, where A relies on B, which relies on C. In modern service ecosystems, the graphs are much flatter and often far more complicated. \u21a9 Continuous integration servers can be used to exercise public APIs and integrations between multiple services. Examples of CI servers include Jenkins, Team City, and Wercker. \u21a9 Check out ProgrammableWeb and API First, as well as the documentation at Apiary and API Blueprint, for more details on the API-first lifestyle. \u21a9","title":"API Design"},{"location":"api-design/#api-design","text":"","title":"API Design"},{"location":"api-design/#building-services-api-first","text":"API first means that what you are building is an API to be consumed by client applications and services. Built into every decision you make and every line of code you write is the notion that every functional requirement of your application will be met through the consumption of an API. Even a user interface, be it web or mobile, is really nothing more than a consumer of an API. By designing your API first, you are able to facilitate discussion with your stakeholders (your internal team, customers, or possibly other teams within your organization who want to consume your API) well before you might have coded yourself past the point of no return. This collaboration then allows you to build user stories, mock your API, and generate documentation that can be used to further socialize the intent and functionality of the service you\u2019re building. All of this can be done to vet (and test!) your direction and plans without investing too much in the plumbing that supports a given API. These days, you\u2019ll find that there are myriad tools and standards to support API-first development. There is a standard format for API specification that uses a markdown-like syntax called API Blueprint. This format is far more human readable than JSON (or WSDL, a relic that belongs in a museum) and can be used by code to generate documentation and even server mocks, which are invaluable in testing service ecosystems. Tool suites like Apiary provide things like GitHub integration and server mocks. If someone wants to build a client to your API, all you have to do is give her a link to your application on Apiary, where she can read your API Blueprint, see example code for consuming your service, and even execute requests against a running server mock. In other words, there is absolutely no excuse for claiming that API first is a difficult or unsupported path. This is a pattern that can be applied to noncloud software development, but it is particularly well suited to cloud development in its ability to allow rapid prototyping, support a services ecosystem, and facilitate the automated deployment testing and continuous delivery pipelines that are some of the hallmarks of modern { cloud-native/Kubernetes/\"cloud-friendly\" } application development. This pattern is an extension of the contract-first development pattern, where developers concentrate on building the edges or seams of their application first. With the integration points tested continuously via CI servers, 2 teams can work on their own services and still maintain reasonable assurance that everything will work together properly. API first frees organizations from the waterfall, deliberately engineered system that follows a preplanned orchestration pattern, and allows products to evolve into organic, self-organizing ecosystems that can grow to handle new and unforeseen demands. If you\u2019ve built a monolith, or even an ecosystem of monoliths, that all interact in tightly coupled ways, then your ability to adapt to new needs or create new consumers of existing functionality is hindered. On the other hand, if you adopt the mentality that all applications are just backing services (more on those later in the book), and that they should be designed API-first, then your system is free to grow, adapt to new load and demand, and accommodate new consumers of existing services without having to stop the world to re-architect yet another closed system. Live, eat, and breathe the API-first 3 lifestyle, and your investment will pay off exponentially. The overall point of view on API design is to approach API design from the 'outside-in' perspective. The orientation for APIs is to think about design choices from the application developer's point of view. The goal is to make the developers as successful as possible. The primary design principle when crafting your API should be to maximize developer productivity and success. This is what we call pragmatic REST. You have to get the design right, because design communicates how something will be used. The question becomes - what is the design with optimal benefit for the app developer? The developer point of view is the guiding principle for all the specific tips and best practices we've compiled We call our point of view in API design \"pragmatic REST\", because it places the success of the developer over and above any other design principle. The developer is the customer for the Web API. The success of an API design is measured by how quickly developers can get up to speed and start enjoying success using your API. Any type of design requires taking well-informed decisions, in order to make a better product. API design is no different. Generally API design decisions can be grouped into four categories: Architectural design decisions . How to gather, structure, format, deliver, secure and protect data. It is also a choice between a RESTful or RPC architectural style. Our focus is REST - should we mention that? API Frontend Design Decisions is about what is visible to the API consumers. This is quite important for the success of an API, and what most of these guidelines are about, and especially this page. API Backend Design Decisions . How to leverage backend systems regarding e.g., integration, transformation, aggregation and security. Non-functional Design Decisions . Topics such as security, performance, availability, and evolvability should not be an afterthought.","title":"Building Services API First"},{"location":"api-design/#architectural-design-decisions","text":"DO use Richardson Maturity Model Level 2. Strive for a good implementation of Level 2 as it enables building resource-oriented APIs that make full use of HTTP verbs and status codes. Although this is not HATEOAS, it should not prevent you from designing proper link relationships in your APIs. What is the difference between API Design and API Architecture? API architecture may refer to the architecture of the complete solution, consisting not only of the API itself. When building, running and exposing not only one, but several APIs, it becomes clear that certain building blocks of the API, runtime functionality and management functionality for the API need to be used over and over again. An API platform provides an infrastructure for developing, running and managing APIs. The API portfolio contains all APIs of the enterprise and needs to be managed like a product. API architecture may refer to the design decisions for a particular API. This is what we usually refer to as API proxy architecture. API design is one aspect of API architecture. API architecture has a wider scope, considering also the API solution, API platform and API portfolio. the API architecture defines the frame, in which API design can take place and does make sense.","title":"Architectural Design Decisions"},{"location":"api-design/#api-frontend-design-decisions","text":"Remember API Frontend Design Decisions will be based on the Architectural design decisions. Using the following overall API design guidelines provides a practical way to develop a RESTful API: DO do build a Consumer-Oriented API by finding out, how the majority of consumers would want to use the new API. DO use a Contract-First approach when designing a new API. DO design the API, so it fits into the API portfolio. DO choose REST as the architectural style (for this guide to make sense). DO design a blueprint of the API using an API description language .","title":"API Frontend Design Decisions"},{"location":"api-design/#consumer-oriented-apis","text":"Investing time and energy in building an API that no one wants is a waste of resources. Therefore, engage with potential API consumers before and during the design of the API. DO identify the target consumers, and engage with them to get to know their needs and learn about the problems they are trying to solve with the API. Ideally you have a pilot group of API consumers that can act as a sounding board. DO aim to develop a reusable API that can be used by many consumers in many different scenarios and applications. Distinguish between the needs and requirements of a particular consumer and the needs and requirements that the majority of consumers will have. DO start by analyzing what is needed by the consumers. What would the consumer want to achieve by using the API? How can I make it easy for the consumer to find the API? How can I help the consumer to build apps with my API and make it convenient for the consumer to use the API? DO use the outside-in approach by analysing what is needed by the consumers. If the consumers are outside the organization, the design process needs to start outside organization. It certainly means more work on the side of the API provider, but there is a bigger chance that the consumer gets an API, she actually like to use. DO collect the feedback as early as possible, when changes to the API are still possible, relatively simple and can be implemented with low risk, low effort and low costs. CONSIDER using a classical requirements engineering process. CONSIDER using more interactive techniques such as workshops or design thinking, to quickly uncover hidden needs, to allow for more creativity and to find unexpected solutions. Outside-in vs. inside-out There are two basic approaches for API design: Inside-out , where the starting point is using already existing backend systems as a basis for defining the API. An API could be built just by forwarding calls to backends, even though building the API may be simple from the perspective of the API provider, it might be quite complex to use the API for the API consumers, since they are confronted with the complex data structures of the backends. Outside-in is the opposite where you start with the contract and where the consumers are in focus. This is the approach we recommend .","title":"Consumer-Oriented APIs"},{"location":"api-design/#contract-first","text":"Using a contract-first approach to API design, allows for a clear separation of the responsibility between API and client. DO create a central artifact as an explicit contract between API provider and API consumer. DO use a contract to provide guarantees to the API consumer, exactly as they were specified in the contract. DO use an API description language to create a contract. DO use these guidelines and a lightweight API review process to get early review feedback from peers and client developers. Peer review is important to get high quality APIs, to enable architectural and design alignment and to supported development of client applications decoupled from service provider engineering life cycle.","title":"Contract-First"},{"location":"api-design/#agile-process","text":"It is important to know that contract-first is not in conflict with the agile development principles. Contract-first does not mean that you must have complete domain and requirement understanding, and an agile approach can help to navigate in situations with unclear or vague requirements. DO use an agile approach until the API is published for the first time. DO allow for breaking changes, of not yet published features, during the contract development life cycle. Warning Once an API has been published, changes need to be controlled more strictly and agility is confined to new versions of the API. New versions may need to be created for each API change that is not backward compatible. See Versioning and extensibility for more info.","title":"Agile Process"},{"location":"api-design/#design-approach","text":"The API design approach can be organized in the following phases: Analysis Prototyping Implementation Publishing Maintenance","title":"Design Approach"},{"location":"api-design/#analysis","text":"The goal of the first phase is to analyze the \u201cproblem domain\u201d, identify resources and sketch a simple API description for each resource. The first step of a domain analysis phase is gaining some clarity on the needs of the consumer and possible usage scenarios. Start by asking yourself: Who are the consumers of the API? What is the purpose of the API? Which API solutions do the consumers plan to build with the API? Which other API solutions would be possible with the API? not only the usage scenario at hand, or the obvious usage scenario should be sketched. Ideally, a broad set of usage scenarios for the API should be sketched. The next step is to build a resource taxonomy for the given usage scenarios. Think from a consumer\u2019s perspective about the usage scenario, try not to have the tinted view of some existing backend structure or existing database tables, since those provide an internal view. To create a taxonomy, write down the usage scenario, then select the nouns in the text. Shortlist the nouns that would make sense as resources, i.e. nouns that describe data objects, which the operations create, read, update or delete can be performed on. The resources in the taxonomy have states and during the execution of the app, the resources may change their states and transition into new states. Express the states and transitions in a state diagram. The transitions in the diagram provide an indicator for the HTTP methods that need to be supported. questions, a first, low-fidelity API prototype should be built. the API prototype should not be implemented manually, but it should be constructed automatically by generating a simulation based on the API description. it should be avoided to reinvent the wheel for common APIs. Instead, published or company-specific API templates should be used. APIs should also be designed as a part of the API portfolio, meaning that the designed API should be consistent with the other APIs in the same portfolio. The simplest demo app would be a little bit more than a curl call. The demo app created in the previous phase can be reused for integration testing of the simulated API.","title":"Analysis"},{"location":"api-design/#prototyping","text":"There are thus two goals for proper prototyping: practical insights into critical implementation and usability issues and a low effort for the creation of the prototype. DO let the API prototype should conform to the API description DO use real data from real backends, if possible. If the real backends are available, they may be integrated, otherwise a simulation of the backend is used. input and output need to be validated and security needs to be implemented and configured, To maximize the learnings from prototyping, one should focus on implementing the aspects, which are most critical. Pilot consumers need to be API consumers, who are willing to work with unfinished APIs with changing interfaces, broken clients, frequent updates, unavailability and low performance of the API. In short: a pilot customer must be able to bear some pain. This is why pilot consumers are typically recruited from inside the organization of the API provider, for example from a department of the API provider. Such a test should include JSON wellformedness checks and JSON schema validation of the results. Both of these tests can be generated from the API description.","title":"Prototyping"},{"location":"api-design/#implementation","text":"When an API has reached this phase of the approach, the API description has been properly designed, has gone through several feedback loops has been verified from several perspectives. The API description should thus be stable.","title":"Implementation"},{"location":"api-design/#publishing","text":"the API and its documentation become publicly available, API consumers will start building API clients and start using the API. Publishing the API also means freezing its interface specification. After publishing, there is no agility in the development process any longer. Changes on published APIs require a traditional change management process. For each published API, the API provider has an implicit contract with all its API consumers, This is why once an API gets published, its interface can never change and the API needs to be maintained for a long time. Publishing an API requires an appropriate documentation for consumers. This can be avoided by generating both the implementation skeleton and the documentation from the same single source of truth, from the API description. To be able to find out if and how this expectation is actually fulfilled by the API, usage of the API has to be monitored, measured and analyzed. Some of the metrics, which might be interesting in this context are: the total number of API calls per time frame, the number of API calls per consumer, and how many API calls resulted in an error vs in success. Not only quantitative analysis is relevant, but also some qualitative analysis. Qualitative analysis includes for example understanding and categorizing the solutions, which the API consumers build with the API. Which of the usage scenarios were correctly predicted and which new usage scenarios evolved? portfolio. A relevant metric is the number successful API calls, especially if compared to the number of unsuccessful API calls. The performance and availability metrics such as status reports, up-time and response time should not only be available to the API provider, but also to the API consumers.","title":"Publishing"},{"location":"api-design/#maintenance","text":"The simple rule for API evolution is, that the externally observable behavior of an API (from the perspective of the clients) cannot be changed, once the API has been published. longevity and stability are important aspects of published APIs. APIs need to stay backward- (and forward-) compatible, so that old clients do not break and new clients can use the new and improved features.","title":"Maintenance"},{"location":"api-design/#api-description-languages","text":"Design a blueprint of the API using an API description language Design the visible frontend interface Build a prototype of the API and simulate its behavior. Collect feedback on the design, check that the design fulfills the non-functional properties. Based on the feedback adapt and iterate. Iteratively the API design is improved. Use a generative API design approach (the generative techniques are only used as far as possible, at some point some code might still need to be written). One of the quickest ways to get feedback on your API is to define it using a specification language. An API specification language allows for building APIs in a consistent manner, utilizing pattern design and code reuse to help ensure that the APIs remains uniform across the full interface, keeping resources and methods standardized. API description languages are domain specific languages, which are especially suited for describing APIs. They are both human readable and machine readable languages, They are intuitive languages that can be easily written, read and understood by API developers and API designers alike. API description languages are also precise, leave little room for ambiguity. They are very expressive and powerful. And they have a well-defined syntax, which makes it possible to process them automatically by software. API description languages use a higher level of abstraction and a declarative paradigm. This means that they can be used to express the \u201cwhat\u201d instead of the \u201chow\u201d. Be aware that none of the popular specification frameworks support Hypermedia Links (let alone HATEOS). API description languages can serve as the single source of truth and as the main reference for all aspects of API design and development. API descriptions also enable the creation of an interactive documentation. Interactive documentation is not only meant to be read like regular documentation, it also includes a testing bed for the APIs. API consumers can make test calls to the real API or to a simulation of the API directly from the documentation page. The API description can be used by the API provider to automatically generate API skeletons. An API skeleton contains some important pieces of the implementation, it is, however, not complete. The skeleton needs to be extended and filled with manual implementation before the API can be used. When the first iteration of the API implementation is generated from the API description, the API implementation is created from scratch. There is no prior implementation to take care of and the API implementation initially only consists of the API skeleton. A challenge for automated code generation are updates to the API description. If a previous implementation already exists, the newly generated code needs to be merged with the existing code. Depending on the code generation framework, this might be supported by specific code markers, which are used to separate the generated code skeleton from the API implementation.","title":"API Description Languages"},{"location":"api-design/#openapiswagger","text":"Especially the frontend design decisions and architectural design decisions, which are visible to the consumer, can be captured by API description languages. they can be used for code generation of the API, code generation of the API clients, generation of documentation, generation of tests, and the generation of mocks. DO define Your API in a flexible, but standard specification language (e.g. RAML, Swagger, JSON-API, etc.). DO NOT not Code to the Spec... and Don't Deviate. If you have taken advantage of the above steps and carefully laid out your API, carefully designed your spec, user tested, and perfected your API \u2013 there is nothing worse than throwing all that work away by deviating from the spec and doing one-off things. DO Follow API First Principle. DO define APIs outside the code first using a standard specification language. By defining APIs outside the code, we want to facilitate early review feedback and also a development discipline that focus service interface design on a profound understanding of the domain and required functionality DO get early review feedback from peers and client DO generalize business entities / resources, i.e. avoidance of use case specific APIs DO a clear separation of WHAT vs. HOW concerns, i.e. abstraction from implementation aspects \u2014 APIs should be stable even if we replace complete service implementation including its underlying technology stack Moreover, API definitions with standardized specification format also facilitate... DO use a single source of truth for the API specification; it is a crucial part of a contract between service provider and client users DO infrastructure tooling for API discovery, API GUIs, API documents, automated quality checks DO write APIs in U.S. English. CONSIDER omitting further description of the following status codes from API specifications (includes but is not limited to): 401 Unauthorized 403 Forbidden 404 Not Found unless it has some additional semantics 405 Method Not Allowed 406 Not Acceptable 408 Request Timeout 413 Payload Too Large 414 URI Too Long 415 Unsupported Media Type 500 Internal Server Error 503 Service Unavailable As they are generally self explanatory. DO Provide Online Access to OpenAPI Reference Definition DO Dog-food your own API AVOID Chatty APIs. Some API designs become very chatty - meaning just to build a simple UI or app, you have dozens or hundreds of API calls back to the server. The API team can sometimes decide not to deal with creating a nice, resource-oriented RESTful API, and just fall back to a mode where they create the 3 or 4 Java-style getter and setter methods they know they need to power a particular user interface. We don't recommend this. You can design a RESTful API and still mitigate the chattiness. Be complete and RESTful and provide shortcuts. First design your API and its resources according to pragmatic RESTful design principles and then provide shortcuts. What kind of shortcut? Say you know that 80% of all your apps are going to need some sort of composite response, then build the kind of request that gives them what they need. Take advantage of the partial response syntax.","title":"OpenAPI/Swagger"},{"location":"api-design/#api-backend-design-decisions","text":"It is beyond the scope of these guidelines to go into details about API backend design decision, however, the following high-level advice should be taken into account: CONSIDER mocking backends when developing APIs, if the real backend is not available yet, a critical production system, etc. See Mocking for further details. AVOID letting the API Backend Design Decisions influence too heavily the API Frontend Design Decisions.","title":"API Backend Design Decisions"},{"location":"api-design/#non-functional-design-decisions","text":"Some of the most important non-functional aspects of APIs are security, performance, and availability.","title":"Non-functional Design Decisions"},{"location":"api-design/#tools","text":"The demand for flexibility and extensibility has driven the development of APIs and tools alike, and in many regards it has never been easier to create an API than it is today with multitudes of frameworks (such as JAX-RS, Apigility, Django REST Framework, Grape), specs (RAML, Swagger, API Blueprint, IO Docs), and tools (API Designer, API Science, APImatic) available.","title":"Tools"},{"location":"api-design/#may-use-rest-maturity-level-3-hateoas","text":"We do not generally recommend to implement REST Maturity Level 3. HATEOAS comes with additional API complexity without real value in our SOA context where client and server interact via REST APIs and provide complex business functions as part of our e-commerce SaaS platform. Our major concerns regarding the promised advantages of HATEOAS (see also RESTistential Crisis over Hypermedia APIs, Why I Hate HATEOAS and others): We follow the API First principle with APIs explicitly defined outside the code with standard specification language. HATEOAS does not really add value for SOA client engineers in terms of API self-descriptiveness: a client engineer finds necessary links and usage description (depending on resource state) in the API reference definition anyway. Generic HATEOAS clients which need no prior knowledge about APIs and explore API capabilities based on hypermedia information provided, is a theoretical concept that we haven't seen working in practise and does not fit to our SOA set-up. The OpenAPI description format (and tooling based on OpenAPI) doesn't provide sufficient support for HATEOAS either. In practice relevant HATEOAS approximations (e.g. following specifications like HAL or JSON API) support API navigation by abstracting from URL endpoint and HTTP method aspects via link types. So, Hypermedia does not prevent clients from required manual changes when domain model changes over time. Hypermedia make sense for humans, less for SOA machine clients. We would expect use cases where it may provide value more likely in the frontend and human facing service domain. Hypermedia does not prevent API clients to implement shortcuts and directly target resources without 'discovering' them However, we do not forbid HATEOAS; you could use it, if you checked its limitations and still see clear value for your usage scenario that justifies its additional complexity. If you use HATEOAS please share experience and present your findings in the API Guild [internal link]. A traditional dependency graph looks very hierarchical, where A relies on B, which relies on C. In modern service ecosystems, the graphs are much flatter and often far more complicated. \u21a9 Continuous integration servers can be used to exercise public APIs and integrations between multiple services. Examples of CI servers include Jenkins, Team City, and Wercker. \u21a9 Check out ProgrammableWeb and API First, as well as the documentation at Apiary and API Blueprint, for more details on the API-first lifestyle. \u21a9","title":"MAY: Use REST Maturity Level 3 - HATEOAS"},{"location":"apis/","text":"Application Programming Interfaces \u00b6 An API specifies how software should interact. Particularly, how a client can use an API, what URIs are available, what HTTP methods may be used, what query string parameters it accepts, what data can be sent, and what responses to expect. Why APIs? \u00b6 The business of an enterprise can be expanded by linking the business to partners up and down the value chain. Another reason for using APIs is their use as an innovation lab of the enterprise. To fulfill this vision, the API portfolio should enable the enterprise to build innovative apps with little effort and spark creativity. By making company assets easily available through API, new uses of these assets can be found. Since APIs provide a new, simple way for accessing company assets, assets can be used in new ways within the company. Providing external access to the assets of your organization via APIs, enables external third party developers to create innovations for your organization by using your organization\u2019s assets. API as a Product \u00b6 Great products are designed with rigorous customer focus, a deep understanding of the customers, their needs and desires. To design great APIs, you first need to realize, that APIs are in fact products. APIs are products of their own! APIs are Products \u00b6 To design great APIs, we first need to realize, that APIs are in fact products. They are intended to be highly reusable software. It may be, that the idea for an API originates from one particular use case for one particular customer, but successful APIs are always more generic. The customers are the developers of the API consumer. When creating APIs intended for use by someone else, whether it is actually sold or not, it can and should most likely be treated as a product in itself. This can be very easy to lose sight of, so before jumping to the more specific advice on how to achieve successful APIs, keep the following overall principles in mind: Be Customer-Centric \u00b6 Do not focus on data rather than customers. Instead focus on target developer's problems, by involving customers early in the design process. Make use of reviews and customers feedback, and provide service level support. Make APIs irresistible by having a high attention to API quality and client developer experience. Treat Every API as if it was a Public API \u00b6 Do not skip on usability and security when creating private or partner APIs, as this can make the transition into a public API difficult. Instead actively improve and maintain API consistency over the long term. Componentize Your Systems \u00b6 Identifying, breaking apart, and isolating small feature sets in big systems: can help innovation by creating platforms to enable other organizations to build new applications that better suit their needs offers a higher degree of composability, which makes it possible to offer products as open systems instead of inflexible off-the-shelf solutions may help monetize existing assets Types of APIs \u00b6 Web APIs can typically be broken into two broad categories: RPC and REST . These guidelines will only deal with REST. RPC (Remote Procedure Call) RPC (e.g., gRPC and SOAP) is generally a poor fit for most public API implementations, as they usually require special purpose libraries or SDKs to work properly. A requirement that a lot of clients cannot meet without a lot of extra work and/or clunky third-party libraries. This, and the fact that most RPC implementations do not use HTTP to its full extend, has led us to exclude RPC from these guidelines . However, if all of your clients are using legacy systems that depend on SOAP libraries, it may make more sense in that case to build a SOAP API for them. A lot of documentation, best practices and general advise on how to build RPC APIs can be found throughout the web. Organizing APIs \u00b6 Large API vendors (like Facebook, Twitter, and Foursquare) usually have their APIs organized behind one or more subdomains, like e.g.: Facebook Twitter Forsquare graph.facebook.com stream.twitter.com api.foursquare.com api.facebook.com api.twitter.com search.twitter.com It is not difficult to imagine how Facebook and Twitter ended up with more than one API - timing, team organization, acquisitions have likely played their part. However, following Foursquare's lead and consolidating all API requests under one API subdomain, might be in the best interest of API consumers. DO consolidate API requests in one subdomain, e.g. https://api.example.org . It is cleaner, easier and more intuitive for developers who want to build cool stuff using your APIs. Developer Portal \u00b6 A developer portal might be a good idea for hosting examples, application notes, authentication guidance, human-readable documentation, error scenarios and descriptions, etc. Again some of the major API vendors all have dedicated developer portals: Facebook Twitter Forsquare developers.facebook.com dev.twitter.com developers.foursquare.com DO use the dev.example.org or developers.example.org subdomains to expose your developer portal, if the API subdomain resides on api.example.org . CONSIDER using redirects, if you can gather based on the incoming request from a browser where the developer really needs to go. E.g., if a developer types api.example.org in the browser without any furher information in the GET request, it is probably safe to redirect to the developer portal and help get the developer. API Operation \u00b6 DO setup instrumentation from the outset of the API implementation to monitor important business metrics/KPIs. DO gather information about the clients. This information, for instance, is useful to identify potential review partners for API changes.","title":"APIs"},{"location":"apis/#application-programming-interfaces","text":"An API specifies how software should interact. Particularly, how a client can use an API, what URIs are available, what HTTP methods may be used, what query string parameters it accepts, what data can be sent, and what responses to expect.","title":"Application Programming Interfaces"},{"location":"apis/#why-apis","text":"The business of an enterprise can be expanded by linking the business to partners up and down the value chain. Another reason for using APIs is their use as an innovation lab of the enterprise. To fulfill this vision, the API portfolio should enable the enterprise to build innovative apps with little effort and spark creativity. By making company assets easily available through API, new uses of these assets can be found. Since APIs provide a new, simple way for accessing company assets, assets can be used in new ways within the company. Providing external access to the assets of your organization via APIs, enables external third party developers to create innovations for your organization by using your organization\u2019s assets.","title":"Why APIs?"},{"location":"apis/#api-as-a-product","text":"Great products are designed with rigorous customer focus, a deep understanding of the customers, their needs and desires. To design great APIs, you first need to realize, that APIs are in fact products. APIs are products of their own!","title":"API as a Product"},{"location":"apis/#apis-are-products","text":"To design great APIs, we first need to realize, that APIs are in fact products. They are intended to be highly reusable software. It may be, that the idea for an API originates from one particular use case for one particular customer, but successful APIs are always more generic. The customers are the developers of the API consumer. When creating APIs intended for use by someone else, whether it is actually sold or not, it can and should most likely be treated as a product in itself. This can be very easy to lose sight of, so before jumping to the more specific advice on how to achieve successful APIs, keep the following overall principles in mind:","title":"APIs are Products"},{"location":"apis/#be-customer-centric","text":"Do not focus on data rather than customers. Instead focus on target developer's problems, by involving customers early in the design process. Make use of reviews and customers feedback, and provide service level support. Make APIs irresistible by having a high attention to API quality and client developer experience.","title":"Be Customer-Centric"},{"location":"apis/#treat-every-api-as-if-it-was-a-public-api","text":"Do not skip on usability and security when creating private or partner APIs, as this can make the transition into a public API difficult. Instead actively improve and maintain API consistency over the long term.","title":"Treat Every API as if it was a Public API"},{"location":"apis/#componentize-your-systems","text":"Identifying, breaking apart, and isolating small feature sets in big systems: can help innovation by creating platforms to enable other organizations to build new applications that better suit their needs offers a higher degree of composability, which makes it possible to offer products as open systems instead of inflexible off-the-shelf solutions may help monetize existing assets","title":"Componentize Your Systems"},{"location":"apis/#types-of-apis","text":"Web APIs can typically be broken into two broad categories: RPC and REST . These guidelines will only deal with REST. RPC (Remote Procedure Call) RPC (e.g., gRPC and SOAP) is generally a poor fit for most public API implementations, as they usually require special purpose libraries or SDKs to work properly. A requirement that a lot of clients cannot meet without a lot of extra work and/or clunky third-party libraries. This, and the fact that most RPC implementations do not use HTTP to its full extend, has led us to exclude RPC from these guidelines . However, if all of your clients are using legacy systems that depend on SOAP libraries, it may make more sense in that case to build a SOAP API for them. A lot of documentation, best practices and general advise on how to build RPC APIs can be found throughout the web.","title":"Types of APIs"},{"location":"apis/#organizing-apis","text":"Large API vendors (like Facebook, Twitter, and Foursquare) usually have their APIs organized behind one or more subdomains, like e.g.: Facebook Twitter Forsquare graph.facebook.com stream.twitter.com api.foursquare.com api.facebook.com api.twitter.com search.twitter.com It is not difficult to imagine how Facebook and Twitter ended up with more than one API - timing, team organization, acquisitions have likely played their part. However, following Foursquare's lead and consolidating all API requests under one API subdomain, might be in the best interest of API consumers. DO consolidate API requests in one subdomain, e.g. https://api.example.org . It is cleaner, easier and more intuitive for developers who want to build cool stuff using your APIs.","title":"Organizing APIs"},{"location":"apis/#developer-portal","text":"A developer portal might be a good idea for hosting examples, application notes, authentication guidance, human-readable documentation, error scenarios and descriptions, etc. Again some of the major API vendors all have dedicated developer portals: Facebook Twitter Forsquare developers.facebook.com dev.twitter.com developers.foursquare.com DO use the dev.example.org or developers.example.org subdomains to expose your developer portal, if the API subdomain resides on api.example.org . CONSIDER using redirects, if you can gather based on the incoming request from a browser where the developer really needs to go. E.g., if a developer types api.example.org in the browser without any furher information in the GET request, it is probably safe to redirect to the developer portal and help get the developer.","title":"Developer Portal"},{"location":"apis/#api-operation","text":"DO setup instrumentation from the outset of the API implementation to monitor important business metrics/KPIs. DO gather information about the clients. This information, for instance, is useful to identify potential review partners for API changes.","title":"API Operation"},{"location":"caching/","text":"Caching and Conditional Requests \u00b6 Caching is one of the most useful features built on top of HTTP's uniform interface. You can take advantage of caching to reduce end user perceived latency, to increase reliasbility, to reduce bandwidth usage and cost, and to reduce server load. They can be in server network, content delivery networks (CDNs), or in the client network (forward proxies). It is common to use the word cache to refer to either an object cache (memcached) or HTTP caches. Both of these caches improve performacne and have key roles to play in the overall web service deployment architecture. But there is an important difference between the two. HTTP caches do not require clients and servers to call any special programming API to manage data in the cache, as long as you are using HTTP as defined. origin server \u00b6 They are slightly different - the ETag does not have any information that the client can use to determine whether or not to make a request for that file again in the future. If ETag is all it has, it will always have to make a request. However, when the server reads the ETag from the client request, the server can then determine whether to send the file (HTTP 200) or tell the client to just use their local copy (HTTP 304). An ETag is basically just a checksum for a file that semantically changes when the content of the file changes. The Expires header is used by the client (and proxies/caches) to determine whether or not it even needs to make a request to the server at all. The closer you are to the Expires date, the more likely it is the client (or proxy) will make an HTTP request for that file from the server. So really what you want to do is use BOTH headers - set the Expires header to a reasonable value based on how often the content changes. Then configure ETags to be sent so that when clients DO send a request to the server, it can more easily determine whether or not to send the file back. One last note about ETag - if you are using a load-balanced server setup with multiple machines running Apache you will probably want to turn off ETag generation. This is because inodes are used as part of the ETag hash algorithm which will be different between the servers. You can configure Apache to not use inodes as part of the calculation but then you'd want to make sure the timestamps on the files are exactly the same, to ensure the same ETag gets generated for all servers. How to set Expiration Caching Headers Expiration caching is based on the Cache-Control and Expires headers. These headers instruct clients and caches to keep a copy of the representation returned by the server for a specific length of time. Based on the frequency of updates, determine a time period (the freshness lifetime ) during which caches can serve a representation. After this time, caches will consider cached representations stale. When serving a representation, include a Cache-Control header with a max-age value (in seconds) equal to the freshness lifetime. The Cache-Control header is a HTTP 1.1 header. To support legacy HTTP 1.0 caches, include a Expires header with the expiration date-time. The expiration date-time is a time at which the server generated the representation plus the freshness lifetime. Also include a Date header with a date-time at which the server returned the response. Including this header helps clients compute the freshness lifetime as the difference between the values of the Expires and Date headers. If the caches must not serve cached copies, add Cache-Control: no-cache to the HTTP headers. Also add a Pragma: no-cache to support HTTP 1.0 caches. List of Cache-Control directives: public (default): when the request is authenticated, but you still want to allow shared caches to serve cached responses. private : when the response is preivate to the client or user. Any client-side caches (browsers/forward proxies) can cache the representation, but sharede caches along the network must not cache it. no-cache and no-store : prevents any cache from storing or serving a cached response. max-age : the freshness lifetime in seconds. s-maxage : life max-age , but is meant only for shared caches. Just use max-age . must-revalidate : caches must check the origin server before serving stale representations. proxy-revalidate : same as must-revalidate but only for shared caches. The Age header is added by the cache. It indicates how long ago the cache retrieved the representation from the origin server. The key to optimal expiration caching is calculating a resonable freshness lifetime value for the resource representation. If you gave historical information such as update logs for the representations, use them to establish a base lifetime. When to set Expiration Caching Headers Set expiration caching headers for responses of GET and HEAD request for all successful response codes. Consider adding caching headers to: 300 Multiple Choices : The representation with this status may not change often. 301 Moved Permanently 404 Not Found 405 Method Not Allowed 410 Gone When and how to use Expiration Headers in Clients Avoid implementing support for expiration caching within clients, instead deploy a forward proxy in the client network, and avoid implementing you own caching layer in the client code. Conditional Request \u00b6 Conditional request address two programs. For GET request conditional request help clients and caches validate that a cached representation can still be considered fresh. For unsafe request such as PUT , POST and DELETE conditional requests provide concurrency control. Not supporting conditional GET request reduces performance. Not makeing unsafe requests conditional, when facing concurrency, ay affect the integrity of the application. In absence of adequate concurrency control checks, the server is susceptible to \"lost updates\" and \"stale deletes\". Perssimistic concurrency control, the client gets a lock, obtain the current state of a resource, makes modifications, and then releases the lock. During this process the server present other clients from aquiring a lock to the same resource. Optimistic concurrency control, the client gets a token, and attempts a write operation with the token. The operation success if the token is still valid or fails otherwise. HTTP, being stateless, is designed for optimistic concurrency control. Conditional GET request can extend the life of stale representations. How to Generate Last-Modified and ETag Headers Use Last-Modified and ETag response headers to drive conditional requests. Clients use the following: If-Modified-Since and If-None-Match for validating cached representations If-Unmodified-Since and If-Match as preconditions for concurrency control a timestamp for the modifed date-time and/or a sequence number of keep track of a version. use an MD5 hash of the representation body, or of some field of the data that changes every time the resource ip updated make sure to use a different ETag value of each representation of the resource (this include different media types, etc.). Last-Modified has a 1 second resolution is considered a \"weak\" validator. ETag is a strong validator since its value can change every time the entity is modified. Entity tag is an object hash code You do not need to use both Last-Modified and ETag headers to support conditional requests. Use either or both consistently to support conditional requests. How to Implement Conditional GET Requests in Servers Send If-Modified-Since and If-None-Match headers based Last-Modified and ETag headers from a previous request. Conditional requests do not cut down on the number of requests from the client, but they can reduce the number of times a server needs to send a fresh representation. Check If-None-Match against ETag , and check If-Modified-Since against Last-Modified , if either checks are false or missing return the latest copy of the representation including new ETag and/or Last-Modified , else return 304 Not Modified . To support validation to extend the life of a cached copy the server must return expiration headers as well. The values of ETag , If-Match , and If-None-Match are quoted strings. How to Submit Conditional GET and HEAD Requests from Clients Store ETag and/or Last-Modified along with the representation data. and reply them on future requests. Include If-Modified-Since with the value from Last-Modified Include If-None-Match with the value from ETag Do not send conditional requests unless you have a copy of the representaton stored locally on the client. How to Implement Conditional PUT Requests in Servers If the resource exists: If the client does not include If-Unmodified-Since and/or If-Match header return 403 Forbidden . Explain why in the body. If the suppiled If-Unmodified-Since or If-Match do not match the server values return 412 Precondition Failed . Explain why in the body. If the conditions match return 200 OK or 204 No Content and update the resource. Optionally include Last-Modified and/or ETag headers provided the response also include a Content-Location header with the URI of the updated resource. How to Implement Conditional DELETE Requests in Servers If the client does not include If-Unmodified-Since and/or If-Match header return 403 Forbidden . Explain why in the body. If the suppiled If-Unmodified-Since or If-Match do not match the server values return 412 Precondition Failed . Explain why in the body. If the conditions match return 200 OK or 204 No Content and delete the resource. How to Make Unconditional GET Requests from Clients HTTP 1.1 allows clients to modify expiration caching and ash for fresh representations. To get a fresh represention after you receive 412 Precondition Failed or even after a successful PUT or PATCH to get the latest representation. Include Cache-Control: no-cache and Pragma: no-cache in the GET request. Do not make unconditional GET request unless necessary as the downgrade performacne and increase latency. How to Submit Conditional PUT and DELETE Requests from Clients Include If-Unmodified-Since with the stored value of Last-Modified Include If-Match with the stored value of ETag If the server return 412 Precondition Failed submit an unconditional request to ontain a fresh Last-Modified and ETag , verify the decision to update or delete the resource is still valid per the fresh representation, and then repeat the PUT or DELETE with the new header values. Do not use HEAD to obtain fresh Last-Modified and ETag you will also new the current satet of the resource to determine if you can go ahead with the operation. How to Make POST Requests Conditional How to Generate One-Time URIs Common methods of ETag generation include using a collision-resistant hash function of the resource's content, a hash of the last modification timestamp, or even just a revision number. MAY: Consider using ETag together with If-(None-)Match header \u00b6 When creating or updating resources it may be necessary to expose conflicts and to prevent the lost update problem. This can be best accomplished by using the ETag header together with the If-Match and If-None-Match. The contents of an ETag: header is either (a) a hash of the response body, (b) a hash of the last modified field of the entity, or \u00a9 a version number or identifier of the entity version. To expose conflicts between concurrent update operations via PUT, POST, or PATCH, the If-Match: header can be used to force the server to check whether the version of the updated entity is conforming to the requested . If no matching entity is found, the operation is supposed a to respond with status code 412 - precondition failed. Beside other use cases, the If-None-Match: header with parameter * can be used in a similar way to expose conflicts in resource creation. If any matching entity is found, the operation is supposed a to respond with status code 412 - precondition failed. The ETag, If-Match, and If-None-Match headers can be defined as follows in the API definition: Etag: name: Etag description: | The RFC7232 ETag header field in a response provides the current entity-tag for the selected resource. An entity-tag is an opaque identifier for different versions of a resource over time, regardless whether multiple versions are valid at the same time. An entity-tag consists of an opaque quoted string, possibly prefixed by a weakness indicator. in: header type: string required: false example: W/\"xy\", \"5\", \"7da7a728-f910-11e6-942a-68f728c1ba70\" IfMatch: name: If-Match description: | The RFC7232 If-Match header field in a request requires the server to only operate on the resource that matches at least one of the provided entity-tags. This allows clients express a precondition that prevent the method from being applied, if there have been any changes to the resource. in: header type: string required: false example: \"5\", \"7da7a728-f910-11e6-942a-68f728c1ba70\" IfNoneMatch: name: If-None-Match description: | The RFC7232 If-None-Match header field in a request requires the server to only operate on the resource if it does not match any of the provided entity-tags. If the provided entity-tag is `*`, it is required that the resource does not exist at all. in: header type: string required: false example: \"7da7a728-f910-11e6-942a-68f728c1ba70\", *","title":"Caching"},{"location":"caching/#caching-and-conditional-requests","text":"Caching is one of the most useful features built on top of HTTP's uniform interface. You can take advantage of caching to reduce end user perceived latency, to increase reliasbility, to reduce bandwidth usage and cost, and to reduce server load. They can be in server network, content delivery networks (CDNs), or in the client network (forward proxies). It is common to use the word cache to refer to either an object cache (memcached) or HTTP caches. Both of these caches improve performacne and have key roles to play in the overall web service deployment architecture. But there is an important difference between the two. HTTP caches do not require clients and servers to call any special programming API to manage data in the cache, as long as you are using HTTP as defined.","title":"Caching and Conditional Requests"},{"location":"caching/#origin-server","text":"They are slightly different - the ETag does not have any information that the client can use to determine whether or not to make a request for that file again in the future. If ETag is all it has, it will always have to make a request. However, when the server reads the ETag from the client request, the server can then determine whether to send the file (HTTP 200) or tell the client to just use their local copy (HTTP 304). An ETag is basically just a checksum for a file that semantically changes when the content of the file changes. The Expires header is used by the client (and proxies/caches) to determine whether or not it even needs to make a request to the server at all. The closer you are to the Expires date, the more likely it is the client (or proxy) will make an HTTP request for that file from the server. So really what you want to do is use BOTH headers - set the Expires header to a reasonable value based on how often the content changes. Then configure ETags to be sent so that when clients DO send a request to the server, it can more easily determine whether or not to send the file back. One last note about ETag - if you are using a load-balanced server setup with multiple machines running Apache you will probably want to turn off ETag generation. This is because inodes are used as part of the ETag hash algorithm which will be different between the servers. You can configure Apache to not use inodes as part of the calculation but then you'd want to make sure the timestamps on the files are exactly the same, to ensure the same ETag gets generated for all servers. How to set Expiration Caching Headers Expiration caching is based on the Cache-Control and Expires headers. These headers instruct clients and caches to keep a copy of the representation returned by the server for a specific length of time. Based on the frequency of updates, determine a time period (the freshness lifetime ) during which caches can serve a representation. After this time, caches will consider cached representations stale. When serving a representation, include a Cache-Control header with a max-age value (in seconds) equal to the freshness lifetime. The Cache-Control header is a HTTP 1.1 header. To support legacy HTTP 1.0 caches, include a Expires header with the expiration date-time. The expiration date-time is a time at which the server generated the representation plus the freshness lifetime. Also include a Date header with a date-time at which the server returned the response. Including this header helps clients compute the freshness lifetime as the difference between the values of the Expires and Date headers. If the caches must not serve cached copies, add Cache-Control: no-cache to the HTTP headers. Also add a Pragma: no-cache to support HTTP 1.0 caches. List of Cache-Control directives: public (default): when the request is authenticated, but you still want to allow shared caches to serve cached responses. private : when the response is preivate to the client or user. Any client-side caches (browsers/forward proxies) can cache the representation, but sharede caches along the network must not cache it. no-cache and no-store : prevents any cache from storing or serving a cached response. max-age : the freshness lifetime in seconds. s-maxage : life max-age , but is meant only for shared caches. Just use max-age . must-revalidate : caches must check the origin server before serving stale representations. proxy-revalidate : same as must-revalidate but only for shared caches. The Age header is added by the cache. It indicates how long ago the cache retrieved the representation from the origin server. The key to optimal expiration caching is calculating a resonable freshness lifetime value for the resource representation. If you gave historical information such as update logs for the representations, use them to establish a base lifetime. When to set Expiration Caching Headers Set expiration caching headers for responses of GET and HEAD request for all successful response codes. Consider adding caching headers to: 300 Multiple Choices : The representation with this status may not change often. 301 Moved Permanently 404 Not Found 405 Method Not Allowed 410 Gone When and how to use Expiration Headers in Clients Avoid implementing support for expiration caching within clients, instead deploy a forward proxy in the client network, and avoid implementing you own caching layer in the client code.","title":"origin server"},{"location":"caching/#conditional-request","text":"Conditional request address two programs. For GET request conditional request help clients and caches validate that a cached representation can still be considered fresh. For unsafe request such as PUT , POST and DELETE conditional requests provide concurrency control. Not supporting conditional GET request reduces performance. Not makeing unsafe requests conditional, when facing concurrency, ay affect the integrity of the application. In absence of adequate concurrency control checks, the server is susceptible to \"lost updates\" and \"stale deletes\". Perssimistic concurrency control, the client gets a lock, obtain the current state of a resource, makes modifications, and then releases the lock. During this process the server present other clients from aquiring a lock to the same resource. Optimistic concurrency control, the client gets a token, and attempts a write operation with the token. The operation success if the token is still valid or fails otherwise. HTTP, being stateless, is designed for optimistic concurrency control. Conditional GET request can extend the life of stale representations. How to Generate Last-Modified and ETag Headers Use Last-Modified and ETag response headers to drive conditional requests. Clients use the following: If-Modified-Since and If-None-Match for validating cached representations If-Unmodified-Since and If-Match as preconditions for concurrency control a timestamp for the modifed date-time and/or a sequence number of keep track of a version. use an MD5 hash of the representation body, or of some field of the data that changes every time the resource ip updated make sure to use a different ETag value of each representation of the resource (this include different media types, etc.). Last-Modified has a 1 second resolution is considered a \"weak\" validator. ETag is a strong validator since its value can change every time the entity is modified. Entity tag is an object hash code You do not need to use both Last-Modified and ETag headers to support conditional requests. Use either or both consistently to support conditional requests. How to Implement Conditional GET Requests in Servers Send If-Modified-Since and If-None-Match headers based Last-Modified and ETag headers from a previous request. Conditional requests do not cut down on the number of requests from the client, but they can reduce the number of times a server needs to send a fresh representation. Check If-None-Match against ETag , and check If-Modified-Since against Last-Modified , if either checks are false or missing return the latest copy of the representation including new ETag and/or Last-Modified , else return 304 Not Modified . To support validation to extend the life of a cached copy the server must return expiration headers as well. The values of ETag , If-Match , and If-None-Match are quoted strings. How to Submit Conditional GET and HEAD Requests from Clients Store ETag and/or Last-Modified along with the representation data. and reply them on future requests. Include If-Modified-Since with the value from Last-Modified Include If-None-Match with the value from ETag Do not send conditional requests unless you have a copy of the representaton stored locally on the client. How to Implement Conditional PUT Requests in Servers If the resource exists: If the client does not include If-Unmodified-Since and/or If-Match header return 403 Forbidden . Explain why in the body. If the suppiled If-Unmodified-Since or If-Match do not match the server values return 412 Precondition Failed . Explain why in the body. If the conditions match return 200 OK or 204 No Content and update the resource. Optionally include Last-Modified and/or ETag headers provided the response also include a Content-Location header with the URI of the updated resource. How to Implement Conditional DELETE Requests in Servers If the client does not include If-Unmodified-Since and/or If-Match header return 403 Forbidden . Explain why in the body. If the suppiled If-Unmodified-Since or If-Match do not match the server values return 412 Precondition Failed . Explain why in the body. If the conditions match return 200 OK or 204 No Content and delete the resource. How to Make Unconditional GET Requests from Clients HTTP 1.1 allows clients to modify expiration caching and ash for fresh representations. To get a fresh represention after you receive 412 Precondition Failed or even after a successful PUT or PATCH to get the latest representation. Include Cache-Control: no-cache and Pragma: no-cache in the GET request. Do not make unconditional GET request unless necessary as the downgrade performacne and increase latency. How to Submit Conditional PUT and DELETE Requests from Clients Include If-Unmodified-Since with the stored value of Last-Modified Include If-Match with the stored value of ETag If the server return 412 Precondition Failed submit an unconditional request to ontain a fresh Last-Modified and ETag , verify the decision to update or delete the resource is still valid per the fresh representation, and then repeat the PUT or DELETE with the new header values. Do not use HEAD to obtain fresh Last-Modified and ETag you will also new the current satet of the resource to determine if you can go ahead with the operation. How to Make POST Requests Conditional How to Generate One-Time URIs Common methods of ETag generation include using a collision-resistant hash function of the resource's content, a hash of the last modification timestamp, or even just a revision number.","title":"Conditional Request"},{"location":"caching/#may-consider-using-etag-together-with-if-none-match-header","text":"When creating or updating resources it may be necessary to expose conflicts and to prevent the lost update problem. This can be best accomplished by using the ETag header together with the If-Match and If-None-Match. The contents of an ETag: header is either (a) a hash of the response body, (b) a hash of the last modified field of the entity, or \u00a9 a version number or identifier of the entity version. To expose conflicts between concurrent update operations via PUT, POST, or PATCH, the If-Match: header can be used to force the server to check whether the version of the updated entity is conforming to the requested . If no matching entity is found, the operation is supposed a to respond with status code 412 - precondition failed. Beside other use cases, the If-None-Match: header with parameter * can be used in a similar way to expose conflicts in resource creation. If any matching entity is found, the operation is supposed a to respond with status code 412 - precondition failed. The ETag, If-Match, and If-None-Match headers can be defined as follows in the API definition: Etag: name: Etag description: | The RFC7232 ETag header field in a response provides the current entity-tag for the selected resource. An entity-tag is an opaque identifier for different versions of a resource over time, regardless whether multiple versions are valid at the same time. An entity-tag consists of an opaque quoted string, possibly prefixed by a weakness indicator. in: header type: string required: false example: W/\"xy\", \"5\", \"7da7a728-f910-11e6-942a-68f728c1ba70\" IfMatch: name: If-Match description: | The RFC7232 If-Match header field in a request requires the server to only operate on the resource that matches at least one of the provided entity-tags. This allows clients express a precondition that prevent the method from being applied, if there have been any changes to the resource. in: header type: string required: false example: \"5\", \"7da7a728-f910-11e6-942a-68f728c1ba70\" IfNoneMatch: name: If-None-Match description: | The RFC7232 If-None-Match header field in a request requires the server to only operate on the resource if it does not match any of the provided entity-tags. If the provided entity-tag is `*`, it is required that the resource does not exist at all. in: header type: string required: false example: \"7da7a728-f910-11e6-942a-68f728c1ba70\", *","title":"MAY: Consider using ETag together with If-(None-)Match header"},{"location":"clients/","text":"Contract-First (Client) \u00b6 Based on this contract, the consumer can already start implementing a solution before having access to the API. Resource Identifiers \u00b6 When using URIs in clients: DO update local copies of old URIs when receiving 301 Moved Permanently . DO verify that the Location URI maps to a trusted server. DO NOT disable support of redirects in client applications. Instead, consider a sensible limit on the number of redirects a client can follow. Disabling redirects altogether will break the client when the server change URIs. Methods \u00b6 When using methods in clients: DO treat GET , OPTIONS , and HEAD as safe operations, and send as required. Include If-Unmodified-Since and/or If-Match conditional headers, if applicable. DO resubmit GET , PUT , and DELETE requests, in case of failures, to confirm. DO implement retry logic, whenever you encounter a failure for an idempotent method. DO submit a POST request with a representation of the resource to be created to the factory resource. DO NOT repeat POST requests unless the documentation states that is idempotent. DO NOT treat various HTTP-level errors as failures (network or software) or like exceptions. Representations \u00b6 When using HTTP Headers in clients: DO return 400 Bad Request on servers, when you receive a representation with no Content-Type . Avoid guessing the type of the representation. Does it belong here? DO let your network library deal with uncompressing compressed representations ( Content-Encoding ) DO read and store the value of Content-Language . DO NOT use Content-Encoding in HTTP requests, unless you know out of band that the target server supports a particular encoding method. DO NOT check for the presence of the Content-Length header without first confirming the absence of Transfer-Encoding: chunked Errors \u00b6 Even though they might not be documented - they may very much occur in production! Clients should be prepared for unexpected response codes. In case of doubt handle them like they would handle the corresponding x00 code. Adding new response codes (specially error responses) should be considered a compatible API evolution. When treating errors in clients: DO treat 400 Bad Request by looking into the body of the error representation for hints for the root cause of the problem. DO retry the request on 401 Unauthorized responses with the Authorization header containing the credentials. If the client is user-facing, prompt the user to supply credentials. In other cases, obtain the necessary security credentials. DO clean up client stored data on 404 Not Found . DO look for the Allow header for valid methods on 405 Not Allowed . DO treat 406 Not Acceptable . See Content Negotiation . DO treat 409 Conflict by looking for the conflicts listed in the body of the representation. DO treat 410 Gone the same as 404 Not Found . DO treat 412 Precondition Failed . See Caching . DO look for hints on valid size in the body of the error on 413 Request Entity Too Large . DO see the body of the representation to learn the supported media types for the request on 415 Unsupported Media Type . DO log the error, and then notify the server developers of 500 Internal Server Error . DO check the response of 503 Service Unavailable for a Retry-After header, avoid retrying until that period of time (back-off logic). DO NOT repeat the request on 403 Forbidden . DO NOT treat HTTP errors as I/O or network exceptions. Treat them as first-class application objects. Hypermedia \u00b6 When using links in clients: DO extract URIs and URI templates from links based on known link relation types. These links along with other resource data constitute the current state of the application. DO store the URIs and the relation type along with other representation data, if the application is long running. DO make flow decisions based on the presence or absence of links. DO store the knowledge of whether a representation contains a given link. DO check the documentation of the link relation to learn any associated business rules regarding authentication, permanence of the URI, methods, and media types supported, etc. Content Negotiation \u00b6 DO add an Accept header with a comma-separated list of media type preferences. If the client prefers one media type over the other, add a q parameter with each media type. DO add *;q=0.0 in the Accept header to indicate to the server it cannot process anything other than the media types listed in the Accept header, if the client can process only certain formats. DO add an Accept-Language header for the preferred language of the representation. DO add an Accept-Charset header with the preferred character set, if the client can process characters of a specific character set only, if not, avoid adding this header. DO add an Accept-Encoding header listing the supported encodings, if the client is able to decompress representations compressed encoding such as gzip , compress , or deflate , if not, skip this header. DO NOT assume that servers support content negotiation, and be prepared to receive a representation that does not meet the Accept-* headers. Extensibility \u00b6 When implementing clients to support extensibility: DO store everything, if the client is capable of storing the complete representation locally. DO NOT assume that the representation is of a fixed media type, character encoding, content language, or content encoding. Complement with an SDK \u00b6 It's a common question for API providers - do you need to complement your API with code libraries and software development kits (SDKs)? If your API follows good design practices, is self consistent, standards-based, and well documented, developers may be able to get rolling without a client SDK. Well-documented code samples are also a critical resource. On the other hand, what about the scenario in which building a UI requires a lot of domain knowledge? This can be a challenging problem for developers even when building UI and apps on top of APIs with pretty simple domains \u2013 think about the Twitter API with it's primary object of 140 characters of text. You shouldn't change your API to try to overcome the domain knowledge hurdle. Instead, you can complement your API with code libraries and a software development kit (SDK). In this way, you don't overburden your API design. Often, a lot of what's needed is on the client side and you can push that burden to an SDK. The SDK can provide the platform-specific code, which developers use in their apps to invoke API operations - meaning you keep your API clean. Other reasons you might consider complementing your API with an SDK include the following: Speed adoption on a specific platform. (For example Objective C SDK for iPhone.) Many experienced developers are just starting off with objective C+ so an SDK might be helpful. Simplify integration effort required to work with your API - If key use cases are complex or need to be complemented by standard on-client processing. An SDK can help reduce bad or inefficient code that might slow down service for everyone. As a developer resource - good SDKs are a forcing function to create good source code examples and documentation. To market your API to a specific community - you upload the SDK to a samples or plugin page on a platform's existing developer community. Last but not least, keep in mind that SDKs or code wrappers/libraries can be extremely helpful. What SDKs/ Code Wrappers offer is a quick, plug and play way for developers to incorporate your API, while also (hopefully) handling error checks/ responses. Building an SDK Doesn't Fix Everything \u00b6 However, if you are building a full out SDK instead of a language wrapper that utilizes the hypermedia to handle responses, remember that you are adding a whole new layer of complexity to your API ecosystem \u2013 that you will have to maintain. The downside is the more complex your SDK becomes, the more tightly coupled it usually is to your API, making any updates to your API a manual and complex process. This means that any new features you roll out will receive rather slow adoption, and you may find yourself providing support to your developers on why they can't do something with your SDK. When building your SDK you should try to keep it as decoupled from your API as possible, relying on dynamic responses and calls while also following the best coding practices for that language (be sure to watch Keith Casey's SPOIL talk or read about it here). Another option is to utilize a SDK building service such as APIMatic.io or REST United, which automatically generates SDKs for your API based on your RAML, Swagger, or API Blueprint spec. This allows you to offer SDKs, automatically have them update when adding new features (although clients will still need to download the updated version), and offer them without adding any additional workload on your end. But again, regardless of whether or not you provide an SDK/ Code Library, you will still want to have multiple code examples in your documentation to help developers who want to utilize your API to its fullest capacity, without relying on additional third party libraries to do so. Remember that having an SDK doesn't replace documentation, if anything \u2013 it creates the need for more.","title":"Clients"},{"location":"clients/#contract-first-client","text":"Based on this contract, the consumer can already start implementing a solution before having access to the API.","title":"Contract-First (Client)"},{"location":"clients/#resource-identifiers","text":"When using URIs in clients: DO update local copies of old URIs when receiving 301 Moved Permanently . DO verify that the Location URI maps to a trusted server. DO NOT disable support of redirects in client applications. Instead, consider a sensible limit on the number of redirects a client can follow. Disabling redirects altogether will break the client when the server change URIs.","title":"Resource Identifiers"},{"location":"clients/#methods","text":"When using methods in clients: DO treat GET , OPTIONS , and HEAD as safe operations, and send as required. Include If-Unmodified-Since and/or If-Match conditional headers, if applicable. DO resubmit GET , PUT , and DELETE requests, in case of failures, to confirm. DO implement retry logic, whenever you encounter a failure for an idempotent method. DO submit a POST request with a representation of the resource to be created to the factory resource. DO NOT repeat POST requests unless the documentation states that is idempotent. DO NOT treat various HTTP-level errors as failures (network or software) or like exceptions.","title":"Methods"},{"location":"clients/#representations","text":"When using HTTP Headers in clients: DO return 400 Bad Request on servers, when you receive a representation with no Content-Type . Avoid guessing the type of the representation. Does it belong here? DO let your network library deal with uncompressing compressed representations ( Content-Encoding ) DO read and store the value of Content-Language . DO NOT use Content-Encoding in HTTP requests, unless you know out of band that the target server supports a particular encoding method. DO NOT check for the presence of the Content-Length header without first confirming the absence of Transfer-Encoding: chunked","title":"Representations"},{"location":"clients/#errors","text":"Even though they might not be documented - they may very much occur in production! Clients should be prepared for unexpected response codes. In case of doubt handle them like they would handle the corresponding x00 code. Adding new response codes (specially error responses) should be considered a compatible API evolution. When treating errors in clients: DO treat 400 Bad Request by looking into the body of the error representation for hints for the root cause of the problem. DO retry the request on 401 Unauthorized responses with the Authorization header containing the credentials. If the client is user-facing, prompt the user to supply credentials. In other cases, obtain the necessary security credentials. DO clean up client stored data on 404 Not Found . DO look for the Allow header for valid methods on 405 Not Allowed . DO treat 406 Not Acceptable . See Content Negotiation . DO treat 409 Conflict by looking for the conflicts listed in the body of the representation. DO treat 410 Gone the same as 404 Not Found . DO treat 412 Precondition Failed . See Caching . DO look for hints on valid size in the body of the error on 413 Request Entity Too Large . DO see the body of the representation to learn the supported media types for the request on 415 Unsupported Media Type . DO log the error, and then notify the server developers of 500 Internal Server Error . DO check the response of 503 Service Unavailable for a Retry-After header, avoid retrying until that period of time (back-off logic). DO NOT repeat the request on 403 Forbidden . DO NOT treat HTTP errors as I/O or network exceptions. Treat them as first-class application objects.","title":"Errors"},{"location":"clients/#hypermedia","text":"When using links in clients: DO extract URIs and URI templates from links based on known link relation types. These links along with other resource data constitute the current state of the application. DO store the URIs and the relation type along with other representation data, if the application is long running. DO make flow decisions based on the presence or absence of links. DO store the knowledge of whether a representation contains a given link. DO check the documentation of the link relation to learn any associated business rules regarding authentication, permanence of the URI, methods, and media types supported, etc.","title":"Hypermedia"},{"location":"clients/#content-negotiation","text":"DO add an Accept header with a comma-separated list of media type preferences. If the client prefers one media type over the other, add a q parameter with each media type. DO add *;q=0.0 in the Accept header to indicate to the server it cannot process anything other than the media types listed in the Accept header, if the client can process only certain formats. DO add an Accept-Language header for the preferred language of the representation. DO add an Accept-Charset header with the preferred character set, if the client can process characters of a specific character set only, if not, avoid adding this header. DO add an Accept-Encoding header listing the supported encodings, if the client is able to decompress representations compressed encoding such as gzip , compress , or deflate , if not, skip this header. DO NOT assume that servers support content negotiation, and be prepared to receive a representation that does not meet the Accept-* headers.","title":"Content Negotiation"},{"location":"clients/#extensibility","text":"When implementing clients to support extensibility: DO store everything, if the client is capable of storing the complete representation locally. DO NOT assume that the representation is of a fixed media type, character encoding, content language, or content encoding.","title":"Extensibility"},{"location":"clients/#complement-with-an-sdk","text":"It's a common question for API providers - do you need to complement your API with code libraries and software development kits (SDKs)? If your API follows good design practices, is self consistent, standards-based, and well documented, developers may be able to get rolling without a client SDK. Well-documented code samples are also a critical resource. On the other hand, what about the scenario in which building a UI requires a lot of domain knowledge? This can be a challenging problem for developers even when building UI and apps on top of APIs with pretty simple domains \u2013 think about the Twitter API with it's primary object of 140 characters of text. You shouldn't change your API to try to overcome the domain knowledge hurdle. Instead, you can complement your API with code libraries and a software development kit (SDK). In this way, you don't overburden your API design. Often, a lot of what's needed is on the client side and you can push that burden to an SDK. The SDK can provide the platform-specific code, which developers use in their apps to invoke API operations - meaning you keep your API clean. Other reasons you might consider complementing your API with an SDK include the following: Speed adoption on a specific platform. (For example Objective C SDK for iPhone.) Many experienced developers are just starting off with objective C+ so an SDK might be helpful. Simplify integration effort required to work with your API - If key use cases are complex or need to be complemented by standard on-client processing. An SDK can help reduce bad or inefficient code that might slow down service for everyone. As a developer resource - good SDKs are a forcing function to create good source code examples and documentation. To market your API to a specific community - you upload the SDK to a samples or plugin page on a platform's existing developer community. Last but not least, keep in mind that SDKs or code wrappers/libraries can be extremely helpful. What SDKs/ Code Wrappers offer is a quick, plug and play way for developers to incorporate your API, while also (hopefully) handling error checks/ responses.","title":"Complement with an SDK"},{"location":"clients/#building-an-sdk-doesnt-fix-everything","text":"However, if you are building a full out SDK instead of a language wrapper that utilizes the hypermedia to handle responses, remember that you are adding a whole new layer of complexity to your API ecosystem \u2013 that you will have to maintain. The downside is the more complex your SDK becomes, the more tightly coupled it usually is to your API, making any updates to your API a manual and complex process. This means that any new features you roll out will receive rather slow adoption, and you may find yourself providing support to your developers on why they can't do something with your SDK. When building your SDK you should try to keep it as decoupled from your API as possible, relying on dynamic responses and calls while also following the best coding practices for that language (be sure to watch Keith Casey's SPOIL talk or read about it here). Another option is to utilize a SDK building service such as APIMatic.io or REST United, which automatically generates SDKs for your API based on your RAML, Swagger, or API Blueprint spec. This allows you to offer SDKs, automatically have them update when adding new features (although clients will still need to download the updated version), and offer them without adding any additional workload on your end. But again, regardless of whether or not you provide an SDK/ Code Library, you will still want to have multiple code examples in your documentation to help developers who want to utilize your API to its fullest capacity, without relying on additional third party libraries to do so. Remember that having an SDK doesn't replace documentation, if anything \u2013 it creates the need for more.","title":"Building an SDK Doesn't Fix Everything"},{"location":"content-negotiation/","text":"Content Negotiation (conneg) \u00b6 There are two types of content negotiation: Server-driven negotiation, uses request HTTP headers to select a variant. Agent-driven negotiation, uses a distinct URI for each variant. Server-Driven Negotiation \u00b6 DO support multiple variants only when your clients need them. DO support multiple variants only when each variant contains the same information. DO NOT use server-driven negotiation when the information content is different, use Agent-Driven Content Negotiation instead. Indicating Client Preferences It is important for the client to indicate its preferences and capabilities to the server, including: representation formats languages character encoding support for compression. Even if you know this information out of band, clearly indicating the client's preferences and capabilities can help in the face of change. It is better to ask for a specific representation instead of getting a default one, because the default can change. Content Type Negotiation \u00b6 content negotiation (include or exclude and force to e.g. json)? DO return a representation using the default format, if the request has no Accept header. DO parse the Accept header, sort the values of media types by the q parameters in descending order, if the request has an Accept header, then select a media type from the list that the server supports. DO return a representation using the default format, if the server does not support any format in the list, and the Accept header does not contain *;q=0.0 . Is this correct for the Content-Type HTTP header? DO handle content negotiation failures to determine an appropriate response. CONSIDER including a Vary (see Vary ) response header. CONSIDER using agent-driven negotiation , if the server starts out ignoring Accept , and, for example, returns Content-Type: application/xml for Accept: application/json requests. Later on , adding content negotiation ( Content-Type: application/json ) will likely break compatibility for working clients. Language Negotiation \u00b6 DO return a representation with all human-readable text in a default language, if the request has no Accept-Language header. DO parse the Accept-Language header, sort the languages by the q parameters in descending order, and select the first language in the list that the server can support, if the request has an Accept-Language header DO use a default language for the response, if the server does not support any languages in the list, and the Accept-Language header does not contain *;q=0.0 . CONSIDER including a Vary (see The Vary Header ) response header. Tip This approach is best suited when representations in different languages differ only in terms of the language used for any human-readable text. If the differences between representations are more significant, use other means of localization such as the client's IP address or region/language-specific URIs. Character Encoding Negotiation \u00b6 DO encode the textual representation of the response using the encoding the client ask for via the Accept-Charset header. Encoding the response using that encoding promotes interoperability. DO parse the header, sort the character set by the q parameters in descending order, and select the first character set that the server can support for encoding. DO return a representation using UTF-8 encoding, if the request has no Accept-Charset header. DO return a representation using UTF-8 encoding, if the server does not support any requested character set and the Accept-Charset header does not contain *;q=0.0 . DO include the charset parameter in the Content-Type header, in all cases, if the media type is textual and allows a charset parameter. CONSIDER including a Vary (see The Vary Header ) response header. AVOID using text/xml since its default encoidng is us-ascii . Supporting Compression \u00b6 To support compression or content encoding : DO use the compression technique from the Accept-Encoding header, if the server is capable of compressing the response body. DO parse the header, sort the character set by the q parameters in descending order, and select the first content encoding the server supports. DO ignore this header, if no encoding in this header matches the server's supported encodings. CONSIDER including a Vary (see The Vary Header ) response header. CONSIDER brotli ( br ). DO NOT compress representations, if the request has no Accept-Encoding header. The Vary Header \u00b6 When a server uses content negotiation to select a representation, the same URI can yield different representations based on Accept-* headers. The Vary header tells clients which request headers the server used when selecting a representation. Caches may use the Vary header as part of cache keys to maintain variants of a resource. CONSIDER including a Vary header whenever multiple representations are available for a resource. The value of this header is a comma-separated list of request headers the server used when choosing representation. CONSIDER including a Vary header with a value of * , if the server uses information other than the headers in the request, such as a client's IP address, time of day, user personalization, etc. Negotiation Failures \u00b6 DO return 406 Not Acceptable with either the body of the representation containing the list of representations, or a Link in the header, when the server cannot serve a representation that meets the client's preferences and if the client explicitly included a *;q=0.0 . DO serve the representation without applying any content encoding, if the server is unable to support the requested Accept-Encoding value. Warning Servers are free to serve any available representation for a given resource. However, clients may not be able to handle arbitrary media types. Agent-Driven Content Negotiation \u00b6 Although server-driven content negotiation is built into HTTP it does not include elements such as currency units, distance units, date formats, and other regional flavors for any human-readable text in representations. In some cases, because of complex localization requirements, the server may decide to maintain different resources for different locales. Info Agent-driven negotiation simply mean providing distinct URIs for each variant and allow the client to use that URI to select the desired representation. DO use out-of-band information from the server to determine which URI to use. If the representation exists, the server returns it, if not, it return 404 Not Found . CONSIDER using agent-driven negotiation, when the client cannot communicate its preferences using Accept-* headers, using one or more of these common approaches: Query parameters, like https://www.example.org/status?format={format} URI extensions, appending a dot ( . ) and a shorthand media type to the base URI. Like https://www.example.org/status.json Subdomains, like https://dk.example.org/status CONSIDER letting the server advertise alternatives using links with the alternate link relation type. Although it is possible to implement agent-driven negotiation for all Accept-* headers, in practice it is most commonly used for media types and languages.","title":"Content Negotiation"},{"location":"content-negotiation/#content-negotiation-conneg","text":"There are two types of content negotiation: Server-driven negotiation, uses request HTTP headers to select a variant. Agent-driven negotiation, uses a distinct URI for each variant.","title":"Content Negotiation (conneg)"},{"location":"content-negotiation/#server-driven-negotiation","text":"DO support multiple variants only when your clients need them. DO support multiple variants only when each variant contains the same information. DO NOT use server-driven negotiation when the information content is different, use Agent-Driven Content Negotiation instead. Indicating Client Preferences It is important for the client to indicate its preferences and capabilities to the server, including: representation formats languages character encoding support for compression. Even if you know this information out of band, clearly indicating the client's preferences and capabilities can help in the face of change. It is better to ask for a specific representation instead of getting a default one, because the default can change.","title":"Server-Driven Negotiation"},{"location":"content-negotiation/#content-type-negotiation","text":"content negotiation (include or exclude and force to e.g. json)? DO return a representation using the default format, if the request has no Accept header. DO parse the Accept header, sort the values of media types by the q parameters in descending order, if the request has an Accept header, then select a media type from the list that the server supports. DO return a representation using the default format, if the server does not support any format in the list, and the Accept header does not contain *;q=0.0 . Is this correct for the Content-Type HTTP header? DO handle content negotiation failures to determine an appropriate response. CONSIDER including a Vary (see Vary ) response header. CONSIDER using agent-driven negotiation , if the server starts out ignoring Accept , and, for example, returns Content-Type: application/xml for Accept: application/json requests. Later on , adding content negotiation ( Content-Type: application/json ) will likely break compatibility for working clients.","title":"Content Type Negotiation"},{"location":"content-negotiation/#language-negotiation","text":"DO return a representation with all human-readable text in a default language, if the request has no Accept-Language header. DO parse the Accept-Language header, sort the languages by the q parameters in descending order, and select the first language in the list that the server can support, if the request has an Accept-Language header DO use a default language for the response, if the server does not support any languages in the list, and the Accept-Language header does not contain *;q=0.0 . CONSIDER including a Vary (see The Vary Header ) response header. Tip This approach is best suited when representations in different languages differ only in terms of the language used for any human-readable text. If the differences between representations are more significant, use other means of localization such as the client's IP address or region/language-specific URIs.","title":"Language Negotiation"},{"location":"content-negotiation/#character-encoding-negotiation","text":"DO encode the textual representation of the response using the encoding the client ask for via the Accept-Charset header. Encoding the response using that encoding promotes interoperability. DO parse the header, sort the character set by the q parameters in descending order, and select the first character set that the server can support for encoding. DO return a representation using UTF-8 encoding, if the request has no Accept-Charset header. DO return a representation using UTF-8 encoding, if the server does not support any requested character set and the Accept-Charset header does not contain *;q=0.0 . DO include the charset parameter in the Content-Type header, in all cases, if the media type is textual and allows a charset parameter. CONSIDER including a Vary (see The Vary Header ) response header. AVOID using text/xml since its default encoidng is us-ascii .","title":"Character Encoding Negotiation"},{"location":"content-negotiation/#supporting-compression","text":"To support compression or content encoding : DO use the compression technique from the Accept-Encoding header, if the server is capable of compressing the response body. DO parse the header, sort the character set by the q parameters in descending order, and select the first content encoding the server supports. DO ignore this header, if no encoding in this header matches the server's supported encodings. CONSIDER including a Vary (see The Vary Header ) response header. CONSIDER brotli ( br ). DO NOT compress representations, if the request has no Accept-Encoding header.","title":"Supporting Compression"},{"location":"content-negotiation/#the-vary-header","text":"When a server uses content negotiation to select a representation, the same URI can yield different representations based on Accept-* headers. The Vary header tells clients which request headers the server used when selecting a representation. Caches may use the Vary header as part of cache keys to maintain variants of a resource. CONSIDER including a Vary header whenever multiple representations are available for a resource. The value of this header is a comma-separated list of request headers the server used when choosing representation. CONSIDER including a Vary header with a value of * , if the server uses information other than the headers in the request, such as a client's IP address, time of day, user personalization, etc.","title":"The Vary Header"},{"location":"content-negotiation/#negotiation-failures","text":"DO return 406 Not Acceptable with either the body of the representation containing the list of representations, or a Link in the header, when the server cannot serve a representation that meets the client's preferences and if the client explicitly included a *;q=0.0 . DO serve the representation without applying any content encoding, if the server is unable to support the requested Accept-Encoding value. Warning Servers are free to serve any available representation for a given resource. However, clients may not be able to handle arbitrary media types.","title":"Negotiation Failures"},{"location":"content-negotiation/#agent-driven-content-negotiation","text":"Although server-driven content negotiation is built into HTTP it does not include elements such as currency units, distance units, date formats, and other regional flavors for any human-readable text in representations. In some cases, because of complex localization requirements, the server may decide to maintain different resources for different locales. Info Agent-driven negotiation simply mean providing distinct URIs for each variant and allow the client to use that URI to select the desired representation. DO use out-of-band information from the server to determine which URI to use. If the representation exists, the server returns it, if not, it return 404 Not Found . CONSIDER using agent-driven negotiation, when the client cannot communicate its preferences using Accept-* headers, using one or more of these common approaches: Query parameters, like https://www.example.org/status?format={format} URI extensions, appending a dot ( . ) and a shorthand media type to the base URI. Like https://www.example.org/status.json Subdomains, like https://dk.example.org/status CONSIDER letting the server advertise alternatives using links with the alternate link relation type. Although it is possible to implement agent-driven negotiation for all Accept-* headers, in practice it is most commonly used for media types and languages.","title":"Agent-Driven Content Negotiation"},{"location":"documentation-and-discovery/","text":"Documentation and Discovery \u00b6 ERRORS \u00b6 Service providers should differentiate between technical and functional errors. In most cases it's not useful to document technical errors that are not in control of the service provider unless the status code convey application-specific semantics. Functional errors on the other hand, that convey domain-specific semantics, must be documented and are strongly encouraged to be expressed with Problem types. SHOULD:: Provide User Manual Documentation \u00b6 In addition to the API as OpenAPI Reference Definition, it's good practice to provide an API User Manual documentation to improve client developer experience, especially of engineers that are less experienced in using this API. A helpful API User Manual typically describes the following API aspects: API's scope, purpose and use cases concrete examples of API usage edge cases, error situation details and repair hints architecture context and major dependencies - including figures and sequence flows how to document api\u2019s swagger for internal developers (readme.md in the repo) API documentation ( https://apihandyman.io/the-data-the-hypermedia-and-the-documentation/ ) Machine readable documentation RAML, Swagger and Blueprint. None of them, for now, handles hypermedia APIs definition. ALPS Human readable documentation Documentation: https://blog.smartbear.com/documentation/the-utopia-of-api-documentation/ For some more or less agreed-upon qualities of good API documentation. It must be: adapted for audience \u2014 like all good marketing and customer support, perhaps multiple documentation depending on the audience\u2019s needs DX-first \u2014 made for humans, by humans machine-readable Google-readable \u2014 search engine optimization matters when most people are typing \u201cX API\u201d into Google well-organized like a reference guide or table of contents entwined with the API itself \u2014 dual-screens or opening in new window, allowing users to try something out right away not a burden to create with pricing and usage policies with contact information adapted to the learner or user riddled with use cases and code examples made up of everything you could need to use the API paired with a story \u2014 why you are doing this to achieve that easy to produce, publish and maintain adapted to what kind of software is being documented, like SaaS versus platform adapted to audience to the people that will use it \u2014 end user versus inside your company adapted to context \u2014 when in the discovery process and how people will use it equipped with some sort of way to collect user feedback on how you can further improve it easily found, whether within the developer portal or prominently placed on your website \u201cIf all your APIs are true REST APIs and you always them design them the same way, you lessen the need for documentation. If you write documentation using command and shared structures, templates, and common and shared vocabulary and concepts, they become easier to write, read, understand.\u201d Documentation and its subjects are analyzed to check that they are consistent with each other. For example, if you have an API descriptor, the system checks that the API is conforming to that descriptor. This already exists with ReadyAPI from SmartBear. You can take an API descriptor in Swagger, and ReadyAPI will create the basic testing to check that the implementation for the API is correct compared to the API descriptor,\u201d remember that Swagger isn\u2019t the final piece of the puzzle. It\u2019ll get down your specs and build the perimeter of your API, but Swagger alone does not make complete API documentation. While formats like Swagger and RAML can automate the raw specification, you can also try a tool like LucyBot, to make Swagger more human-readable. An API is only as good as its documentation. The docs should be easy to find and publically accessible. When the docs are hidden inside a PDF file or require signing in, they're not only difficult to find but also not easy to search. The docs should show examples of complete request/response cycles. (Pastable examples) The documentation must include any deprecation schedules and details surrounding externally visible API updates. Updates should be delivered via a blog (i.e. a changelog) or a mailing list (preferably both!). When building RESTful web services, you need to address two kinds of discoverability. These are design-time dicoverability and runtime discoverability. Design-time discoverability helps others design and build clients. It describes all the essentials that client developer teams and administrators need to know in order to build and launch clients. Runtime discoverability helps maintain loose coupling between clients and severs and enables plung-and-play style automation. Runtime discovery invlolves HTTP's uniforma interface, mean types, links, and link relations. This chapter is about design-time discoverability. Design-time dicoverability simply mean describing you web service in prose, whether such prove is generated by some tools or created manually by the designers or developers of the web service. Client developers can consult this prose to understand the \"semantics\" of the resources, media types, link relations, and so on, and implement clients. The best way to promote design- and development-time dicoverability is to unambiguously document the information needed to implement clients. Fully describe the following in human-readable documentation: All resources and methods supported for each resource. Media types and representation formats for resources in requests and responses. Each link relation used, its business significance, HTTP method to be used, and resource that the link identifies. All fixed URIs that are not supported via links. Query parameters used for all fixed URIs. URI templates and token substitution rules. Authenticaton and security credentials for accessing resources. For XML representations, if your clients and servers are capable of supporting XML schemas, use a schema language as a \"convention\" to describe the structure of XML documents used for representation un requests and responses. For other formats, use conventions to describe representations in prose. No machine-readable description can replace human-readable documentation. Documenting your web service in human-readble format such as HTML is the most useful way to enable design-time dicovery. When documenting your service, include all the information necessary to implement a client.","title":"Documentation and Discovery"},{"location":"documentation-and-discovery/#documentation-and-discovery","text":"","title":"Documentation and Discovery"},{"location":"documentation-and-discovery/#errors","text":"Service providers should differentiate between technical and functional errors. In most cases it's not useful to document technical errors that are not in control of the service provider unless the status code convey application-specific semantics. Functional errors on the other hand, that convey domain-specific semantics, must be documented and are strongly encouraged to be expressed with Problem types.","title":"ERRORS"},{"location":"documentation-and-discovery/#should-provide-user-manual-documentation","text":"In addition to the API as OpenAPI Reference Definition, it's good practice to provide an API User Manual documentation to improve client developer experience, especially of engineers that are less experienced in using this API. A helpful API User Manual typically describes the following API aspects: API's scope, purpose and use cases concrete examples of API usage edge cases, error situation details and repair hints architecture context and major dependencies - including figures and sequence flows how to document api\u2019s swagger for internal developers (readme.md in the repo) API documentation ( https://apihandyman.io/the-data-the-hypermedia-and-the-documentation/ ) Machine readable documentation RAML, Swagger and Blueprint. None of them, for now, handles hypermedia APIs definition. ALPS Human readable documentation Documentation: https://blog.smartbear.com/documentation/the-utopia-of-api-documentation/ For some more or less agreed-upon qualities of good API documentation. It must be: adapted for audience \u2014 like all good marketing and customer support, perhaps multiple documentation depending on the audience\u2019s needs DX-first \u2014 made for humans, by humans machine-readable Google-readable \u2014 search engine optimization matters when most people are typing \u201cX API\u201d into Google well-organized like a reference guide or table of contents entwined with the API itself \u2014 dual-screens or opening in new window, allowing users to try something out right away not a burden to create with pricing and usage policies with contact information adapted to the learner or user riddled with use cases and code examples made up of everything you could need to use the API paired with a story \u2014 why you are doing this to achieve that easy to produce, publish and maintain adapted to what kind of software is being documented, like SaaS versus platform adapted to audience to the people that will use it \u2014 end user versus inside your company adapted to context \u2014 when in the discovery process and how people will use it equipped with some sort of way to collect user feedback on how you can further improve it easily found, whether within the developer portal or prominently placed on your website \u201cIf all your APIs are true REST APIs and you always them design them the same way, you lessen the need for documentation. If you write documentation using command and shared structures, templates, and common and shared vocabulary and concepts, they become easier to write, read, understand.\u201d Documentation and its subjects are analyzed to check that they are consistent with each other. For example, if you have an API descriptor, the system checks that the API is conforming to that descriptor. This already exists with ReadyAPI from SmartBear. You can take an API descriptor in Swagger, and ReadyAPI will create the basic testing to check that the implementation for the API is correct compared to the API descriptor,\u201d remember that Swagger isn\u2019t the final piece of the puzzle. It\u2019ll get down your specs and build the perimeter of your API, but Swagger alone does not make complete API documentation. While formats like Swagger and RAML can automate the raw specification, you can also try a tool like LucyBot, to make Swagger more human-readable. An API is only as good as its documentation. The docs should be easy to find and publically accessible. When the docs are hidden inside a PDF file or require signing in, they're not only difficult to find but also not easy to search. The docs should show examples of complete request/response cycles. (Pastable examples) The documentation must include any deprecation schedules and details surrounding externally visible API updates. Updates should be delivered via a blog (i.e. a changelog) or a mailing list (preferably both!). When building RESTful web services, you need to address two kinds of discoverability. These are design-time dicoverability and runtime discoverability. Design-time discoverability helps others design and build clients. It describes all the essentials that client developer teams and administrators need to know in order to build and launch clients. Runtime discoverability helps maintain loose coupling between clients and severs and enables plung-and-play style automation. Runtime discovery invlolves HTTP's uniforma interface, mean types, links, and link relations. This chapter is about design-time discoverability. Design-time dicoverability simply mean describing you web service in prose, whether such prove is generated by some tools or created manually by the designers or developers of the web service. Client developers can consult this prose to understand the \"semantics\" of the resources, media types, link relations, and so on, and implement clients. The best way to promote design- and development-time dicoverability is to unambiguously document the information needed to implement clients. Fully describe the following in human-readable documentation: All resources and methods supported for each resource. Media types and representation formats for resources in requests and responses. Each link relation used, its business significance, HTTP method to be used, and resource that the link identifies. All fixed URIs that are not supported via links. Query parameters used for all fixed URIs. URI templates and token substitution rules. Authenticaton and security credentials for accessing resources. For XML representations, if your clients and servers are capable of supporting XML schemas, use a schema language as a \"convention\" to describe the structure of XML documents used for representation un requests and responses. For other formats, use conventions to describe representations in prose. No machine-readable description can replace human-readable documentation. Documenting your web service in human-readble format such as HTML is the most useful way to enable design-time dicovery. When documenting your service, include all the information necessary to implement a client.","title":"SHOULD:: Provide User Manual Documentation"},{"location":"examples/","text":"Examples \u00b6 Here is some incorrect Markdown. I am adding this here . Here is some more text that I am removing text. And here is even more text that I am adding. Paragraph was deleted and replaced with some spaces. Spaces were removed and a paragraph was added. And here is a comment on some text This works quite well. I just wanted to comment on it. . Substitutions is are great! General block handling. test remove test remove test remove test remove test remove test add test add test add test add test add H 2 O H this is a test Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Note this is a test public void readonly static public static readonly string Name = \"NoName\" ; \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz import foo.bar \u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc \u00be 1 st 2 nd 3 rd \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Seealso Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Summary Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tldr Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Todo Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hint Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Important Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Check Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Done Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Help Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Faq Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Caution Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Attention Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Fail Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Missing Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Error Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Snippet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Cite Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Examples"},{"location":"examples/#examples","text":"Here is some incorrect Markdown. I am adding this here . Here is some more text that I am removing text. And here is even more text that I am adding. Paragraph was deleted and replaced with some spaces. Spaces were removed and a paragraph was added. And here is a comment on some text This works quite well. I just wanted to comment on it. . Substitutions is are great! General block handling. test remove test remove test remove test remove test remove test add test add test add test add test add H 2 O H this is a test Lorem ipsum dolor sit amet, consectetur adipiscing elit Nulla lobortis egestas semper Curabitur elit nibh, euismod et ullamcorper at, iaculis feugiat est Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Sed egestas felis quis elit dapibus, ac aliquet turpis mattis Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Nulla vel eros venenatis, imperdiet enim id, faucibus nisi Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Note this is a test public void readonly static public static readonly string Name = \"NoName\" ; \"\"\"Some file.\"\"\" import foo.bar import boo.baz import foo.bar.baz import foo.bar \u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc \u00be 1 st 2 nd 3 rd \"\"\" Bubble sort \"\"\" def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Seealso Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Abstract Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Summary Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tldr Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Todo Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Info Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Hint Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Important Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Success Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Check Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Done Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Help Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Faq Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Warning Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Caution Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Attention Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Failure Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Fail Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Missing Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Danger Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Error Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Bug Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Example Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Snippet Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Quote Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Cite Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Examples"},{"location":"http/","text":"HTTP \u00b6 When designing a RESTful API delivered over HTTP, it is important to maintain a high degree of visibility. This will let the API benefit from existing software and infrastructure for features that you would otherwise have to build yourself. Visibility in the context of HTTP Visibility simply means that one component of an architecture can monitor (and even participate in) the interaction between other components of the same architecture. Luckily HTTP is an application-level protocol designed to keep communication between clients and servers visible. HTTP defines operations for transferring representations between clients and servers, eliminating the need for application-specific operations (e.g. createBooking , changeBooking , etc.). Tip Using the capabilities of HTTP means that caches, proxies, firewalls, etc., can monitor and participate in the protocol. Visibility \u00b6 For better visibility: DO make interactions stateless. DO use HTTP methods. DO use appropriate headers to describe requests and responses. DO use appropriate status codes and messages. DO NOT change syntax and meaning, specified by HTTP, from application to application nor from resource to resource. Warning Keep in mind that focusing solely on visibility may create too fine-grained resources, and poor separation of concern between clients and servers. Trading visibility for other benefits is not necessarily bad. Consider trading visibility: when multiple resources share data when operations modifies multiple resources for better abstraction and loose coupling for network efficiency for better resource granularity or simply for pure client convenience","title":"HTTP"},{"location":"http/#http","text":"When designing a RESTful API delivered over HTTP, it is important to maintain a high degree of visibility. This will let the API benefit from existing software and infrastructure for features that you would otherwise have to build yourself. Visibility in the context of HTTP Visibility simply means that one component of an architecture can monitor (and even participate in) the interaction between other components of the same architecture. Luckily HTTP is an application-level protocol designed to keep communication between clients and servers visible. HTTP defines operations for transferring representations between clients and servers, eliminating the need for application-specific operations (e.g. createBooking , changeBooking , etc.). Tip Using the capabilities of HTTP means that caches, proxies, firewalls, etc., can monitor and participate in the protocol.","title":"HTTP"},{"location":"http/#visibility","text":"For better visibility: DO make interactions stateless. DO use HTTP methods. DO use appropriate headers to describe requests and responses. DO use appropriate status codes and messages. DO NOT change syntax and meaning, specified by HTTP, from application to application nor from resource to resource. Warning Keep in mind that focusing solely on visibility may create too fine-grained resources, and poor separation of concern between clients and servers. Trading visibility for other benefits is not necessarily bad. Consider trading visibility: when multiple resources share data when operations modifies multiple resources for better abstraction and loose coupling for network efficiency for better resource granularity or simply for pure client convenience","title":"Visibility"},{"location":"hypermedia/","text":"Hypermedia \u00b6 What is Hypermedia? \u00b6 One of the challenges to implementing and correctly using hypermedia in your REST API is first understanding what hypermedia is, and what it means to use hypermedia as the engine of application state (HATEOAS). The term \"hypermedia\" was coined back in 1965 by Ted Nelson, and over the years has dominated the technology industry. Hypermedia, in its most basic sense is an extension of hypertext \u2013 something you may recognize from HTML. Hypertext is essentially text that is written in a structured format and contains relationships to other objects via links. If you think of a webpage, using HTML \u2013 the Hypertext Markup Language \u2013 text is then interpreted by a browser to become a webpage, or an interactive environment capable of doing more than just providing a blob of text. However, to be truly interactive and guide the user, the crucial component of all of this is links, something we use everyday when surfing the web. Enter hypermedia, again just an extension of the term hypertext, hypermedia includes images, video, audio, text, and links. In a REST API, this means that your API is able to function similarly to a webpage, providing the user with guidance on what type of content they can retrieve, or what actions they can perform, as well as the appropriate links to do so. Essentially, the easiest way to take advantage of hypermedia in your API is to provide valuable information to direct the user or client to the next possible actions they can take based on the object (whether it be a collection, or item within the resource) or \"page\" they are on via links. Another way to think of it is to bring back the Hypercard application from the early Apple days (we're talking Macintosh Classic). With this application you could create \"cards\" or slides with links that performed different functions. Clicking on link might play a sound, another a video, or another would take you to a different card. On each card you were presented with certain links (whether in the form of a button or text) for the available actions to you on this card. This is exactly how hypertext links work within RESTful APIs\u2013 they are designed to take you to the other components of the API, your other \"cards.\" By adding these links, because REST is stateless, we are providing the client with a method for determining the state of the current item/ collection that they are on, as well as giving them the roadmap or engine for what they can do with it. In other words, we are using Hypermedia as the Engine of the Application's State, or implementing HATEOAS. The HATEOAS Debate \u00b6 Currently, there is a lot of disagreement around hypermedia's place in RESTful APIs\u2013 with Dr. Roy Fielding saying it is critical to REST, and without it your API cannot be RESTful; and hypermedia skeptics arguing that you are doing nothing more than bloating your API and making it more difficult to use under the guise of usability. This means you will find many different stances on the web today, ranging from Peter Williams explanation of how hypermedia has helped him, Jeff Krupp's rant against hypermedia, and even Kin Lane's summary of the whole debate. But what it really comes down to isn't what do others think about hypermedia, as most major frameworks for building APIs now include hypermedia specs such as HAL, JSON-LD, JSON API, Siren, or Collection+JSON, but rather what benefit does hypermedia offer- and is that benefit enough to outweigh the cost/ disadvantages of incorporating hypermedia. The Claims For and Against Hypermedia \u00b6 Some of the most common arguments for and against hypermedia include: Hypermedia creates more work \u00b6 This is absolutely true. Adding hypermedia or hypertext links to your API output does require more work \u2013 and more thought to go into your API. Going back to the Planning Your API post in this series, hypermedia explains how your API works together, what resources work with which, and what you can do with a collection, or specific items in that collection. This will add to both the workload and the time it takes to build your API. However, by having these relationships already drawn out as suggested, the additional time required will be minimal. Hypermedia will require more Data-Transfer \u00b6 Again, absolutely true. By adding the links to your response you are increasing the amount of data that needs to be sent back, and slowly down the responses ever so slightly. However, for MOST APIs the amount of data being added will be minimal and the data/ time difference result will realistically be unnoticeable. What you will do, however, is help avoid unnecessary calls that are not accessible to a particular item, but may be available to other items. See the next section for more on this. You are creating an API for a client that doesn't exist \u00b6 Another argument against APIs is that unlike web browsers, there are no browsers for APIs. Likewise a valid argument, however, one I personally believe falls short as like the web we are seeing advances in APIs and recently an explosion of API exploration tools ranging from API Consoles to the API Notebook. As more and more APIs are developed there will be more and more emphasis on being able to explore them pragmatically. Another reason I believe this argument falls short is that by utilizing hypermedia you allow the developers USING your API to build their systems around the information you provide them, creating a more dynamic application that relies on the available actions YOU return to them instead of creating rigidity by hardcoding what actions the user can take from their application in relation to your API and returning errors when those actions fail because they simply were not available to begin with. For example, for one user you may be able to: Edit Suspend Delete But for another user who has been suspended, maybe you can only: Restore Delete Because this is dynamic data the developer has no way to determine what actions are available and which ones are not, unless they dig through and try to decipher the user's data which has been returned- making assumptions about your architectures rules which may or may not be correct, and may change with future development. Hypertext links in this case allow the developer to rely on YOUR rules and architecture, rather than trying to mimic it with their own. Nobody knows how to use Hypermedia \u00b6 There is some truth into this. Despite existing since 1965, many developers are flustered when first running into hypermedia, choosing to ignore it or misunderstanding how to use it (i.e. hard-coding the links). What can help with this, however, is more APIs taking advantage of hypermedia (as being encouraged in the most popular development frameworks), and simple explanations within documentation explaining how the hypertext links work (just as we had to explain how to do a GET, POST, PUT, PATCH, and DELETE when REST first came out). However, unlike many specs and challenges out there, utilizing hypermedia from a client has a very fast learning curve, and most developers are able to implement it quickly and without any real issue once they understand what it is designed to do. And once implemented correctly, many developers appreciate knowing that as resource URIs change or additional query parameters are required of them, that their application will be able to \"automatically upgrade\" to these new requirements without any work or concern on their part. Hypermedia makes your API MORE Flexible \u00b6 Absolutely true. By adding Hypermedia you are able to add new features more seamlessly- making them immediately available to your users. It also gives you the power to change certain aspects of your API (i.e. changing resources, requiring additional GET parameters) without necessarily breaking backwards compatibility IF implemented correctly by your users. Hypermedia prevents APIs from breaking \u00b6 Absolutely false. As mentioned above, Hypermedia does make your API more flexible, but does not excuse poor design or allow you to break backwards compatibility. One of the reasons for this, is even if you have a purely hypermedia driven API, such as Stormpath's, developers will still hardcode certain URLs to avoid having to make multiple calls. For example, they may access the /users resource directly to get a list of all users instead of starting from the gateway or entry point of your API and working up to that point through multiple calls. While this technique does save you (and them) multiple API calls, it does limit your ability to change common or most frequently used resource URIs. Hypertext links also do nothing for backwards incompatible changes in data\u2013 although you could hypothetically use them as a form of versioning. Hypermedia replaces Documentation \u00b6 Hypermedia does increase discoverability of your API's resources and methods, however, it does not replace documentation as it doesn't provide method information, schemas, or examples. Documentation also serves as a preview to your API, letting developers understand how it works, and potentially make sample calls using an API Console before implementing it into their application. Hypermedia also does not play a solid role in debugging the implementation of the API when things go wrong. For this reason, having well written, informative documentation is vital to any API. In fact, many RESTful hypermedia specs including HAL grant you the ability to embed documentation links for quick reference by developers. Hypermedia is a Best Practice \u00b6 Where hypermedia shines is in its ability to create a flexible API that provides dynamic data for developers based on your architecture and not their own. In essence, it provides a shortcut for developers that allows them to utilize your API to its fullest without having to rely solely on documentation and writing rules that may or may not not be consistent with those in your application. Like when building a simple website, one may not see the advantage of Hypermedia right away, but as your API grows and becomes more complex you will find that it becomes a powerful convenience layer, one that will help developers better understand and navigate your API, and one that may prevent you from having to version your API for certain changes that would otherwise be backwards incompatible \u2014 if implemented correctly by your users. As Peter Williams explains in his post, hypermedia turned out to be a saving grace. And as a key component of REST as defined by Dr. Roy Fielding, it remains a best practice to implement. In Summary \u00b6 Hypermedia is often misunderstood in regards to APIs, but essentially it functions exactly like links on a webpage. And while the technology is both praised and criticized, it does provide an array of short and long-term gains. These gains, I believe, become more and more noticeable the larger and more complex your API becomes. However, the best features of utilizing HATEOAS, in my opinion, are yet to be seen as technology and API exploration tools continue to advance. Be sure to join us next week as we take a look at implementing hypermedia into your API and the current specs out there to help you do so. The Harsh Reality of the State of Hypermedia Specs \u00b6 Hypermedia sounds great in theory, but theory only goes so far. Where hypermedia really shines, or completely fails, is in implementation. Unfortunately, as hypermedia is still a relatively new aspect of web based APIs, there isn't one specified way of doing things. In fact, you'll find that even some of the most popular APIs operate completely differently from each other. After all, there are several different hypermedia formats available for API providers to choose from. Just for starters there is HAL, Collection+JSON, JSON-LD, JSON API, and Siren! But the list doesn't stop there, as some popular APIs have even elected to create their own format. For example, while PayPal's API closely mimics the JSON API format, it goes a step further and adds a method property (not part of the JSON API spec), creating a more flexible spec and transforming it from being resource driven to being action driven: This has the potential to let developers create a more agile client based on the actions (and methods) available to them. However, for developers not familiar with PayPal's format, but familiar with JSON API this may cause slight confusion (although it should be quickly remedied by reading their docs). VerticalResponse, on the other hand, has taken a different, albeit interesting approach. For their API they likewise start with the basic JSON API format, but for some reason decided against the universally accepted \"href\" or Hypertext Reference property, instead opting to use \"url\" or the uniform resource locator as the link URI identifier: Personally, I would recommend staying with the uniform \"href\" attribute as it denotes a reference to a hypertext link and is not as exclusive as an URL- which is not (although it commonly is) to be confused with URI. But you can read more on that here. On the other hand, Amazon's AppStream API, Clarify, Microsoft's Lync, and FoxyCart all prefer to follow HAL or the Hypertext Application Language format. HAL provides a simple format for nest-able links, but like other specs omits the methods property as included by PayPal, making theirs truly unique in that sense: However FoxyCart takes it one step further, not only taking advantage of hypermedia, but offering multiple formats for their clients to choose from, including HAL+JSON, HAL+XML, and Siren. This, however, highlights once again one of the biggest challenges with hypermedia driven APIs, the abundance of ideas and specs available for execution. While on one-hand I believe that by supporting both XML and JSON, as well as having multiple JSON formats FoxyCart is by far the most flexible (format wise) of the APIs, not having a singular standard for each language does present the challenge of forcing developers (and hypermedia clients) to support multiple formats (as they integrate more and more hypermedia APIs), while also having the understanding that not one format meets every API's needs. The good news, is that despite these growing pains, we are starting to see companies adopting certain specs over others, while also identifying areas for improvement (such as with PayPal's adding of methods to JSON API). Next week we'll take a look at some of the most popular formats out there in-depth, keying in on the strengths and weaknesses of each. But it's important that as you build your API, you understand WHY you are building it the way you are. And this extends into how you build your hypermedia links, and whether or not you choose to take advantage of a standardized format (recommended), or venture off on your own to meet your developers' needs. One of the best ways to do this is to explore what others have done with their APIs, and learn from their successes, and their mistakes. It's also important to consider where technology is going. And as more and more formats become available and change in popularity, it may be smart to follow FoxyCart's lead \u2013 taking advantage of the spec that best meets your developers' needs, but also keeping the link format decoupled enough from your data that you are able to return multiple formats based on the content-type received. Something that will allow you to take advantage of this best practice, while also being prepared for whatever the future may hold. Specs \u00b6 What essentially every hypertext linking spec does provide is a name for the link and a hypertext reference, but outside of that, it's a crapshoot. As such, it's important to understand the different specs that are out there, which ones are leading the industry, and which ones meet your needs. We may not be able to get it down to one spec, but at least we'll be able to provide our users with a uniform response that they can easily incorporate into their application: Collection+JSON \u00b6 Collection+JSON is a JSON-based read/write hypermedia-type designed by Mike Amundsen back in 2011 to support the management and querying of simple collections. It's based on the Atom Publication and Syndication specs, defining both in a single spec and supporting simple queries through the use of templates. While originally widely used among APIs, Collection+JSON has struggled to maintain its popularity against JSON API and HAL. Strengths: strong choice for collections, templated queries, early wide adoption, recognized as a standard Weaknesses: JSON only, lack of identifier for documentation, more complex/ difficult to implement JSON API \u00b6 JSON API is a newer spec created in 2013 by Steve Klabnik and Yahuda Klaz. It was designed to ensure separation between clients and servers (an important aspect of REST) while also minimizing the number of requests without compromising readability, flexibility, or discovery. JSON API has quickly become a favorite receiving wide adoption and is arguably one of the leading specs for JSON based RESTful APIs. JSON API currently bares a warning that it is a work in progress, and while widely adopted not necessarily stable. Strengths: simple versatile format, easy to read/ implement, flat link grouping, URL templating, wide adoption, strong community, recognized as a hypermedia standard Weaknesses: JSON only, lack of identifier for documentation, still a work in progress HAL \u00b6 HAL is an older spec, created in 2011 by Mike Kelly to be easily consumed across multiple formats including XML and JSON. One of the key strengths of HAL is that it is nestable, meaning that _links can be incorporated within each item of a collection. HAL also incorporates CURIEs, a feature that makes it unique in that it allows for inclusion of documentation links in the response \u2013 albeit they are tightly coupled to the link name. HAL is one of the most supported and most widely used hypermedia specs out there today, and is surrounded by a strong and vocal community. Strengths: dynamic, nestable, easy to read/ implement, multi-format, URL templating, inclusion of documentation, wide adoption, strong community, recognized as a standard hypermedia spec, RFC proposed Weaknesses: JSON/XML formats architecturally different, CURIEs are tightly coupled JSON-LD \u00b6 JSON-LD is a lightweight spec focused on machine to machine readable data. Beyond just RESTful APIs, JSON-LD was also designed to be utilized within non-structured or NoSQL databases such as MongoDB or CouchDB. Developed by the W3C JSON-LD Community group, and formally recommended by W3C as a JSON data linking spec in early 2014, the spec has struggled to keep pace with JSON API and HAL. However, it has built a strong community around it with a fairly active mailing list, weekly meetings, and an active IRC channel. Strengths: strong format for data linking, can be used across multiple data formats (Web API & Databases), strong community, large working group, recognized by W3C as a standard Weaknesses: JSON only, more complex to integrate/ interpret, no identifier for documentation Siren \u00b6 Created in 2012 by Kevin Swiber, Siren is a more descriptive spec made up of classes, entities, actions, and links. It was designed specifically for Web API clients in order to communicate entity information, actions for executing state transitions, and client navigation/ discoverability within the API. Siren was also designed to allow for sub-entities or nesting, as well as multiple formats including XML \u2013 although no example or documentation regarding XML usage is provided. Despite being well intentioned and versatile, Siren has struggled to gain the same level of attention as JSON API and HAL. Siren is still listed as a work in progress. Strengths: provides a more verbose spec, query templating, incorporates actions, multi-format Weaknesses: poor adoption, lacks documentation, work in progress Other Specs \u00b6 Along with some of the leading specs mentioned above, new specs are being created every day including UBER, Mason, Yahapi, and CPHL. This presents a very interesting question, and that is are we reinventing the wheel, or is there something missing in the above specs. I believe the answer is a combination of both, with developers being notorious for reinventing the wheel, but also because each developer looks at the strengths and weaknesses of other specs and envisions a better way of doing things. You may recognize this issue from the last post, where some specs were modified by the companies using them to meet their individual needs. For example, PayPal wanted to include methods in their response, but you'll notice that only Siren of the above include methods in the link definition. The Future of Specs \u00b6 Given that new specs are being created every day, each with different ideas and in different formats, it's extremely important to keep your system as decoupled and versatile as possible, and it will be very interesting to see what the future of hypermedia specs will look like. In the mean-time, it's best to choose the spec that meets your needs, while also being recognized as a standard for easy integration by developers. Of the specs above, I would personally recommend sticking with HAL or JSON API, although each has its own strengths and weaknesses, and I believe that universal spec of the future has yet to be created. But by adhering to these common specs while the new specs battle things out, I think we will finally find that standard method of road signs, detours, and a single solution to provide API clients with a standardized GPS system. For more on the different specs, I highly recommend reading Kevin Sookocheff's review. I'd also love to hear your thoughts in the comments below. Linking and Application State \u00b6 <!-- HATEOS / HAL / JSONAPI etc. --> A link provides a mean to navigate from one resource to another. Application state is the state the server needs to maintain between each request for each client. Keeping state in clients does not mean serializing session state into URIs or HTML forms. If the amount of data is small, the best place to maintain application state is within links in representations of resources, where the server can encode the state within the URI itself. However, the server can stores data in a durable storage and encodes its primary key in the URI. Use a combination of both approaches for managing application state to strike a balance between network performance, scalability and reliability. ``` HTTP/1.1 200 OK Content-Type: application/xml;charset=UTF-8 <quote xmlns:atom=\"http://www.w3.org/2005/Atom\"> <driver>...</driver> <vehicle>...</vehicle> <offer> <valid-until>2009-10-02</valid-until> <atom:link href=\"http://www.example.org/quotes/buy?quote=abc1234\" rel=\"http://www.example.org/rels/quotes/buy\" /> </offer> </quote> ``` DO encode application state into URIs, and include those URIs into representations via links. DO store the application state in a durable storage, and encode a reference to that state in URIs, if the state is large or cannot be transported to the clients for security or privacy reasons. DO make sure to add checks (such as signatures) to detect/prevent tampering of state, when using application state in links. #### *Links in XML Representations* *[Atom](http://www.w3.org/2005/Atom)* MUST: Use Common Hypertext Controls \u00b6 When embedding links to other resources into representations you must use the common hypertext control object. It contains at least one attribute: href: The URI of the resource the hypertext control is linking to. All our API are using HTTP(s) as URI scheme. In API that contain any hypertext controls, the attribute name href is reserved for usage within hypertext controls. The schema for hypertext controls can be derived from this model: HttpLink: description: A base type of objects representing links to resources. type: object properties: href: description: Any URI that is using http or https protocol type: string format: uri required: [ \"href\" ] The name of an attribute holding such a HttpLink object specifies the relation between the object that contains the link and the linked resource. Implementations should use names from the IANA Link Relation Registry whenever appropriate. As IANA link relation names use hyphen-case notation, while this guide enforces snake_case notation for attribute names, hyphens in IANA names have to be replaced with underscores (e.g. the IANA link relation type version-history would become the attribute version_history) Specific link objects may extend the basic link type with additional attributes, to give additional information related to the linked resource or the relationship between the source resource and the linked one. E.g. a service providing \"Person\" resources could model a person who is married with some other person with a hypertext control that contains attributes which describe the other person (id, name) but also the relationship \"spouse\" between between the two persons (since): { \"id\": \"446f9876-e89b-12d3-a456-426655440000\", \"name\": \"Peter Mustermann\", \"spouse\": { \"href\": \"https://...\", \"since\": \"1996-12-19\", \"id\": \"123e4567-e89b-12d3-a456-426655440000\", \"name\": \"Linda Mustermann\" } } Hypertext controls are allowed anywhere within a JSON model. While this specification would allow HAL, we actually don't recommend/enforce the usage of HAL anymore as the structural separation of meta-data and data creates more harm than value to the understandability and usability of an API. Links in JSON Representations \u00b6 ## What about attribute names? In the previous section, we talked about formats - supporting multiple formats and working with JSON as the default. This time, let's talk about what happens when a response comes back. You have an object with data attributes on it. How should you name the attributes? Here are API responses from a few leading APIs: Twitter \"created_at\": \"Thu Nov 03 05:19;38 +0000 2011\" Bing \"DateTime\": \"2011-10-29T09:35:00Z\" Foursquare \"createdAt\": 1320296464 They each use a different code convention. Although the Twitter approach is familiar to me as a Ruby on Rails developer, we think that Foursquare has the best approach. How does the API response get back in the code? You parse the response (JSON parser); what comes back populates the Object. It looks like this var myObject = JSON.parse(response); If you chose the Twitter or Bing approach, your code looks like this. Its not JavaScript convention and looks weird - looks like the name of another object or class in the system, which is not correct. timing = myObject.created_at; timing - myObject.DateTime; Recommendations * Use JSON as default * Follow JavaScript conventions for naming attributes - Use medial capitalization (aka CamelCase) - Use uppercase or lowercase depending on type of object This results in code that looks like the following, allowing the JavaScript developer to write it in a way that makes sense for JavaScript. \"createdAt\": 1320296464 timing = myObject.createdAt; DO use a link property or a links property to include several links as an array whose value is a link object or a link object array. DO include href and rel properties in each link object Examples: { \"link\" : { \"rel\" : \"alternate\" , \"href\" : \"http://www.example.org/customers?format=json\" } } { \"link\" : { \"alternate\" : \"http://www.example.org/customers?format=json\" } } { \"links\" : [{ \"rel\" : \"alternate\" , \"href\" : \"http://www.example.org/customers?format=json\" }, { \"rel\" : \"http://www.example.org/rels/owner\" , \"href\" : \"...\" }] } { \"links\" : { \"alternate\" : \"http://www.example.org/customers?format=json\" \"http://www.example.org/rels/owner\" : \"http://www.example.org/owner\" } } Link Header \u00b6 The Link header provides a format-independent means to convey links, which is one of the key benefits, along with visibility at the protocol level. Also the need for documentation on how to discover links in XML or JSON representations is lowered. # Link header format Link: <{URI}>;rel=\"{relation}\";type=\"{media type\"};title=\"{title}\"... DO use link header when you want to convey links in a format-independent manner DO use link header when a representation format does not support links. E.g.: Binary format Formats that do not allow for easy discovery of links (e.g., plain-text documents) When your client/server software needs to add links or read links without parsing the body of representations Link Relation Types \u00b6 ALWAYS supply a link relation to act as an identifier for the semantics associated with the link ALWAYS use URIs (such as http://www.example.org/rels/create-po ) to express extended link relation types DO choose unambiguous value of link relations DO use one of the standard relation types , when appropriate DO use lowercase for all link relation types DO use multiple values in link relations, if applicable CONSIDER providing an informational resource as an HTML document at that URI, describing the semantics of the link relation type. Include details such as HTTP methods supported, formats supported, and business rules about using the link. Link relation types meant for public use should register that link relation per the process outlined in section 6.2 of the Web Linking Internet-Draft. Managing Application Flow with Links \u00b6 One of the key applications of hypermedia and links is the ability to decouple the client from learning about the rules the server uses to manage its application flow. The server can provide links containing application state, thereby using Hypermedia As The Engine Of Application State. This prevents clients from having to learn and hard-code application flow, however, the mere presence of a link will not decouple the client from having to know how to prepare the data and make a request for the transition. DO design representations such that it contain links that help clients transition to all the next possible steps DO encode the state that needs to be carried forward in the links DO document how to find links and the semantics of all extended link relation types. DO , for clients, assume that absent links means the transition is not possible. Ephemeral URIs \u00b6 Does this belong in Resource Identifiers? A URI may be temporary and valid only for a single use or may expire after a fixed period of time. DO communicate ephemeral URIs via links. DO assign extended relation types for those links and document how long such URIs are valid and what the client should do after expiry. DO return appropriate 4xx code when responding to expired URIs, with an instructions in the body of any actions the client can take. URI Template \u00b6 When the server does not have all the information necessary to generate a valid and complete URI for each link. A URI template is a string consisting of token marked off between matching braces ( { and } ). Clients substitute these tokens (including matching braces) with URI-safe strings to convert the template into a valid URI. For simplicity limit the tokens to the following parts of URIs: Path segments Values of query parameters Values of matrix parameters To include URI templates in a representation: DO use link-template or link-templates properties to convey URI templates, for JSON representations. DO document the tokens used in you URI template, since URI templates are semi-opaque and contain tokens that clients need to substitute, and you need a way to tell clients what values are valid for each token. CONSIDER using braces ( { and } ) to specify replacement tokens, as this a common standard in a lot of other templating system (e.g., WSDL 2.0 and WADL. // JSON representation { \"link-templates\" : [{ \"rel\" : \"http://www.example.org/rels/customer\" , \"href\" : \"http://www.example.org/customer/{customer-id}\" }] }","title":"Hypermedia"},{"location":"hypermedia/#hypermedia","text":"","title":"Hypermedia"},{"location":"hypermedia/#what-is-hypermedia","text":"One of the challenges to implementing and correctly using hypermedia in your REST API is first understanding what hypermedia is, and what it means to use hypermedia as the engine of application state (HATEOAS). The term \"hypermedia\" was coined back in 1965 by Ted Nelson, and over the years has dominated the technology industry. Hypermedia, in its most basic sense is an extension of hypertext \u2013 something you may recognize from HTML. Hypertext is essentially text that is written in a structured format and contains relationships to other objects via links. If you think of a webpage, using HTML \u2013 the Hypertext Markup Language \u2013 text is then interpreted by a browser to become a webpage, or an interactive environment capable of doing more than just providing a blob of text. However, to be truly interactive and guide the user, the crucial component of all of this is links, something we use everyday when surfing the web. Enter hypermedia, again just an extension of the term hypertext, hypermedia includes images, video, audio, text, and links. In a REST API, this means that your API is able to function similarly to a webpage, providing the user with guidance on what type of content they can retrieve, or what actions they can perform, as well as the appropriate links to do so. Essentially, the easiest way to take advantage of hypermedia in your API is to provide valuable information to direct the user or client to the next possible actions they can take based on the object (whether it be a collection, or item within the resource) or \"page\" they are on via links. Another way to think of it is to bring back the Hypercard application from the early Apple days (we're talking Macintosh Classic). With this application you could create \"cards\" or slides with links that performed different functions. Clicking on link might play a sound, another a video, or another would take you to a different card. On each card you were presented with certain links (whether in the form of a button or text) for the available actions to you on this card. This is exactly how hypertext links work within RESTful APIs\u2013 they are designed to take you to the other components of the API, your other \"cards.\" By adding these links, because REST is stateless, we are providing the client with a method for determining the state of the current item/ collection that they are on, as well as giving them the roadmap or engine for what they can do with it. In other words, we are using Hypermedia as the Engine of the Application's State, or implementing HATEOAS.","title":"What is Hypermedia?"},{"location":"hypermedia/#the-hateoas-debate","text":"Currently, there is a lot of disagreement around hypermedia's place in RESTful APIs\u2013 with Dr. Roy Fielding saying it is critical to REST, and without it your API cannot be RESTful; and hypermedia skeptics arguing that you are doing nothing more than bloating your API and making it more difficult to use under the guise of usability. This means you will find many different stances on the web today, ranging from Peter Williams explanation of how hypermedia has helped him, Jeff Krupp's rant against hypermedia, and even Kin Lane's summary of the whole debate. But what it really comes down to isn't what do others think about hypermedia, as most major frameworks for building APIs now include hypermedia specs such as HAL, JSON-LD, JSON API, Siren, or Collection+JSON, but rather what benefit does hypermedia offer- and is that benefit enough to outweigh the cost/ disadvantages of incorporating hypermedia.","title":"The HATEOAS Debate"},{"location":"hypermedia/#the-claims-for-and-against-hypermedia","text":"Some of the most common arguments for and against hypermedia include:","title":"The Claims For and Against Hypermedia"},{"location":"hypermedia/#hypermedia-creates-more-work","text":"This is absolutely true. Adding hypermedia or hypertext links to your API output does require more work \u2013 and more thought to go into your API. Going back to the Planning Your API post in this series, hypermedia explains how your API works together, what resources work with which, and what you can do with a collection, or specific items in that collection. This will add to both the workload and the time it takes to build your API. However, by having these relationships already drawn out as suggested, the additional time required will be minimal.","title":"Hypermedia creates more work"},{"location":"hypermedia/#hypermedia-will-require-more-data-transfer","text":"Again, absolutely true. By adding the links to your response you are increasing the amount of data that needs to be sent back, and slowly down the responses ever so slightly. However, for MOST APIs the amount of data being added will be minimal and the data/ time difference result will realistically be unnoticeable. What you will do, however, is help avoid unnecessary calls that are not accessible to a particular item, but may be available to other items. See the next section for more on this.","title":"Hypermedia will require more Data-Transfer"},{"location":"hypermedia/#you-are-creating-an-api-for-a-client-that-doesnt-exist","text":"Another argument against APIs is that unlike web browsers, there are no browsers for APIs. Likewise a valid argument, however, one I personally believe falls short as like the web we are seeing advances in APIs and recently an explosion of API exploration tools ranging from API Consoles to the API Notebook. As more and more APIs are developed there will be more and more emphasis on being able to explore them pragmatically. Another reason I believe this argument falls short is that by utilizing hypermedia you allow the developers USING your API to build their systems around the information you provide them, creating a more dynamic application that relies on the available actions YOU return to them instead of creating rigidity by hardcoding what actions the user can take from their application in relation to your API and returning errors when those actions fail because they simply were not available to begin with. For example, for one user you may be able to: Edit Suspend Delete But for another user who has been suspended, maybe you can only: Restore Delete Because this is dynamic data the developer has no way to determine what actions are available and which ones are not, unless they dig through and try to decipher the user's data which has been returned- making assumptions about your architectures rules which may or may not be correct, and may change with future development. Hypertext links in this case allow the developer to rely on YOUR rules and architecture, rather than trying to mimic it with their own.","title":"You are creating an API for a client that doesn't exist"},{"location":"hypermedia/#nobody-knows-how-to-use-hypermedia","text":"There is some truth into this. Despite existing since 1965, many developers are flustered when first running into hypermedia, choosing to ignore it or misunderstanding how to use it (i.e. hard-coding the links). What can help with this, however, is more APIs taking advantage of hypermedia (as being encouraged in the most popular development frameworks), and simple explanations within documentation explaining how the hypertext links work (just as we had to explain how to do a GET, POST, PUT, PATCH, and DELETE when REST first came out). However, unlike many specs and challenges out there, utilizing hypermedia from a client has a very fast learning curve, and most developers are able to implement it quickly and without any real issue once they understand what it is designed to do. And once implemented correctly, many developers appreciate knowing that as resource URIs change or additional query parameters are required of them, that their application will be able to \"automatically upgrade\" to these new requirements without any work or concern on their part.","title":"Nobody knows how to use Hypermedia"},{"location":"hypermedia/#hypermedia-makes-your-api-more-flexible","text":"Absolutely true. By adding Hypermedia you are able to add new features more seamlessly- making them immediately available to your users. It also gives you the power to change certain aspects of your API (i.e. changing resources, requiring additional GET parameters) without necessarily breaking backwards compatibility IF implemented correctly by your users.","title":"Hypermedia makes your API MORE Flexible"},{"location":"hypermedia/#hypermedia-prevents-apis-from-breaking","text":"Absolutely false. As mentioned above, Hypermedia does make your API more flexible, but does not excuse poor design or allow you to break backwards compatibility. One of the reasons for this, is even if you have a purely hypermedia driven API, such as Stormpath's, developers will still hardcode certain URLs to avoid having to make multiple calls. For example, they may access the /users resource directly to get a list of all users instead of starting from the gateway or entry point of your API and working up to that point through multiple calls. While this technique does save you (and them) multiple API calls, it does limit your ability to change common or most frequently used resource URIs. Hypertext links also do nothing for backwards incompatible changes in data\u2013 although you could hypothetically use them as a form of versioning.","title":"Hypermedia prevents APIs from breaking"},{"location":"hypermedia/#hypermedia-replaces-documentation","text":"Hypermedia does increase discoverability of your API's resources and methods, however, it does not replace documentation as it doesn't provide method information, schemas, or examples. Documentation also serves as a preview to your API, letting developers understand how it works, and potentially make sample calls using an API Console before implementing it into their application. Hypermedia also does not play a solid role in debugging the implementation of the API when things go wrong. For this reason, having well written, informative documentation is vital to any API. In fact, many RESTful hypermedia specs including HAL grant you the ability to embed documentation links for quick reference by developers.","title":"Hypermedia replaces Documentation"},{"location":"hypermedia/#hypermedia-is-a-best-practice","text":"Where hypermedia shines is in its ability to create a flexible API that provides dynamic data for developers based on your architecture and not their own. In essence, it provides a shortcut for developers that allows them to utilize your API to its fullest without having to rely solely on documentation and writing rules that may or may not not be consistent with those in your application. Like when building a simple website, one may not see the advantage of Hypermedia right away, but as your API grows and becomes more complex you will find that it becomes a powerful convenience layer, one that will help developers better understand and navigate your API, and one that may prevent you from having to version your API for certain changes that would otherwise be backwards incompatible \u2014 if implemented correctly by your users. As Peter Williams explains in his post, hypermedia turned out to be a saving grace. And as a key component of REST as defined by Dr. Roy Fielding, it remains a best practice to implement.","title":"Hypermedia is a Best Practice"},{"location":"hypermedia/#in-summary","text":"Hypermedia is often misunderstood in regards to APIs, but essentially it functions exactly like links on a webpage. And while the technology is both praised and criticized, it does provide an array of short and long-term gains. These gains, I believe, become more and more noticeable the larger and more complex your API becomes. However, the best features of utilizing HATEOAS, in my opinion, are yet to be seen as technology and API exploration tools continue to advance. Be sure to join us next week as we take a look at implementing hypermedia into your API and the current specs out there to help you do so.","title":"In Summary"},{"location":"hypermedia/#the-harsh-reality-of-the-state-of-hypermedia-specs","text":"Hypermedia sounds great in theory, but theory only goes so far. Where hypermedia really shines, or completely fails, is in implementation. Unfortunately, as hypermedia is still a relatively new aspect of web based APIs, there isn't one specified way of doing things. In fact, you'll find that even some of the most popular APIs operate completely differently from each other. After all, there are several different hypermedia formats available for API providers to choose from. Just for starters there is HAL, Collection+JSON, JSON-LD, JSON API, and Siren! But the list doesn't stop there, as some popular APIs have even elected to create their own format. For example, while PayPal's API closely mimics the JSON API format, it goes a step further and adds a method property (not part of the JSON API spec), creating a more flexible spec and transforming it from being resource driven to being action driven: This has the potential to let developers create a more agile client based on the actions (and methods) available to them. However, for developers not familiar with PayPal's format, but familiar with JSON API this may cause slight confusion (although it should be quickly remedied by reading their docs). VerticalResponse, on the other hand, has taken a different, albeit interesting approach. For their API they likewise start with the basic JSON API format, but for some reason decided against the universally accepted \"href\" or Hypertext Reference property, instead opting to use \"url\" or the uniform resource locator as the link URI identifier: Personally, I would recommend staying with the uniform \"href\" attribute as it denotes a reference to a hypertext link and is not as exclusive as an URL- which is not (although it commonly is) to be confused with URI. But you can read more on that here. On the other hand, Amazon's AppStream API, Clarify, Microsoft's Lync, and FoxyCart all prefer to follow HAL or the Hypertext Application Language format. HAL provides a simple format for nest-able links, but like other specs omits the methods property as included by PayPal, making theirs truly unique in that sense: However FoxyCart takes it one step further, not only taking advantage of hypermedia, but offering multiple formats for their clients to choose from, including HAL+JSON, HAL+XML, and Siren. This, however, highlights once again one of the biggest challenges with hypermedia driven APIs, the abundance of ideas and specs available for execution. While on one-hand I believe that by supporting both XML and JSON, as well as having multiple JSON formats FoxyCart is by far the most flexible (format wise) of the APIs, not having a singular standard for each language does present the challenge of forcing developers (and hypermedia clients) to support multiple formats (as they integrate more and more hypermedia APIs), while also having the understanding that not one format meets every API's needs. The good news, is that despite these growing pains, we are starting to see companies adopting certain specs over others, while also identifying areas for improvement (such as with PayPal's adding of methods to JSON API). Next week we'll take a look at some of the most popular formats out there in-depth, keying in on the strengths and weaknesses of each. But it's important that as you build your API, you understand WHY you are building it the way you are. And this extends into how you build your hypermedia links, and whether or not you choose to take advantage of a standardized format (recommended), or venture off on your own to meet your developers' needs. One of the best ways to do this is to explore what others have done with their APIs, and learn from their successes, and their mistakes. It's also important to consider where technology is going. And as more and more formats become available and change in popularity, it may be smart to follow FoxyCart's lead \u2013 taking advantage of the spec that best meets your developers' needs, but also keeping the link format decoupled enough from your data that you are able to return multiple formats based on the content-type received. Something that will allow you to take advantage of this best practice, while also being prepared for whatever the future may hold.","title":"The Harsh Reality of the State of Hypermedia Specs"},{"location":"hypermedia/#specs","text":"What essentially every hypertext linking spec does provide is a name for the link and a hypertext reference, but outside of that, it's a crapshoot. As such, it's important to understand the different specs that are out there, which ones are leading the industry, and which ones meet your needs. We may not be able to get it down to one spec, but at least we'll be able to provide our users with a uniform response that they can easily incorporate into their application:","title":"Specs"},{"location":"hypermedia/#collectionjson","text":"Collection+JSON is a JSON-based read/write hypermedia-type designed by Mike Amundsen back in 2011 to support the management and querying of simple collections. It's based on the Atom Publication and Syndication specs, defining both in a single spec and supporting simple queries through the use of templates. While originally widely used among APIs, Collection+JSON has struggled to maintain its popularity against JSON API and HAL. Strengths: strong choice for collections, templated queries, early wide adoption, recognized as a standard Weaknesses: JSON only, lack of identifier for documentation, more complex/ difficult to implement","title":"Collection+JSON"},{"location":"hypermedia/#json-api","text":"JSON API is a newer spec created in 2013 by Steve Klabnik and Yahuda Klaz. It was designed to ensure separation between clients and servers (an important aspect of REST) while also minimizing the number of requests without compromising readability, flexibility, or discovery. JSON API has quickly become a favorite receiving wide adoption and is arguably one of the leading specs for JSON based RESTful APIs. JSON API currently bares a warning that it is a work in progress, and while widely adopted not necessarily stable. Strengths: simple versatile format, easy to read/ implement, flat link grouping, URL templating, wide adoption, strong community, recognized as a hypermedia standard Weaknesses: JSON only, lack of identifier for documentation, still a work in progress","title":"JSON API"},{"location":"hypermedia/#hal","text":"HAL is an older spec, created in 2011 by Mike Kelly to be easily consumed across multiple formats including XML and JSON. One of the key strengths of HAL is that it is nestable, meaning that _links can be incorporated within each item of a collection. HAL also incorporates CURIEs, a feature that makes it unique in that it allows for inclusion of documentation links in the response \u2013 albeit they are tightly coupled to the link name. HAL is one of the most supported and most widely used hypermedia specs out there today, and is surrounded by a strong and vocal community. Strengths: dynamic, nestable, easy to read/ implement, multi-format, URL templating, inclusion of documentation, wide adoption, strong community, recognized as a standard hypermedia spec, RFC proposed Weaknesses: JSON/XML formats architecturally different, CURIEs are tightly coupled","title":"HAL"},{"location":"hypermedia/#json-ld","text":"JSON-LD is a lightweight spec focused on machine to machine readable data. Beyond just RESTful APIs, JSON-LD was also designed to be utilized within non-structured or NoSQL databases such as MongoDB or CouchDB. Developed by the W3C JSON-LD Community group, and formally recommended by W3C as a JSON data linking spec in early 2014, the spec has struggled to keep pace with JSON API and HAL. However, it has built a strong community around it with a fairly active mailing list, weekly meetings, and an active IRC channel. Strengths: strong format for data linking, can be used across multiple data formats (Web API & Databases), strong community, large working group, recognized by W3C as a standard Weaknesses: JSON only, more complex to integrate/ interpret, no identifier for documentation","title":"JSON-LD"},{"location":"hypermedia/#siren","text":"Created in 2012 by Kevin Swiber, Siren is a more descriptive spec made up of classes, entities, actions, and links. It was designed specifically for Web API clients in order to communicate entity information, actions for executing state transitions, and client navigation/ discoverability within the API. Siren was also designed to allow for sub-entities or nesting, as well as multiple formats including XML \u2013 although no example or documentation regarding XML usage is provided. Despite being well intentioned and versatile, Siren has struggled to gain the same level of attention as JSON API and HAL. Siren is still listed as a work in progress. Strengths: provides a more verbose spec, query templating, incorporates actions, multi-format Weaknesses: poor adoption, lacks documentation, work in progress","title":"Siren"},{"location":"hypermedia/#other-specs","text":"Along with some of the leading specs mentioned above, new specs are being created every day including UBER, Mason, Yahapi, and CPHL. This presents a very interesting question, and that is are we reinventing the wheel, or is there something missing in the above specs. I believe the answer is a combination of both, with developers being notorious for reinventing the wheel, but also because each developer looks at the strengths and weaknesses of other specs and envisions a better way of doing things. You may recognize this issue from the last post, where some specs were modified by the companies using them to meet their individual needs. For example, PayPal wanted to include methods in their response, but you'll notice that only Siren of the above include methods in the link definition.","title":"Other Specs"},{"location":"hypermedia/#the-future-of-specs","text":"Given that new specs are being created every day, each with different ideas and in different formats, it's extremely important to keep your system as decoupled and versatile as possible, and it will be very interesting to see what the future of hypermedia specs will look like. In the mean-time, it's best to choose the spec that meets your needs, while also being recognized as a standard for easy integration by developers. Of the specs above, I would personally recommend sticking with HAL or JSON API, although each has its own strengths and weaknesses, and I believe that universal spec of the future has yet to be created. But by adhering to these common specs while the new specs battle things out, I think we will finally find that standard method of road signs, detours, and a single solution to provide API clients with a standardized GPS system. For more on the different specs, I highly recommend reading Kevin Sookocheff's review. I'd also love to hear your thoughts in the comments below.","title":"The Future of Specs"},{"location":"hypermedia/#linking-and-application-state","text":"<!-- HATEOS / HAL / JSONAPI etc. --> A link provides a mean to navigate from one resource to another. Application state is the state the server needs to maintain between each request for each client. Keeping state in clients does not mean serializing session state into URIs or HTML forms. If the amount of data is small, the best place to maintain application state is within links in representations of resources, where the server can encode the state within the URI itself. However, the server can stores data in a durable storage and encodes its primary key in the URI. Use a combination of both approaches for managing application state to strike a balance between network performance, scalability and reliability. ``` HTTP/1.1 200 OK Content-Type: application/xml;charset=UTF-8 <quote xmlns:atom=\"http://www.w3.org/2005/Atom\"> <driver>...</driver> <vehicle>...</vehicle> <offer> <valid-until>2009-10-02</valid-until> <atom:link href=\"http://www.example.org/quotes/buy?quote=abc1234\" rel=\"http://www.example.org/rels/quotes/buy\" /> </offer> </quote> ``` DO encode application state into URIs, and include those URIs into representations via links. DO store the application state in a durable storage, and encode a reference to that state in URIs, if the state is large or cannot be transported to the clients for security or privacy reasons. DO make sure to add checks (such as signatures) to detect/prevent tampering of state, when using application state in links. #### *Links in XML Representations* *[Atom](http://www.w3.org/2005/Atom)*","title":"Linking and Application State"},{"location":"hypermedia/#must-use-common-hypertext-controls","text":"When embedding links to other resources into representations you must use the common hypertext control object. It contains at least one attribute: href: The URI of the resource the hypertext control is linking to. All our API are using HTTP(s) as URI scheme. In API that contain any hypertext controls, the attribute name href is reserved for usage within hypertext controls. The schema for hypertext controls can be derived from this model: HttpLink: description: A base type of objects representing links to resources. type: object properties: href: description: Any URI that is using http or https protocol type: string format: uri required: [ \"href\" ] The name of an attribute holding such a HttpLink object specifies the relation between the object that contains the link and the linked resource. Implementations should use names from the IANA Link Relation Registry whenever appropriate. As IANA link relation names use hyphen-case notation, while this guide enforces snake_case notation for attribute names, hyphens in IANA names have to be replaced with underscores (e.g. the IANA link relation type version-history would become the attribute version_history) Specific link objects may extend the basic link type with additional attributes, to give additional information related to the linked resource or the relationship between the source resource and the linked one. E.g. a service providing \"Person\" resources could model a person who is married with some other person with a hypertext control that contains attributes which describe the other person (id, name) but also the relationship \"spouse\" between between the two persons (since): { \"id\": \"446f9876-e89b-12d3-a456-426655440000\", \"name\": \"Peter Mustermann\", \"spouse\": { \"href\": \"https://...\", \"since\": \"1996-12-19\", \"id\": \"123e4567-e89b-12d3-a456-426655440000\", \"name\": \"Linda Mustermann\" } } Hypertext controls are allowed anywhere within a JSON model. While this specification would allow HAL, we actually don't recommend/enforce the usage of HAL anymore as the structural separation of meta-data and data creates more harm than value to the understandability and usability of an API.","title":"MUST: Use Common Hypertext Controls"},{"location":"hypermedia/#links-in-json-representations","text":"## What about attribute names? In the previous section, we talked about formats - supporting multiple formats and working with JSON as the default. This time, let's talk about what happens when a response comes back. You have an object with data attributes on it. How should you name the attributes? Here are API responses from a few leading APIs: Twitter \"created_at\": \"Thu Nov 03 05:19;38 +0000 2011\" Bing \"DateTime\": \"2011-10-29T09:35:00Z\" Foursquare \"createdAt\": 1320296464 They each use a different code convention. Although the Twitter approach is familiar to me as a Ruby on Rails developer, we think that Foursquare has the best approach. How does the API response get back in the code? You parse the response (JSON parser); what comes back populates the Object. It looks like this var myObject = JSON.parse(response); If you chose the Twitter or Bing approach, your code looks like this. Its not JavaScript convention and looks weird - looks like the name of another object or class in the system, which is not correct. timing = myObject.created_at; timing - myObject.DateTime; Recommendations * Use JSON as default * Follow JavaScript conventions for naming attributes - Use medial capitalization (aka CamelCase) - Use uppercase or lowercase depending on type of object This results in code that looks like the following, allowing the JavaScript developer to write it in a way that makes sense for JavaScript. \"createdAt\": 1320296464 timing = myObject.createdAt; DO use a link property or a links property to include several links as an array whose value is a link object or a link object array. DO include href and rel properties in each link object Examples: { \"link\" : { \"rel\" : \"alternate\" , \"href\" : \"http://www.example.org/customers?format=json\" } } { \"link\" : { \"alternate\" : \"http://www.example.org/customers?format=json\" } } { \"links\" : [{ \"rel\" : \"alternate\" , \"href\" : \"http://www.example.org/customers?format=json\" }, { \"rel\" : \"http://www.example.org/rels/owner\" , \"href\" : \"...\" }] } { \"links\" : { \"alternate\" : \"http://www.example.org/customers?format=json\" \"http://www.example.org/rels/owner\" : \"http://www.example.org/owner\" } }","title":"Links in JSON Representations"},{"location":"hypermedia/#link-header","text":"The Link header provides a format-independent means to convey links, which is one of the key benefits, along with visibility at the protocol level. Also the need for documentation on how to discover links in XML or JSON representations is lowered. # Link header format Link: <{URI}>;rel=\"{relation}\";type=\"{media type\"};title=\"{title}\"... DO use link header when you want to convey links in a format-independent manner DO use link header when a representation format does not support links. E.g.: Binary format Formats that do not allow for easy discovery of links (e.g., plain-text documents) When your client/server software needs to add links or read links without parsing the body of representations","title":"Link Header"},{"location":"hypermedia/#link-relation-types","text":"ALWAYS supply a link relation to act as an identifier for the semantics associated with the link ALWAYS use URIs (such as http://www.example.org/rels/create-po ) to express extended link relation types DO choose unambiguous value of link relations DO use one of the standard relation types , when appropriate DO use lowercase for all link relation types DO use multiple values in link relations, if applicable CONSIDER providing an informational resource as an HTML document at that URI, describing the semantics of the link relation type. Include details such as HTTP methods supported, formats supported, and business rules about using the link. Link relation types meant for public use should register that link relation per the process outlined in section 6.2 of the Web Linking Internet-Draft.","title":"Link Relation Types"},{"location":"hypermedia/#managing-application-flow-with-links","text":"One of the key applications of hypermedia and links is the ability to decouple the client from learning about the rules the server uses to manage its application flow. The server can provide links containing application state, thereby using Hypermedia As The Engine Of Application State. This prevents clients from having to learn and hard-code application flow, however, the mere presence of a link will not decouple the client from having to know how to prepare the data and make a request for the transition. DO design representations such that it contain links that help clients transition to all the next possible steps DO encode the state that needs to be carried forward in the links DO document how to find links and the semantics of all extended link relation types. DO , for clients, assume that absent links means the transition is not possible.","title":"Managing Application Flow with Links"},{"location":"hypermedia/#ephemeral-uris","text":"Does this belong in Resource Identifiers? A URI may be temporary and valid only for a single use or may expire after a fixed period of time. DO communicate ephemeral URIs via links. DO assign extended relation types for those links and document how long such URIs are valid and what the client should do after expiry. DO return appropriate 4xx code when responding to expired URIs, with an instructions in the body of any actions the client can take.","title":"Ephemeral URIs"},{"location":"hypermedia/#uri-template","text":"When the server does not have all the information necessary to generate a valid and complete URI for each link. A URI template is a string consisting of token marked off between matching braces ( { and } ). Clients substitute these tokens (including matching braces) with URI-safe strings to convert the template into a valid URI. For simplicity limit the tokens to the following parts of URIs: Path segments Values of query parameters Values of matrix parameters To include URI templates in a representation: DO use link-template or link-templates properties to convey URI templates, for JSON representations. DO document the tokens used in you URI template, since URI templates are semi-opaque and contain tokens that clients need to substitute, and you need a way to tell clients what values are valid for each token. CONSIDER using braces ( { and } ) to specify replacement tokens, as this a common standard in a lot of other templating system (e.g., WSDL 2.0 and WADL. // JSON representation { \"link-templates\" : [{ \"rel\" : \"http://www.example.org/rels/customer\" , \"href\" : \"http://www.example.org/customer/{customer-id}\" }] }","title":"URI Template"},{"location":"methods/","text":"HTTP Methods \u00b6 Before discussing the individual HTTP methods here is some general advise: DO use HTTP methods correctly. DO stay consistent with the HTTP verb definitions. DO use CRUD for basic operations, as most developers will be familiar with this way of working with an API. CRUD C reate using POST R ead using GET U pdate using PUT D elete using DELETE HTTP supports the following methods: Method Safe Idempotent GET Yes Yes HEAD Yes Yes OPTIONS Yes Yes PUT No Yes DELETE No Yes PATCH No No POST No No Safety and idempotency are guarantees a server must provide to clients. Safe methods Safe methods (or read-only operations) are expected not to cause side-effects, however, it does not mean the server must return the same response every time. Idempotent methods Idempotent methods must guarantee that every request has the same effect, however, this does not necessarily mean returning the same status code. This is highly important in case of failures. GET \u00b6 DO use GET to get a safe and idempotent representation of a resource. DO use GET as the preferred method of requesting resources using URL encoded query parameter. When these become too extensive use POST instead. DO use GET to read an individual resource or query a collection of resources. DO return 404 Not Found if an individual resource or resource collection does not exist. CONSIDER returning 200 Ok if the resource collection is empty, along with a representation of an empty collection. DO NOT add request body payload as these will be ignored, use POST instead. DO NOT use GET for unsafe and nonidempotent operations, use POST instead. HEAD \u00b6 HEAD requests are used to retrieve header information of resources, and has the exact same semantics as GET , but only returns headers. CONSIDER using HEAD to check whether a resource exists or get its metadata. OPTIONS \u00b6 OPTIONS is rarely implemented. Although OPTIONS could be used at runtime to discover methods supported by the resource, doing so is expensive. The method is not cacheable. DO return a comma separated list of methods in the Allow header. When a resource supports PATCH add an Accept-Patch header with the supported media types. DO use OPTIONS to get a list of available HTTP methods supported by a given resource. DO use OPTIONS to ping the server or find the supported HTTP version OPTIONS * HTTP/1.1 CONSIDER adding a Link header containing a link to a human-readble document that describes the resource. This can be used to develop a browser plug-in to automatically show the documentation when you type the resource URI in the browser. AVOID relying on OPTIONS at runtime for discovery, instead use documentation and links to discover URIs and make requests. PUT \u00b6 DO use PUT to update an entire single resource. The operation implies that the entire resource located at the URI will be replaced with the new representation in the request. DO return 200 Ok or 204 No Content if the resource was updated. DO use a comibation of ETag and If-Match header for concurrency. DO return 404 Not Found is PUT does not support creation. CONSIDER using PATCH if making partial updates to entire resources become increasingly cumbersome. CONSIDER before using PUT on a collection resources as it implies replacing the entire collection. DO NOT use PUT to create new resources unless the clients can decide URIs of resources (e.g., WebDAV), instead use POST . DELETE \u00b6 DO use DELETE to delete a resource. Always return 200 OK , however, it is impractical to keep track of all deleted resource, so a 404 Not Found may be a viable alternative. Security policies may also require the server to return 404 Not Found for any resource that does not currently exist. CONSIDER before using DELETE on a collection resources as it implies deleting the entire collection. PATCH \u00b6 It is important to understand the difference between PUT and PATCH . PUT is designed to update/replace the entire resource. This means that omitted fields will be removed, which is rarely the desired effect. PATCH is designed to support partial updates. This means the request should supply a set of changes (or instructions) for updating a resource and these should be applied atomically, leaving any fields not passed along intact. Warning Be aware, even though PATCH has gain a lot of use, PATCH is only a proposed standard, and details around the semantics are not widely understood. It's not an alternative to POST or PUT where you supply a flat list of values to change. Please see RFC-5789 for more information. DO use PATCH for partial updates of a single resources, i.e. where only a specific subset of fields should be replaced. DO document the semantic of the PATCH changeset, as it is not defined in the HTTP standard. DO return 200 Ok or 204 No Content for successful PATCH requests. DO use a comibation of ETag and If-Match (and/or If-Unmodified-Since ) header for concurrency. Return 412 Precondition Failed if the supplied preconditions do not match. DO include the Content-Location header along with the entire representation of the resource. If not, the client must issue an unconditional GET request to fetch the updated representation of the resource, along with fresh ETag and/or If-Unmodified-Since if applicable. DO include the latest Last-Modified and/or ETag headers to support conditional requests. DO return 415 Unsupported Media Type when the client sends a patch document format that the server does not support. Include the Accept-Patch header with the supported media types. DO return 422 Unprocessable Entity when the server cannot honor the request because it might result in a bad state for the resource. CONSIDER using suitable media types to describe the changeset. CONSIDER using the lightweight JSON merge patch (RFC 7386) to describe changesets in JSON format. CONSIDER using the JSON Patch (RFC 6902) to describe changesets in JSON format. CONSIDER before using PATCH on a collection resources as it implies patching the entire collection. CONSIDER overloading POST , not PUT , when PATCH is not an option. CONSIDER using PUT instead of PATCH , if possible, either to update the entire resource, or by designing a new resource to encapsulate the parts of the original resource that can be updated. Such resources may seem inconsistent (or even polluting), but anything that is appropriate for retrival and updates is a candidate as a resource . CONSIDER advertising the support for PATCH via the Allow header of the OPTIONS response, also including an Accept-Patch header with the supported media types for the PATCH method. CONSIDER designing a specific format for each resource, to ensure that PATCH request only include valid combinations of changes. DO NOT repeat PATCH requests unless explicitly stated in the documentation. POST \u00b6 POST requests are most often used to create resources, by using a collection resource as a factory. However, POST may also be used for other operations that fall outside the scope of the other methods. DO use POST to create resources. DO use POST to modify one or more resources. DO use POST to for queries with large inputs. DO use POST to perform any unsafe and nonidempotent operation, when no other HTTP method seems appropriate, and only as a last resort. AVOID passing data in query string along POST request, use the body instead. AVOID using POST for tunneling, like SOAP. DO NOT use nonstandard custom HTTP methods. Instead, design a controller resource that can abstract such operations, and POST . Creating Resources \u00b6 While it is valid to use either PUT or POST to create new resources, the general consensus is that creating a new resource without knowing the final URI is a POST operation (each call will yield a new resource). If the URI (or part of it) is known, use PUT , because successive calls will not create a new resource, as PUT is idempotent. DO return 201 Created and a Location header containing the URI of the newly created resource. CONSIDER returning the newly created resource representation in the response. CONSIDER include a Content-Location header containing the URI of the newly created resource, if the response body includes a complete representation of the newly created resource. CONSIDER including the Last-Modified and ETag headers of the newly created resource for optimistic concurrency. Large and Stored Queries \u00b6 Sometimes it may be necessary to support queries with large inputs, and the query string may no longer be an option. For those cases: DO use POST to support large queries, as a necessary trade-off to address a practical limitation, even though this is a misuse of the uniform interface, and a consequence is a loss of cacheability CONSIDER the fact tha pagination may also cause extra latency, since POST s are not cached. CONSIDER using stored queries to improve cacheability and reuse across clients: DO create a new resource whose state contains the query criteria. Return 201 Created and a Location header referring to a resource created. DO implement a GET request for the new resource such that it returns query results. DO find the resource that matches the request, and redirect the client to the URI of that resource, if the same or another client repeats the same query request using POST . DO support pagination via GET instead of POST . CONSIDER the number of different queries and evaluate cache hit ratio, and whether named queries are a better option. Asynchronous Tasks \u00b6 To enable asynchronous processing of request, follow these guidelines (these step are also valid for DELETE ): DO use POST to create and return a representation of a new task resource, and return status code 202 Accepted . The purpose of this resource is to let a client track the status of the asynchronous task. Design this resource such that its representation includes the current status of the request and related information such as a time estimate. DO use GET to return a representation of the task resource, depending of the current status: DO return 200 Ok and a representation of the task resource with the current status, when still processing (pending). DO return 303 See Other and a Location header containing a URI of a resource that shows the outcome of the task, once the task has successfully completed . DO return 200 Ok with a representation of the task resource informing that the resource creation has failed . Clients will need to read the body of the representation to find the reason for failure. Example of pending HTTP / 1.1 202 Accepted Content-Type : application/json Content-Location : https://www.example.org/images/task/1 { \"state\" : \"pending\" , \"message\" : \"Your request is being processed shortly.\" , \"pingAfter\" : \"2009-09-13T01:59:27Z\" , \"link\" : { \"href\" : \"https://www.example.org/images/task/1\" , \"rel\" : \"self\" } } Example of done HTTP / 1.1 303 See Other Content-Type : application/json Location : https://www.example.org/images/1 Content-Location : https://www.example.org/images/task/1 { \"state\" : \"done\" , \"message\" : \"Your request has been processed.\" , \"link\" : { \"href\" : \"https://www.example.org/images/task/1\" , \"rel\" : \"self\" } } Note The 303 See Other does not mean that the resource at the request URI has moved to a new location. It merely states that the result exists at the URI indicated in the Location header. Batch Operations \u00b6 TBD MUST: Use 207 for Batch or Bulk Requests \u00b6 Some APIs are required to provide either batch or bulk requests using POST for performance reasons, i.e. for communication and processing efficiency. In this case services may be in need to signal multiple response codes for each part of an batch or bulk request. As HTTP does not provide proper guidance for handling batch/bulk requests and responses, we herewith define the following approach: A batch or bulk request always has to respond with HTTP status code 207, unless it encounters a generic or unexpected failure before looking at individual parts. A batch or bulk response with status code 207 always returns a multi-status object containing sufficient status and/or monitoring information for each part of the batch or bulk request. A batch or bulk request may result in a status code 400/500, only if the service encounters a failure before looking at individual parts or, if an unanticipated failure occurs. The before rules apply even in the case that processing of all individual part fail or each part is executed asynchronously! They are intended to allow clients to act on batch and bulk responses by inspecting the individual results in a consistent way. Note: while a batch defines a collection of requests triggering independent processes, a bulk defines a collection of independent resources created or updated together in one request. With respect to response processing this distinction normally does not matter.","title":"Methods"},{"location":"methods/#http-methods","text":"Before discussing the individual HTTP methods here is some general advise: DO use HTTP methods correctly. DO stay consistent with the HTTP verb definitions. DO use CRUD for basic operations, as most developers will be familiar with this way of working with an API. CRUD C reate using POST R ead using GET U pdate using PUT D elete using DELETE HTTP supports the following methods: Method Safe Idempotent GET Yes Yes HEAD Yes Yes OPTIONS Yes Yes PUT No Yes DELETE No Yes PATCH No No POST No No Safety and idempotency are guarantees a server must provide to clients. Safe methods Safe methods (or read-only operations) are expected not to cause side-effects, however, it does not mean the server must return the same response every time. Idempotent methods Idempotent methods must guarantee that every request has the same effect, however, this does not necessarily mean returning the same status code. This is highly important in case of failures.","title":"HTTP Methods"},{"location":"methods/#get","text":"DO use GET to get a safe and idempotent representation of a resource. DO use GET as the preferred method of requesting resources using URL encoded query parameter. When these become too extensive use POST instead. DO use GET to read an individual resource or query a collection of resources. DO return 404 Not Found if an individual resource or resource collection does not exist. CONSIDER returning 200 Ok if the resource collection is empty, along with a representation of an empty collection. DO NOT add request body payload as these will be ignored, use POST instead. DO NOT use GET for unsafe and nonidempotent operations, use POST instead.","title":"GET"},{"location":"methods/#head","text":"HEAD requests are used to retrieve header information of resources, and has the exact same semantics as GET , but only returns headers. CONSIDER using HEAD to check whether a resource exists or get its metadata.","title":"HEAD"},{"location":"methods/#options","text":"OPTIONS is rarely implemented. Although OPTIONS could be used at runtime to discover methods supported by the resource, doing so is expensive. The method is not cacheable. DO return a comma separated list of methods in the Allow header. When a resource supports PATCH add an Accept-Patch header with the supported media types. DO use OPTIONS to get a list of available HTTP methods supported by a given resource. DO use OPTIONS to ping the server or find the supported HTTP version OPTIONS * HTTP/1.1 CONSIDER adding a Link header containing a link to a human-readble document that describes the resource. This can be used to develop a browser plug-in to automatically show the documentation when you type the resource URI in the browser. AVOID relying on OPTIONS at runtime for discovery, instead use documentation and links to discover URIs and make requests.","title":"OPTIONS"},{"location":"methods/#put","text":"DO use PUT to update an entire single resource. The operation implies that the entire resource located at the URI will be replaced with the new representation in the request. DO return 200 Ok or 204 No Content if the resource was updated. DO use a comibation of ETag and If-Match header for concurrency. DO return 404 Not Found is PUT does not support creation. CONSIDER using PATCH if making partial updates to entire resources become increasingly cumbersome. CONSIDER before using PUT on a collection resources as it implies replacing the entire collection. DO NOT use PUT to create new resources unless the clients can decide URIs of resources (e.g., WebDAV), instead use POST .","title":"PUT"},{"location":"methods/#delete","text":"DO use DELETE to delete a resource. Always return 200 OK , however, it is impractical to keep track of all deleted resource, so a 404 Not Found may be a viable alternative. Security policies may also require the server to return 404 Not Found for any resource that does not currently exist. CONSIDER before using DELETE on a collection resources as it implies deleting the entire collection.","title":"DELETE"},{"location":"methods/#patch","text":"It is important to understand the difference between PUT and PATCH . PUT is designed to update/replace the entire resource. This means that omitted fields will be removed, which is rarely the desired effect. PATCH is designed to support partial updates. This means the request should supply a set of changes (or instructions) for updating a resource and these should be applied atomically, leaving any fields not passed along intact. Warning Be aware, even though PATCH has gain a lot of use, PATCH is only a proposed standard, and details around the semantics are not widely understood. It's not an alternative to POST or PUT where you supply a flat list of values to change. Please see RFC-5789 for more information. DO use PATCH for partial updates of a single resources, i.e. where only a specific subset of fields should be replaced. DO document the semantic of the PATCH changeset, as it is not defined in the HTTP standard. DO return 200 Ok or 204 No Content for successful PATCH requests. DO use a comibation of ETag and If-Match (and/or If-Unmodified-Since ) header for concurrency. Return 412 Precondition Failed if the supplied preconditions do not match. DO include the Content-Location header along with the entire representation of the resource. If not, the client must issue an unconditional GET request to fetch the updated representation of the resource, along with fresh ETag and/or If-Unmodified-Since if applicable. DO include the latest Last-Modified and/or ETag headers to support conditional requests. DO return 415 Unsupported Media Type when the client sends a patch document format that the server does not support. Include the Accept-Patch header with the supported media types. DO return 422 Unprocessable Entity when the server cannot honor the request because it might result in a bad state for the resource. CONSIDER using suitable media types to describe the changeset. CONSIDER using the lightweight JSON merge patch (RFC 7386) to describe changesets in JSON format. CONSIDER using the JSON Patch (RFC 6902) to describe changesets in JSON format. CONSIDER before using PATCH on a collection resources as it implies patching the entire collection. CONSIDER overloading POST , not PUT , when PATCH is not an option. CONSIDER using PUT instead of PATCH , if possible, either to update the entire resource, or by designing a new resource to encapsulate the parts of the original resource that can be updated. Such resources may seem inconsistent (or even polluting), but anything that is appropriate for retrival and updates is a candidate as a resource . CONSIDER advertising the support for PATCH via the Allow header of the OPTIONS response, also including an Accept-Patch header with the supported media types for the PATCH method. CONSIDER designing a specific format for each resource, to ensure that PATCH request only include valid combinations of changes. DO NOT repeat PATCH requests unless explicitly stated in the documentation.","title":"PATCH"},{"location":"methods/#post","text":"POST requests are most often used to create resources, by using a collection resource as a factory. However, POST may also be used for other operations that fall outside the scope of the other methods. DO use POST to create resources. DO use POST to modify one or more resources. DO use POST to for queries with large inputs. DO use POST to perform any unsafe and nonidempotent operation, when no other HTTP method seems appropriate, and only as a last resort. AVOID passing data in query string along POST request, use the body instead. AVOID using POST for tunneling, like SOAP. DO NOT use nonstandard custom HTTP methods. Instead, design a controller resource that can abstract such operations, and POST .","title":"POST"},{"location":"methods/#creating-resources","text":"While it is valid to use either PUT or POST to create new resources, the general consensus is that creating a new resource without knowing the final URI is a POST operation (each call will yield a new resource). If the URI (or part of it) is known, use PUT , because successive calls will not create a new resource, as PUT is idempotent. DO return 201 Created and a Location header containing the URI of the newly created resource. CONSIDER returning the newly created resource representation in the response. CONSIDER include a Content-Location header containing the URI of the newly created resource, if the response body includes a complete representation of the newly created resource. CONSIDER including the Last-Modified and ETag headers of the newly created resource for optimistic concurrency.","title":"Creating Resources"},{"location":"methods/#large-and-stored-queries","text":"Sometimes it may be necessary to support queries with large inputs, and the query string may no longer be an option. For those cases: DO use POST to support large queries, as a necessary trade-off to address a practical limitation, even though this is a misuse of the uniform interface, and a consequence is a loss of cacheability CONSIDER the fact tha pagination may also cause extra latency, since POST s are not cached. CONSIDER using stored queries to improve cacheability and reuse across clients: DO create a new resource whose state contains the query criteria. Return 201 Created and a Location header referring to a resource created. DO implement a GET request for the new resource such that it returns query results. DO find the resource that matches the request, and redirect the client to the URI of that resource, if the same or another client repeats the same query request using POST . DO support pagination via GET instead of POST . CONSIDER the number of different queries and evaluate cache hit ratio, and whether named queries are a better option.","title":"Large and Stored Queries"},{"location":"methods/#asynchronous-tasks","text":"To enable asynchronous processing of request, follow these guidelines (these step are also valid for DELETE ): DO use POST to create and return a representation of a new task resource, and return status code 202 Accepted . The purpose of this resource is to let a client track the status of the asynchronous task. Design this resource such that its representation includes the current status of the request and related information such as a time estimate. DO use GET to return a representation of the task resource, depending of the current status: DO return 200 Ok and a representation of the task resource with the current status, when still processing (pending). DO return 303 See Other and a Location header containing a URI of a resource that shows the outcome of the task, once the task has successfully completed . DO return 200 Ok with a representation of the task resource informing that the resource creation has failed . Clients will need to read the body of the representation to find the reason for failure. Example of pending HTTP / 1.1 202 Accepted Content-Type : application/json Content-Location : https://www.example.org/images/task/1 { \"state\" : \"pending\" , \"message\" : \"Your request is being processed shortly.\" , \"pingAfter\" : \"2009-09-13T01:59:27Z\" , \"link\" : { \"href\" : \"https://www.example.org/images/task/1\" , \"rel\" : \"self\" } } Example of done HTTP / 1.1 303 See Other Content-Type : application/json Location : https://www.example.org/images/1 Content-Location : https://www.example.org/images/task/1 { \"state\" : \"done\" , \"message\" : \"Your request has been processed.\" , \"link\" : { \"href\" : \"https://www.example.org/images/task/1\" , \"rel\" : \"self\" } } Note The 303 See Other does not mean that the resource at the request URI has moved to a new location. It merely states that the result exists at the URI indicated in the Location header.","title":"Asynchronous Tasks"},{"location":"methods/#batch-operations","text":"TBD","title":"Batch Operations"},{"location":"methods/#must-use-207-for-batch-or-bulk-requests","text":"Some APIs are required to provide either batch or bulk requests using POST for performance reasons, i.e. for communication and processing efficiency. In this case services may be in need to signal multiple response codes for each part of an batch or bulk request. As HTTP does not provide proper guidance for handling batch/bulk requests and responses, we herewith define the following approach: A batch or bulk request always has to respond with HTTP status code 207, unless it encounters a generic or unexpected failure before looking at individual parts. A batch or bulk response with status code 207 always returns a multi-status object containing sufficient status and/or monitoring information for each part of the batch or bulk request. A batch or bulk request may result in a status code 400/500, only if the service encounters a failure before looking at individual parts or, if an unanticipated failure occurs. The before rules apply even in the case that processing of all individual part fail or each part is executed asynchronously! They are intended to allow clients to act on batch and bulk responses by inspecting the individual results in a consistent way. Note: while a batch defines a collection of requests triggering independent processes, a bulk defines a collection of independent resources created or updated together in one request. With respect to response processing this distinction normally does not matter.","title":"MUST: Use 207 for Batch or Bulk Requests"},{"location":"mocking/","text":"Mocking \u00b6 Mock Your API and get User Feedback Another huge advantage of tools like RAML or Swagger is that they allow you to mock your API. This means that you can not only build your API in a visual interface and take advantage of the very same best practices we utilize in development, but you can also share a mock version of your API with potential clients. Using MuleSoft's API Designer you can easily turn on a mocking service that gives you a URL that can be shared with other developers. This allows your clients to \"test out\" your API by making real calls as if they would from their application. By utilizing the example responses defined in the RAML file developers can quickly identify issues and inconsistencies, helping you eliminate the majority of design related issues before development even starts. And by passing along tools like the API Notebook, developers can interact with your Mock API through JavaScript without having to code any calls themselves, and also having the ability to send you the specific use case back giving you true examples of what your developers are trying to accomplish. This process can be repeated as necessary, as modifying the spec and creating a new mock only takes minutes, empowering you to perfect the design of your API and ensure that it not only meets your developers' needs, but also provides a solid, strong foundation for the future of your API. After all, the nemesis of a long-lived API is not the code, nor the system architecture, but the API design itself. No matter how careful you are with your API, without a solid foundation it will crumble quickly, costing you thousands to hundreds of thousands of dollars down the road. It's better to take the time now, up front and ensure that your API is well designed. testing api\u2019s automated in-memory integration tests (with live dependencies stubed out) Implement a \u201cHealth-Check\u201d Endpoint Simulation \u00b6 Basically every software system has dependencies to other software, The consequence is sequential development of the components, Simulations offer a solution: they make it possible to break up the dependencies between software components and allow for integration and development of software components, even though their dependencies have not been developed, yet. Dependencies are replaced by simulations. In API design there are two use cases for simulations: The simulation of backend systems allows for developing APIs without fully implemented backend systems. If the real backend is not available yet, a simulation of the backend can be used in its place. The simulation of APIs allows for developing apps (or other API solutions) without fully implemented APIs. Both types of simulations have their place in API design - but in different scenarios. The development of the API and the development of the mobile app can take place in parallel. The simulation speeds up the development time of the overall API solution, Simulation-based design of APIs and the contract-first design for APIs actually go hand-in-hand. The contract for the API can be used as a specification for the simulation. Applying ideas of the simulation approach allows for breaking up the dependencies during development. It allows for an independent development of client and API, despite the dependencies between them.","title":"-Mocking"},{"location":"mocking/#mocking","text":"Mock Your API and get User Feedback Another huge advantage of tools like RAML or Swagger is that they allow you to mock your API. This means that you can not only build your API in a visual interface and take advantage of the very same best practices we utilize in development, but you can also share a mock version of your API with potential clients. Using MuleSoft's API Designer you can easily turn on a mocking service that gives you a URL that can be shared with other developers. This allows your clients to \"test out\" your API by making real calls as if they would from their application. By utilizing the example responses defined in the RAML file developers can quickly identify issues and inconsistencies, helping you eliminate the majority of design related issues before development even starts. And by passing along tools like the API Notebook, developers can interact with your Mock API through JavaScript without having to code any calls themselves, and also having the ability to send you the specific use case back giving you true examples of what your developers are trying to accomplish. This process can be repeated as necessary, as modifying the spec and creating a new mock only takes minutes, empowering you to perfect the design of your API and ensure that it not only meets your developers' needs, but also provides a solid, strong foundation for the future of your API. After all, the nemesis of a long-lived API is not the code, nor the system architecture, but the API design itself. No matter how careful you are with your API, without a solid foundation it will crumble quickly, costing you thousands to hundreds of thousands of dollars down the road. It's better to take the time now, up front and ensure that your API is well designed. testing api\u2019s automated in-memory integration tests (with live dependencies stubed out) Implement a \u201cHealth-Check\u201d Endpoint","title":"Mocking"},{"location":"mocking/#simulation","text":"Basically every software system has dependencies to other software, The consequence is sequential development of the components, Simulations offer a solution: they make it possible to break up the dependencies between software components and allow for integration and development of software components, even though their dependencies have not been developed, yet. Dependencies are replaced by simulations. In API design there are two use cases for simulations: The simulation of backend systems allows for developing APIs without fully implemented backend systems. If the real backend is not available yet, a simulation of the backend can be used in its place. The simulation of APIs allows for developing apps (or other API solutions) without fully implemented APIs. Both types of simulations have their place in API design - but in different scenarios. The development of the API and the development of the mobile app can take place in parallel. The simulation speeds up the development time of the overall API solution, Simulation-based design of APIs and the contract-first design for APIs actually go hand-in-hand. The contract for the API can be used as a specification for the simulation. Applying ideas of the simulation approach allows for breaking up the dependencies during development. It allows for an independent development of client and API, despite the dependencies between them.","title":"Simulation"},{"location":"more-references/","text":"API Resources \u00b6 2014-10-10 \u00b6 http://www.troyhunt.com/2014/02/your-api-versioning-is-wrong-which-is.html[Troy Hunt: Your API versioning is wrong, which is why I decided to do it 3 different wrong ways] 2018-03-14 \u00b6 http://idratherbewriting.com/learnapidoc/[Documenting APIs: A guide for technical writers | Document REST APIs] 2018-04-15 \u00b6 https://8thlight.com/blog/jason-desrosiers/2018/04/11/decoupling-the-client-and-server-with-hypermedia.html[Decoupling the Client and Server with Hypermedia | 8 th Light] 2018-07-13 \u00b6 http://apievangelist.com/2018/07/10/json-patch-to-articulate-change-for-your-api-roadmap/[Using OpenAPI And JSON PATCH To Articulate Changes For Your API Road Map] 2018-07-27 \u00b6 https://blog.goodapi.co/rest-vs-graphql-a-critical-review-5f77392658e7[REST vs. GraphQL: A Critical Review \u2013 Good API] 2018-08-29 \u00b6 http://apievangelist.com/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in-action/[Provide Your API Developers With A Forkable Example of API Documentation In Action] http://apievangelist.com/2018/08/27/how-do-we-get-developers-to-follow-the-minimum-viable-api-documentation-guidance/[How Do We Get API Developers To Follow The Minimum Viable API Documentation Guidance?] 2018-09-14 \u00b6 http://apievangelist.com/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/[Please Refer The Engineer From Your API Team To This Story] http://apievangelist.com/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api-developers/[Providing Minimum Viable API Documentation Blueprints To Help Guide Your API Developers] 2018-12-08 \u00b6 https://blog.philipphauer.de/rest-api-documentation-swagger-asciidoc/[RESTful API Documentation with Swagger and AsciiDoc] https://www.novatec-gmbh.de/en/the-problems-with-swagger/[The problems with Swagger | Novatec] 2019-01-06 \u00b6 https://www.zeroequalsfalse.press/2018/12/30/documentation[The Art of Writing Documentation | Zero Equals False] 2019-01-21 \u00b6 https://www.thoughtworks.com/insights/blog/rest-api-design-resource-modeling[REST API Design - Resource Modeling | ThoughtWorks] 2019-02-10 \u00b6 https://enterprisecraftsmanship.com/2014/12/27/dont-use-ids-domain-entities/[Don't use Ids in your domain entities! - Enterprise Craftsmanship] https://github.com/zalando/restful-api-guidelines/issues/62[Identity : ID vs. URL vs. URN \u00b7 Issue #62 \u00b7 zalando/restful-api-guidelines] https://developer.linkedin.com/docs/guide/v2/concepts/urns[URNs & IDs | LinkedIn Developer Network] 2019-03-06 \u00b6 https://phauer.com/2018/web-api-pagination-timestamp-id-continuation-token/[Web API Pagination with the 'Timestamp_ID' Continuation Token] https://phauer.com/2017/web-api-pagination-continuation-token/[Web API Pagination with the 'Timestamp_Offset_Checksum' Continuation Token] https://hackernoon.com/guys-were-doing-pagination-wrong-f6c18a91b232 [\u201cGuys, we\u2019re doing pagination wrong\u2026\u201d \u2013 Hacker Noon] 2019-04-01 \u00b6 https://damienbod.com/2019/03/25/grpc-bi-directional-streaming-with-razor-pages-and-a-hosted-service-grpc-client/[gRPC Bi-directional streaming with Razor Pages and a Hosted Service gRPC client | Software Engineering] 2019-04-10 \u00b6 https://andrewlock.net/using-strongly-typed-entity-ids-to-avoid-primitive-obsession-part-1/[Using strongly-typed entity IDs to avoid primitive obsession (Part 1)] 2019-04-23 \u00b6 https://itnext.io/learn-how-you-can-build-a-serverless-graphql-api-on-top-of-a-microservice-architecture-354ed0e67d7e[Learn how YOU can build a Serverless GraphQL API on top of a Microservice architecture]","title":"API Resources"},{"location":"more-references/#api-resources","text":"","title":"API Resources"},{"location":"more-references/#2014-10-10","text":"http://www.troyhunt.com/2014/02/your-api-versioning-is-wrong-which-is.html[Troy Hunt: Your API versioning is wrong, which is why I decided to do it 3 different wrong ways]","title":"2014-10-10"},{"location":"more-references/#2018-03-14","text":"http://idratherbewriting.com/learnapidoc/[Documenting APIs: A guide for technical writers | Document REST APIs]","title":"2018-03-14"},{"location":"more-references/#2018-04-15","text":"https://8thlight.com/blog/jason-desrosiers/2018/04/11/decoupling-the-client-and-server-with-hypermedia.html[Decoupling the Client and Server with Hypermedia | 8 th Light]","title":"2018-04-15"},{"location":"more-references/#2018-07-13","text":"http://apievangelist.com/2018/07/10/json-patch-to-articulate-change-for-your-api-roadmap/[Using OpenAPI And JSON PATCH To Articulate Changes For Your API Road Map]","title":"2018-07-13"},{"location":"more-references/#2018-07-27","text":"https://blog.goodapi.co/rest-vs-graphql-a-critical-review-5f77392658e7[REST vs. GraphQL: A Critical Review \u2013 Good API]","title":"2018-07-27"},{"location":"more-references/#2018-08-29","text":"http://apievangelist.com/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in-action/[Provide Your API Developers With A Forkable Example of API Documentation In Action] http://apievangelist.com/2018/08/27/how-do-we-get-developers-to-follow-the-minimum-viable-api-documentation-guidance/[How Do We Get API Developers To Follow The Minimum Viable API Documentation Guidance?]","title":"2018-08-29"},{"location":"more-references/#2018-09-14","text":"http://apievangelist.com/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/[Please Refer The Engineer From Your API Team To This Story] http://apievangelist.com/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api-developers/[Providing Minimum Viable API Documentation Blueprints To Help Guide Your API Developers]","title":"2018-09-14"},{"location":"more-references/#2018-12-08","text":"https://blog.philipphauer.de/rest-api-documentation-swagger-asciidoc/[RESTful API Documentation with Swagger and AsciiDoc] https://www.novatec-gmbh.de/en/the-problems-with-swagger/[The problems with Swagger | Novatec]","title":"2018-12-08"},{"location":"more-references/#2019-01-06","text":"https://www.zeroequalsfalse.press/2018/12/30/documentation[The Art of Writing Documentation | Zero Equals False]","title":"2019-01-06"},{"location":"more-references/#2019-01-21","text":"https://www.thoughtworks.com/insights/blog/rest-api-design-resource-modeling[REST API Design - Resource Modeling | ThoughtWorks]","title":"2019-01-21"},{"location":"more-references/#2019-02-10","text":"https://enterprisecraftsmanship.com/2014/12/27/dont-use-ids-domain-entities/[Don't use Ids in your domain entities! - Enterprise Craftsmanship] https://github.com/zalando/restful-api-guidelines/issues/62[Identity : ID vs. URL vs. URN \u00b7 Issue #62 \u00b7 zalando/restful-api-guidelines] https://developer.linkedin.com/docs/guide/v2/concepts/urns[URNs & IDs | LinkedIn Developer Network]","title":"2019-02-10"},{"location":"more-references/#2019-03-06","text":"https://phauer.com/2018/web-api-pagination-timestamp-id-continuation-token/[Web API Pagination with the 'Timestamp_ID' Continuation Token] https://phauer.com/2017/web-api-pagination-continuation-token/[Web API Pagination with the 'Timestamp_Offset_Checksum' Continuation Token] https://hackernoon.com/guys-were-doing-pagination-wrong-f6c18a91b232 [\u201cGuys, we\u2019re doing pagination wrong\u2026\u201d \u2013 Hacker Noon]","title":"2019-03-06"},{"location":"more-references/#2019-04-01","text":"https://damienbod.com/2019/03/25/grpc-bi-directional-streaming-with-razor-pages-and-a-hosted-service-grpc-client/[gRPC Bi-directional streaming with Razor Pages and a Hosted Service gRPC client | Software Engineering]","title":"2019-04-01"},{"location":"more-references/#2019-04-10","text":"https://andrewlock.net/using-strongly-typed-entity-ids-to-avoid-primitive-obsession-part-1/[Using strongly-typed entity IDs to avoid primitive obsession (Part 1)]","title":"2019-04-10"},{"location":"more-references/#2019-04-23","text":"https://itnext.io/learn-how-you-can-build-a-serverless-graphql-api-on-top-of-a-microservice-architecture-354ed0e67d7e[Learn how YOU can build a Serverless GraphQL API on top of a Microservice architecture]","title":"2019-04-23"},{"location":"references/","text":"References \u00b6 Bibliography \u00b6 Allamaraju, S. (2010). RESTful web services cookbook: Solutions for improving scalability and simplicity. Yahoo Press. Biehl, M. (2016). RESTful API Design: APIs your customers will love. API-University Press. Sturgeon, P. (2016). Build APIs You Wont Hate. Leanpub Webber J., Parastatidis S., Robinson I. (2010). REST in Practice: Hypermedia and Systems Architecture. O'Reilly Media API Design \u00b6 API Design Guide API Strategy 301: API-as-a-Product APIs as a product Characteristics of Good APIs Dogfooding: how to build a great API Driving Architectural Simplicity - The Value, Challenge, and Practice of Simple Solutions New Series: API Design Best Practices Signs you're veering from good API design The 5 laws of API dates and times Web API Design - Crafting Interfaces that Developers Love What Is an API? API First \u00b6 Consumer-Driven Contracts: A Service Evolution Pattern How to Scale Your API Design Process with OpenAPI Intro to RAML - The RESTful API Modeling Language Open API and RAML: Better Together OpenAPI code generators for Visual Studio Planning Your API Strategy for 2018: Tools and Resources to Set Your Team Up for Success Simplify RAML with Resource Types and Traits Swagger is now the OpenAPI Specification What Is the Difference Between Swagger and OpenAPI? Writing OpenAPI (Swagger) Specification Tutorial - Part 8 - Splitting specification file\u00df Documentation \u00b6 Generate beautiful Swagger API documentation from Insomnia Generate C# API Documentation with Wyam GitHub - lord/slate: Beautiful static documentation for your API Ten Extras for Great API Documentation The data, the hypermedia and the documentation The Easiest Ways to Generate API Documentation The Ten Essentials for Good API Documentation The Utopia of API Documentation Ultimate Guide to 30+ API Documentation Solutions What nobody tells you about documentation Which tool to choose for API docs Error \u00b6 Apigility problem/problem/src/main/java/org/zalando/problem at master \u00b7 zalando/problem GraphQL and Queries \u00b6 ...And GraphQL for all? A few things to think about before blindly dumping REST for GraphQL Client-directed query Is using OData\\IQueryable in your Web API an inherently bad thing? So what\u2019s this GraphQL thing I keep hearing about? You Might Not Need GraphQL HTTP \u00b6 HTTP headers Hypermedia \u00b6 Hackable URIs may look nice, but they don\u2019t have much to do with REST and HATEOAS Hypermedia kevinswiber/siren: Structured Interface for Representing Entities, super-rad hypermedia REST: From GET to HATEOAS Self Descriptive HTTP API in ASP.NET Core: HATEOAS Self Descriptive HTTP API in ASP.NET Core: Hypermedia Self Descriptive HTTP API in ASP.NET Core: Siren What is the RESTed NARWHL? REST \u00b6 Anemic REST API Design Tips And Tricks - Getting, creating, updating or deleting multiple resources in one API call Best Practices for Designing a Pragmatic RESTful API Choosing an HTTP Status Code \u2014 Stop Making It Hard DDD & REST - Domain Driven APIs for the web Evolving API Pagination at Slack Fielding Dissertation: CHAPTER 5: Representational State Transfer (REST) Five RESTFul Web Design Patterns Implemented in ASP.NET Core 2.0 Part 1: Content Negotiation Is Proliferation Of Custom Media Types RESTFul? Nobody Understands REST or HTTP PUT or POST: The REST of the Story REST API Design - Resource Modeling REST API Tutorial REST Architectural constraints REST Beyond the Obvious \u2013 API design for ever evolving systems REST efficiency RESTful API Design Tips from Experience RESTful APIs - An accurate description RESTful Resource Naming The RESTafarian flame wars \u2013 common disagreements over REST API design Thoughts on RESTful API Design Top REST API best practices Turning Up The Good On REST APIs Why REST is not a silver bullet for service integration Zalando RESTful API and Event Scheme Guidelines Security \u00b6 API Keys \u2260 Security: Why API Keys Are Not Enough API Keys vs OAuth Tokens vs JSON Web Tokens Strict-Transport-Security The Basics of Web Application Security Using JSON Web Tokens as API Keys Tracing \u00b6 A consistent approach to track correlation IDs through microservices URI \u00b6 Opaque URIs != Unreadable URIs The Opacity Axiom Versioning \u00b6 RESTFul API Versioning Insights Troy Hunt: Your API versioning is wrong, which is why I decided to do it 3 different wrong ways Versioning RESTful Services Versioning URIs","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#bibliography","text":"Allamaraju, S. (2010). RESTful web services cookbook: Solutions for improving scalability and simplicity. Yahoo Press. Biehl, M. (2016). RESTful API Design: APIs your customers will love. API-University Press. Sturgeon, P. (2016). Build APIs You Wont Hate. Leanpub Webber J., Parastatidis S., Robinson I. (2010). REST in Practice: Hypermedia and Systems Architecture. O'Reilly Media","title":"Bibliography"},{"location":"references/#api-design","text":"API Design Guide API Strategy 301: API-as-a-Product APIs as a product Characteristics of Good APIs Dogfooding: how to build a great API Driving Architectural Simplicity - The Value, Challenge, and Practice of Simple Solutions New Series: API Design Best Practices Signs you're veering from good API design The 5 laws of API dates and times Web API Design - Crafting Interfaces that Developers Love What Is an API?","title":"API Design"},{"location":"references/#api-first","text":"Consumer-Driven Contracts: A Service Evolution Pattern How to Scale Your API Design Process with OpenAPI Intro to RAML - The RESTful API Modeling Language Open API and RAML: Better Together OpenAPI code generators for Visual Studio Planning Your API Strategy for 2018: Tools and Resources to Set Your Team Up for Success Simplify RAML with Resource Types and Traits Swagger is now the OpenAPI Specification What Is the Difference Between Swagger and OpenAPI? Writing OpenAPI (Swagger) Specification Tutorial - Part 8 - Splitting specification file\u00df","title":"API First"},{"location":"references/#documentation","text":"Generate beautiful Swagger API documentation from Insomnia Generate C# API Documentation with Wyam GitHub - lord/slate: Beautiful static documentation for your API Ten Extras for Great API Documentation The data, the hypermedia and the documentation The Easiest Ways to Generate API Documentation The Ten Essentials for Good API Documentation The Utopia of API Documentation Ultimate Guide to 30+ API Documentation Solutions What nobody tells you about documentation Which tool to choose for API docs","title":"Documentation"},{"location":"references/#error","text":"Apigility problem/problem/src/main/java/org/zalando/problem at master \u00b7 zalando/problem","title":"Error"},{"location":"references/#graphql-and-queries","text":"...And GraphQL for all? A few things to think about before blindly dumping REST for GraphQL Client-directed query Is using OData\\IQueryable in your Web API an inherently bad thing? So what\u2019s this GraphQL thing I keep hearing about? You Might Not Need GraphQL","title":"GraphQL and Queries"},{"location":"references/#http","text":"HTTP headers","title":"HTTP"},{"location":"references/#hypermedia","text":"Hackable URIs may look nice, but they don\u2019t have much to do with REST and HATEOAS Hypermedia kevinswiber/siren: Structured Interface for Representing Entities, super-rad hypermedia REST: From GET to HATEOAS Self Descriptive HTTP API in ASP.NET Core: HATEOAS Self Descriptive HTTP API in ASP.NET Core: Hypermedia Self Descriptive HTTP API in ASP.NET Core: Siren What is the RESTed NARWHL?","title":"Hypermedia"},{"location":"references/#rest","text":"Anemic REST API Design Tips And Tricks - Getting, creating, updating or deleting multiple resources in one API call Best Practices for Designing a Pragmatic RESTful API Choosing an HTTP Status Code \u2014 Stop Making It Hard DDD & REST - Domain Driven APIs for the web Evolving API Pagination at Slack Fielding Dissertation: CHAPTER 5: Representational State Transfer (REST) Five RESTFul Web Design Patterns Implemented in ASP.NET Core 2.0 Part 1: Content Negotiation Is Proliferation Of Custom Media Types RESTFul? Nobody Understands REST or HTTP PUT or POST: The REST of the Story REST API Design - Resource Modeling REST API Tutorial REST Architectural constraints REST Beyond the Obvious \u2013 API design for ever evolving systems REST efficiency RESTful API Design Tips from Experience RESTful APIs - An accurate description RESTful Resource Naming The RESTafarian flame wars \u2013 common disagreements over REST API design Thoughts on RESTful API Design Top REST API best practices Turning Up The Good On REST APIs Why REST is not a silver bullet for service integration Zalando RESTful API and Event Scheme Guidelines","title":"REST"},{"location":"references/#security","text":"API Keys \u2260 Security: Why API Keys Are Not Enough API Keys vs OAuth Tokens vs JSON Web Tokens Strict-Transport-Security The Basics of Web Application Security Using JSON Web Tokens as API Keys","title":"Security"},{"location":"references/#tracing","text":"A consistent approach to track correlation IDs through microservices","title":"Tracing"},{"location":"references/#uri","text":"Opaque URIs != Unreadable URIs The Opacity Axiom","title":"URI"},{"location":"references/#versioning","text":"RESTFul API Versioning Insights Troy Hunt: Your API versioning is wrong, which is why I decided to do it 3 different wrong ways Versioning RESTful Services Versioning URIs","title":"Versioning"},{"location":"representations/","text":"Representations \u00b6 A representation (request/response) is concrete and real. Here we will offer guidelines as to what makes a good response design. To ensure scalability and longevity of the API, along with providing helpful responses that developers can understand and trust, we will cover what HTTP headers are appropriate in a response, which HTTP status codes to return, as well as how to include descriptive error representations on failures. Bandwidth It might be pertinent to support techniques for reducing bandwidth based on client needs. This is especially true for APIs that have large payloads and/or are used in high-traffic scenarios. Supporting mobile clients with less bandwidth connectivity, would be one such case. To reduce bandwidth and improve responsiveness consider: Compression using Content-Encoding . Partial responses using query parameters. Dividing the size of the payload using Pagination . ETag and If-Modified-Since / If-None-Match for Caching . Using Batch Operations . All these common techniques are discussed in details elsewhere in these guidelines. HTTP Headers \u00b6 Headers ensure visibility, discoverability, routing by proxies, caching, optimistic concurrency, and correct operation of HTTP as an application protocol. Use the following headers to annotate representations that contains a message body. Content-Type \u00b6 Even though it is perfectly acceptable to use only a single format, in order to keep the API flexible and extendable, it is also important to build for the future. Until recently XML was the format of choice, but then along came JSON. DO use Content-Type , to describe the type of representation, including a charset parameter or other parameters defined for that media type. DO include the charset parameter, if the media type supports it, with a value of the character encoding used to convert characters into bytes. DO use the specified encoding, when you receive a representation with a media type that supports the charset parameter. DO let your parser interpret the character set, if you receive an XML representation with a missing charset parameter. AVOID using the text/xml media type for XML-formatted representations. The default charset for text/xml is us-ascii , whereas application/xml uses UTF-8 . Tip Note that Text and XML media types let you specify the character encoding. The JSON media type application/json does not specify a charset parameter, but uses UTF-8 as the default encoding. Content-Length \u00b6 DO use Content-Length , to specify the size in bytes of the body. Or specify Transfer-Encoding: chunked . Some proxies reject POST and PUT requests that contain neither of these headers. Content-Language \u00b6 DO use Content-Language , two-letter RFC 5646 language tag, optionally followed by a hyphen (-) and any two-letter country code. Content-Encoding \u00b6 DO use Content-Encoding . Clients can indicate their preference for Content-Encoding using the Accept-Encoding header, however, there is no standard way for the client to learn whether a server can process representations compressed in a given encoding. Other Common Headers \u00b6 CONSIDER using ETag and/or Last-Modified for caching and/or concurrency, the latter applies for responses only. CONSIDER using Content-MD5 , when sending or receiving large representations over potentially unreliable networks to verify the integrity of the message. DO NOT use Content-MD5 as any measure of security. Custom HTTP Headers \u00b6 Warning Generally custom HTTP headers should be avoided, as they may impede interoperability. We discourage the use of hop-by-hop custom HTTP headers. However, depending on what clients and servers use custom headers for, they can be useful in cases where context information needs to be passed through multiple services in an end-to-end fashion. DO use custom headers for informational purposes. CONSIDER using the convention X-{company-name}-{header-name} , when introducing custom headers, as there is no established convention for naming custom headers. Avoid camelCase (without hyphens). Exceptions are common abbreviations like ID . the usage of X- headers is deprecated (RFC-6648) CONSIDER including the information in a custom HTTP header in the body or the URI, if the information is important for the correct interpretation of the request DO NOT implement clients and servers such that they fail when they do not find expected custom headers. DO NOT use custom HTTP headers to change behavior of HTTP methods, and limit any behavior-changing headers to POST . DO NOT use X-HTTP-Method-Override to override POST , use a distinct resource to process the same request using POST without the header. Any HTTP intermediary between the client and the server may omit custom headers. End-To-End HTTP Headers \u00b6 DO use the specified end-to-end HTTP headers. DO propagate the end-to-end HTTP headers to upstream servers. Header names and values must remain intact. The following may be candidates for end-to-end HTTP headers Correlation ID of the request, which is written into the logs and passed to upstream services. Generic user id that owns the passed (OAuth2) access token. May save additional token validation round trips Sales channel Device type, OS, and version App version, etc. Format and a Media Type \u00b6 HTTP's message format is designed to allow different media types and formats for requests and responses. When it comes to format and media type selection, the rule of thumb is to let the use cases and the types of clients dictate the choice. For this reason, it is important not to pick up a development framework that rigidly enforces one or two formats for all resource with no flexibility to use other formats. DO determine whether there is a standard format and media type that matches your use cases. Check IANA 1 . DO use extensible formats such as XML, Atom Syndication Format, or JSON, if there is no standard media type and format. CONSIDER adding a Content-Disposition: attachment; filename=<status.xls> to give a hint of the filename, when using image formats like image/png or rich document formats like application/vnd.ms-excel or application/pdf to provide alternative representations of data. If you choose to create new media types of your own, consider: DO use a sub-type that ends with +xml in the Content-Type header, when using XML DO use a sub-type starting with vnd ( application/vnd.example.org.user+xml ), when the media type is for private use, DO register your media type with IANA as per RFC 4288, if the media type is for public use, AVOID introducing new application-specific media types unless they are expected to be broadly used, as this may impede interoperability. Warning Although custom media types improve protocol-level visibility, existing protocol-level tools for monitoring, filtering, or routing HTTP traffic pay little or no attention to media types. Hence, using custom media types only for the sake of protocol-level visibility is not necessary. Portable Data Formats \u00b6 DO use decimal, float and double data types defined in the W3C XML Schema for formatting numbers including currency. DO use ISO 3166 (ISO 3166-1-alpha2) codes for countries and dependent territories. DO use ISO 4217 alphabetic or numeric codes for denoting currency. DO use RFC 3339 for dates, times, and date-time values used in representations. Time durations and intervals could conform to ISO 8601 DO use ISO 639-1 language tags for representing the language of text. BCP-47 (based on ISO 639-1) for language variants. DO use time zone identifiers from the Olson Time Zone Database to convey time zones. DO use the HTTP-date format defined in RFC 7231 Section 7.1.1.1. (Date/Time Formats) for Last-Modified , Date , and Expires HTTP headers. AVOID using language-, region-, or country-specific formats or format identifiers, except when the text is meant for presentation to end users. Also, consider defining the format for numbers and integers. The precision could be described as follows, to prevent clients from guessing the precision: Type Format Specified value range integer int32 integer between -2^31 and 2^31-1 integer int64 integer between -2^63 and 2^63-1 integer bigint arbitrarily large signed integer number number float IEEE 754-2008/ISO 60559:2011 binary64 decimal number number double IEEE 754-2008/ISO 60559:2011 binary128 decimal number number decimal arbitrarily precise signed decimal number JSON \u00b6 JSON has become the de facto format within RESTful APIs, by providing a compact, human-readable format for accessing data, which can help minimalize the bandwidth required. One of the advantages to REST is that it is not limited to a single format, and the choice of format should be based on what is most suitable for the clients. DO prefer JSON over XML to encode structured representations, whenever possible. DO include a self link to the resource in each representation. DO add a property to indicate the language, if an object in the representation is localized. DO pretty print the representation by default, as this will help when using a browser to access the public API. Together with compression the additional white-space characters are negligible. DO use gzip compression, if applicable. DO prefer application/json over more specialized and custom media type like application/example.booking+json . DO use consistent property names. DO use camelCase for property names. DO use a subset of us-ascii for property names. The first character must be a letter, an underscore or a dollar sign, and subsequent characters can be a letter, an underscore, a dollar sign, or a number. DO pluralize arrays to indicate they contain multiple values prefer to pluralize array names. This implies that object names should in turn be singular. DO use consistent property values DO use string to represent enumerations. CONSIDER using common field names and semantics to achieve consistency across the API portfolio, as there is very little utility for clients in having different names or value types for these fields across APIs. Candidates are, but not limited to: id , relation_id , type , created , modified/updated . CONSIDER including entity identifiers for each of the application domain entities that make up the resource. CONSIDER the use of an envelope by default. Only use envelopes in exceptional cases. CONSIDER follow RFC-7159 by having (if possible) a serialized object as the top-level structure, since it would allow for future extension. DO NOT return an array as the top-level structure, as this may expose security vunerabilities. Instead use an envelope, like: { list: [...]} . DO NOT use null for boolean property values must not be null. A boolean is essentially a closed enumeration of two values, true and false. If the content has a meaningful null value, strongly prefer to replace the boolean with enumeration of named values or statuses. DO NOT return null for empty array values should not be null. Empty array values can unambiguously be represented as the the empty list, []. Example { \"name\" : \"John\" , \"id\" : \"urn:example:user:1234\" , \"link\" : { \"rel\" : \"self\" , \"href\" : \"https://www.example.org/person/john\" }, \"address\" : { \"id\" : \"urn:example:address:4567\" , \"link\" : { \"rel\" : \"self\" , \"href\" : \"https://www.example.org/person/john/address\" } }, \"content\" : { \"text\" : [{ \"value\" : \"...\" }, { \"lang\" : \"en-GB\" , \"value\" : \"...\" }, { \"lang\" : \"en-US\" , \"value\" : \"...\" }] } } JSON Collections \u00b6 DO return an empty collection, if the query does not match any resources. DO include Pagination Links . XML \u00b6 Suggested topics: Atom resources, AtomPub Service, category documents, AtomPub for feed and entry resources, media resources HTML \u00b6 DO provide HTML representations, for resources that are expected to be consumed by end users. CONSIDER using microformats or RDFx to annotate data within the markup. AVOID avoid designing HTML representations for machine clients. Binary Data \u00b6 DO use multipart media types such as multipart/mixed , multipart/related , or multipart/alternative . CONSIDER providing a link to fetch the binary data as a separate resource as an alternative. Creating and parsing multipart messages in some programming languages may be cumbersome and complex. AVOID encoding binary data within textual formats using Base64 encoding. Pagination \u00b6 Access to lists of data items must support pagination for best client side batch processing and iteration experience. This holds true for all lists that are (potentially) larger than just a few hundred entries. It's almost always a bad idea to return every resource in a database. There are two overall pagination techniques: Offset/limit based pagination, where a numeric offset identifies the first page entry and a limit signifies how many entries are returned. Cursor based (or key based) pagination, where a unique key element identifies the first page entry. Offset/limit-based vs. cursor-based pagination Cursor-based pagination is usually better and more efficient when compared to offset/limit-based pagination. Especially when it comes to high-data volumes and/or storage in NoSQL databases, so there may be a tendency to prefer cursor-based pagination, however, before choosing cursor-based pagination, consider the following: Feature Offset/limit Cursor Comments Usability + - Offset/limit-based pagination is more wellknown, so it may be easier to use for most API clients. Framework support + - Offset/limit-based pagination is more wellknown, so it may have better framework support. Total count + - Cursor-based pagination may not work if you need the total count of results and/or backward iteration support. Use case: Jump to a certain page + - If jumping to a particular page in a range (e.g., 51 of 100) is really a required use case, cursor-based navigation is not feasible. Performance - + Efficient server-side processing using offset/limit-based pagination is hardly feasible for higher data list volumes, especially if they do not reside in the database's main memory. Sharded or NoSQL databases - + Not feasable for offset/limit-based pagination for most database implementations. Tip The technical conception of pagination should also consider user experience related issues. Jumping to a specific page is far less used than navigation via next/previous page links. This favours cursor -based over offset/limit -based pagination. When providing paginated collections: DO use offset/limit-based pagination when resource collections are backed by traditional relation databases. It is more common, well understood in leading databases, and easy for developers. DO use a cursor-based pagination strategy, when the dataset is very large and/or backed by a non-traditional relational data store, like a document database. SHOULD:: Use Simple Hypertext Controls for Pagination and Self-References \u00b6 Hypertext controls for pagination inside collections and self-references should use a simple URI value in combination with their corresponding link relations (next, prev, first, last, self) instead of the extensible common hypertext control See Pagination for information how to best represent paginateable collections. Variability Both pagination strategies suffer when data changes, which may lead to anomalies in result pages. Offset/limit-based pagination may create duplicates or lead to missing entries if rows are inserted or deleted between two subsequent paging requests. When using cursor-based pagination, paging cannot continue when the cursor entry has been deleted while fetching two page Links \u00b6 Use pagination links where applicable. DO add a self link to the collection resource. DO add link to the next page, if the collection is paginated and has a next page. DO a link to the previous page, if the collection is paginated and has a previous page. DO keep collections homogeneous by include only the homogeneous aspects of its member resources. CONSIDER adding an indicator of the size of the collection, either embedded in the collection representation or using a custom HTTP header, like X-Total-Count . CONSIDER using the HTTP Link header to supply pagination links. MUST: Not Use Link Headers with JSON entities \u00b6 We don't allow the use of the Link Header defined by RFC 5988 in conjunction with JSON media types. We prefer links directly embedded in JSON payloads to the uncommon link header syntax. Warning Although the size of the collection is useful for building user interfaces, avoid computing the exact size of the collection. It may be expensive to compute, volatile, or even confidential. Providing a hint is usually good enough. You should avoid providing a total count in your API unless there's a clear need to do so. Very often, there are systems and performance implications to supporting full counts, especially as datasets grow and requests become complex queries or filters that drive full scans (e.g., your database might need to look at all candidate items to count them). While this is an implementation detail relative to the API, it's important to consider your ability to support serving counts over the life of a service. Representations \u00b6 Pagination links can be represented as simplified hypertext controls for pagination within the collection representation. In the following example the collections has an items: [ ... ] attribute holding the items of the current page, as well as pagination links (highlighted below). The collection may also contain additional metadata about the collection (e.g. offset, limit) when necessary. JSON representation containing pagination links { \"self\" : \"https://example.org/articles/authors?offset=4&limit=5\" , \"first\" : \"https://example.org/articles/authors?offset=0&limit=5\" , \"next\" : \"https://example.org/articles/authors?offset=9&limit=5\" , \"prev\" : \"https://example.org/articles/authors?offset=0&limit=5\" , \"last\" : \"https://example.org/articles/authors?offset=100&limit=5\" , \"offset\" : 10 , \"limit\" : 5 , \"items\" : [ { \"href\" : \"https://example.org/authors/123e4567-e89b-12d3-a456-426655440000\" , \"id\" : \"123e4567-e89b-12d3-a456-426655440000\" , \"name\" : \"Kent Beck\" }, { \"href\" : \"https://example.org/authors/987e2343-e89b-12d3-a456-426655440000\" , \"id\" : \"987e2343-e89b-12d3-a456-426655440000\" , \"name\" : \"Martin Fowler\" }, ... ] } An alternative approach may be to using HTTP Link header introduced by RFC 5988 2 : HTTP header Using the HTTP Link header for pagination links: # Request GET /articles/authors?offset=4&limit=5 # Response Link: <https://example.org/articles/authors?offset=9&limit=5>; rel=\"next\", <https://example.org/articles/authors?offset=100&limit=5>; rel=\"last\", <https://example.org/articles/authors?offset=0&limit=5>; rel=\"first\", <https://example.org/articles/authors?offset=0&limit=5>; rel=\"prev\" Or using custom HTTP headers: # Request GET projects/1/stories?offset=1300&limit=300 # Response X-Tracker-Pagination-Total: 1543 X-Tracker-Pagination-Limit: 300 X-Tracker-Pagination-Offset: 1300 X-Tracker-Pagination-Returned: 243 Error Representations \u00b6 Communicate errors through standard HTTP status codes along with details supplied in the response body. Errors are a key element for providing context and visibility, and status codes enable clients to quickly ascertain the status of their request. While well-formed and descriptive error details will tell the client what happened, why it happened, and how to fix it. DO provide error documentation. DO include a Date header with a value indicating the date-time at which the error occurred. DO include a body in the representation formatted and localized using content negotiation or in human-readable HTML or plain text, unless the request method is HEAD . DO provide an identifier or a link that can be used to refer to that error, if you are logging errors on the server side for later tracking or analysis. DO include a brief message describing the error condition DO include a longer description with information on how to fix the error condition, if applicable DO describe any action that the client can take to correct the error or to help the server debug and fix the error, if appropriate. CONSIDER using Problem Detail. The RFC 7807 3 defines the media type application/problem+json. Operations should return that (together with a suitable status code) when any problem occurred during processing and you can give more details than the status code itself can supply, whether it be caused by the client or the server (i.e. both for 4xx or 5xx errors). CONSIDER adding a field breakdown, providing detailed errors, along with a fixed top-level error description, for PUT , PATCH and POST requests. CONSIDER including a link to that document via a Link header or a link in the body, if information to correct or debug the error is available as a separate human-readable document. Also consider tracking the hits to these pages to see what areas tend to be more troublesome for your users \u2013 allowing you to provide even better documentation and/or build a better API. AVOID including details such as stack traces, errors from database connections failures, etc. AVOID using generic or non-descriptive error messages as they are often one of the biggest hinderances to API integration, as developers may struggle for hours trying to figure out why the call is failing, even misinterpreting the intent of the error message altogether. DO NOT return 2xx and include a message body that describes an error condition. Doing so prevents HTTP-aware software from detecting errors. Error example { \"error\" : { \"code\" : \"e3526\" , \"message\" : \"Missing UserID\" , \"description\" : \"A UserID is required to edit a user.\" , \"link\" : \"https://docs.mysite.com/errors/e3526/\" } } Validation error example { \"code\" : 1024 , \"message\" : \"Validation Failed\" , \"errors\" : [ { \"code\" : 5432 , \"field\" : \"first_name\" , \"message\" : \"First name cannot have fancy characters\" }, { \"code\" : 5622 , \"field\" : \"password\" , \"message\" : \"Password cannot be blank\" } ] } HTTP Status Codes \u00b6 Always return meaningful HTTP Status Codes! By using meaningful status codes, developers can quickly see what is happening with the application and do a \"quick check\" for errors without having to rely on the body's response. DO return 2xx when the request was received, understood, and accepted. DO return 3xx when the client needs to take further action. Include the necessary information in order for the client to complete the request. DO return a representation with a 4xx status code, for errors due to client inputs. DO return a representation with a 5xx status code, for errors due to server implementation or its current state. DO use the most specific HTTP status code for your concrete resource request processing status or error situation. DO provide good documentation in the API definition when using HTTP status codes that are less commonly used and not listed below. DO NOT invent new HTTP status codes; only use standardized HTTP status codes and consistent with its intended semantics. 2xx \u00b6 For successful responses: DO return 200 OK when the request completed without issues. DO return 201 Created when a resource was successfully created. Always set the Location header with the URI of the newly created resource. Return either an empty response or the created resource. DO return 202 Accepted when a request was successful and will be processed asynchronously. DO return 204 No content when there is no response body. DO return 207 Multi-Status when the response body contains multiple status informations for different parts of a batch/bulk request. 3xx \u00b6 For any action required by the client: DO return 301 Moved Permanently when this and all future requests should be directed to the URI specified in the Location header. DO return 303 See Other when the response to the request can be found under the URI specified in the Location header using a GET method. DO return 304 Not Modified when the resource has not been modified since the date or version passed via request headers If-Modified-Since or If-None-Match . 4xx \u00b6 For client errors: DO return 400 Bad Request when your server cannot decipher client requests because of syntactical errors. DO return 401 Unauthorized when the client is not authorized to access the resource, but may be able to gain access after authentication. If your server will not let the client access the resource even after authentication return 403 Forbidden instead. When returning this error code, include a WWW-Authenticate header field with the authentication method to use. DO return 403 Forbidden when your serer will not let the client gain access (authenticated or not). DO return 404 Not Found when the resource is not found. If possible, return a reason in the message body. DO return 405 Not Allowed when an HTTP method is not allowed. Return an Allow header with methods that are valid for the resource. DO return 406 Not Acceptable when the resource can only generate content not acceptable according to the Accept headers sent in the request. DO return 408 Request Timeout when the server times out waiting for the resource. DO return 409 Conflict when the request conflicts with the current state of the resource. Include a body explaining the reason. DO return 410 Gone when the resource used to exist, but it does not anymore. You may not be able to return this unless you tracked deleted resources, then return 404 Not Found . DO return 412 Precondition Failed for conditional requests, when If-Match and/or ETag preconditions fail. DO return 413 Request Entity Too Large when the body of a POST or PUT is too large. If possible, specify what is allowed in the body and provide alternatives. DO return 415 Unsupported Media Type when a client sends the message body in a form that the server does not understand. DO return 423 Locked when using pessimistic locking. DO return 428 Precondition Required when the server requires the request to be conditional (e.g. to make sure that the \"lost update problem\" is avoided). DO return 429 Too Many Requests when the client does not consider rate limiting and sent too many requests. Include headers to indicate rate limits. 5xx \u00b6 For server errors: DO return 500 Internal Server Error when your code on the server side failed due to come implementation bug. DO return 501 Not Implemented to indicate that a future feature may become available. DO return 503 Service Unavailable when the server cannot fulfill the request either for some specific interval or undetermined amount of time. If possible, include a Retry-After response header with either a date or a number of seconds as a hint. Entity Identifiers in Representations \u00b6 When an API is part of a larger API portfolio or system, information may cross several system boundaries, and entity identifiers can be used to uniformly identify, cross-reference or transform data. Especially when a lot of different technologies are envolved (RPC, SOAP, asynchronous messaging, stored procedures, etc.) and/or third-party applications, the only common denominator may very well be entity identifiers. CONSIDER consider formatting identifiers as URNs, for each of the application domain entities included in the representation of a resource, to maintain uniqueness of identifiers. CONSIDER using strings rather than number types for identifiers, as this gives more flexibility to evolve the identifier naming scheme. Accordingly, if used as identifiers, UUIDs should not be qualified using a format property. CONSIDER using UUIDs as entity identifiers for scaling in high frequency and near real time use cases, as they can be generated without collisions in a distributed, non-coordinated way and without additional server roundtrips. CONSIDER limiting the use of UUIDs as entity identifiers when it is not strictly necessary, if it possible to come up with a better naming scheme, if the id volume is low, or if the ids have widespread steering functionality. The Internet Assigned Number Authority ( IANA ) media type registry. \u21a9 Web Linking . \u21a9 Problem Details for HTTP APIs . \u21a9","title":"Representations"},{"location":"representations/#representations","text":"A representation (request/response) is concrete and real. Here we will offer guidelines as to what makes a good response design. To ensure scalability and longevity of the API, along with providing helpful responses that developers can understand and trust, we will cover what HTTP headers are appropriate in a response, which HTTP status codes to return, as well as how to include descriptive error representations on failures. Bandwidth It might be pertinent to support techniques for reducing bandwidth based on client needs. This is especially true for APIs that have large payloads and/or are used in high-traffic scenarios. Supporting mobile clients with less bandwidth connectivity, would be one such case. To reduce bandwidth and improve responsiveness consider: Compression using Content-Encoding . Partial responses using query parameters. Dividing the size of the payload using Pagination . ETag and If-Modified-Since / If-None-Match for Caching . Using Batch Operations . All these common techniques are discussed in details elsewhere in these guidelines.","title":"Representations"},{"location":"representations/#http-headers","text":"Headers ensure visibility, discoverability, routing by proxies, caching, optimistic concurrency, and correct operation of HTTP as an application protocol. Use the following headers to annotate representations that contains a message body.","title":"HTTP Headers"},{"location":"representations/#content-type","text":"Even though it is perfectly acceptable to use only a single format, in order to keep the API flexible and extendable, it is also important to build for the future. Until recently XML was the format of choice, but then along came JSON. DO use Content-Type , to describe the type of representation, including a charset parameter or other parameters defined for that media type. DO include the charset parameter, if the media type supports it, with a value of the character encoding used to convert characters into bytes. DO use the specified encoding, when you receive a representation with a media type that supports the charset parameter. DO let your parser interpret the character set, if you receive an XML representation with a missing charset parameter. AVOID using the text/xml media type for XML-formatted representations. The default charset for text/xml is us-ascii , whereas application/xml uses UTF-8 . Tip Note that Text and XML media types let you specify the character encoding. The JSON media type application/json does not specify a charset parameter, but uses UTF-8 as the default encoding.","title":"Content-Type"},{"location":"representations/#content-length","text":"DO use Content-Length , to specify the size in bytes of the body. Or specify Transfer-Encoding: chunked . Some proxies reject POST and PUT requests that contain neither of these headers.","title":"Content-Length"},{"location":"representations/#content-language","text":"DO use Content-Language , two-letter RFC 5646 language tag, optionally followed by a hyphen (-) and any two-letter country code.","title":"Content-Language"},{"location":"representations/#content-encoding","text":"DO use Content-Encoding . Clients can indicate their preference for Content-Encoding using the Accept-Encoding header, however, there is no standard way for the client to learn whether a server can process representations compressed in a given encoding.","title":"Content-Encoding"},{"location":"representations/#other-common-headers","text":"CONSIDER using ETag and/or Last-Modified for caching and/or concurrency, the latter applies for responses only. CONSIDER using Content-MD5 , when sending or receiving large representations over potentially unreliable networks to verify the integrity of the message. DO NOT use Content-MD5 as any measure of security.","title":"Other Common Headers"},{"location":"representations/#custom-http-headers","text":"Warning Generally custom HTTP headers should be avoided, as they may impede interoperability. We discourage the use of hop-by-hop custom HTTP headers. However, depending on what clients and servers use custom headers for, they can be useful in cases where context information needs to be passed through multiple services in an end-to-end fashion. DO use custom headers for informational purposes. CONSIDER using the convention X-{company-name}-{header-name} , when introducing custom headers, as there is no established convention for naming custom headers. Avoid camelCase (without hyphens). Exceptions are common abbreviations like ID . the usage of X- headers is deprecated (RFC-6648) CONSIDER including the information in a custom HTTP header in the body or the URI, if the information is important for the correct interpretation of the request DO NOT implement clients and servers such that they fail when they do not find expected custom headers. DO NOT use custom HTTP headers to change behavior of HTTP methods, and limit any behavior-changing headers to POST . DO NOT use X-HTTP-Method-Override to override POST , use a distinct resource to process the same request using POST without the header. Any HTTP intermediary between the client and the server may omit custom headers.","title":"Custom HTTP Headers"},{"location":"representations/#end-to-end-http-headers","text":"DO use the specified end-to-end HTTP headers. DO propagate the end-to-end HTTP headers to upstream servers. Header names and values must remain intact. The following may be candidates for end-to-end HTTP headers Correlation ID of the request, which is written into the logs and passed to upstream services. Generic user id that owns the passed (OAuth2) access token. May save additional token validation round trips Sales channel Device type, OS, and version App version, etc.","title":"End-To-End HTTP Headers"},{"location":"representations/#format-and-a-media-type","text":"HTTP's message format is designed to allow different media types and formats for requests and responses. When it comes to format and media type selection, the rule of thumb is to let the use cases and the types of clients dictate the choice. For this reason, it is important not to pick up a development framework that rigidly enforces one or two formats for all resource with no flexibility to use other formats. DO determine whether there is a standard format and media type that matches your use cases. Check IANA 1 . DO use extensible formats such as XML, Atom Syndication Format, or JSON, if there is no standard media type and format. CONSIDER adding a Content-Disposition: attachment; filename=<status.xls> to give a hint of the filename, when using image formats like image/png or rich document formats like application/vnd.ms-excel or application/pdf to provide alternative representations of data. If you choose to create new media types of your own, consider: DO use a sub-type that ends with +xml in the Content-Type header, when using XML DO use a sub-type starting with vnd ( application/vnd.example.org.user+xml ), when the media type is for private use, DO register your media type with IANA as per RFC 4288, if the media type is for public use, AVOID introducing new application-specific media types unless they are expected to be broadly used, as this may impede interoperability. Warning Although custom media types improve protocol-level visibility, existing protocol-level tools for monitoring, filtering, or routing HTTP traffic pay little or no attention to media types. Hence, using custom media types only for the sake of protocol-level visibility is not necessary.","title":"Format and a Media Type"},{"location":"representations/#portable-data-formats","text":"DO use decimal, float and double data types defined in the W3C XML Schema for formatting numbers including currency. DO use ISO 3166 (ISO 3166-1-alpha2) codes for countries and dependent territories. DO use ISO 4217 alphabetic or numeric codes for denoting currency. DO use RFC 3339 for dates, times, and date-time values used in representations. Time durations and intervals could conform to ISO 8601 DO use ISO 639-1 language tags for representing the language of text. BCP-47 (based on ISO 639-1) for language variants. DO use time zone identifiers from the Olson Time Zone Database to convey time zones. DO use the HTTP-date format defined in RFC 7231 Section 7.1.1.1. (Date/Time Formats) for Last-Modified , Date , and Expires HTTP headers. AVOID using language-, region-, or country-specific formats or format identifiers, except when the text is meant for presentation to end users. Also, consider defining the format for numbers and integers. The precision could be described as follows, to prevent clients from guessing the precision: Type Format Specified value range integer int32 integer between -2^31 and 2^31-1 integer int64 integer between -2^63 and 2^63-1 integer bigint arbitrarily large signed integer number number float IEEE 754-2008/ISO 60559:2011 binary64 decimal number number double IEEE 754-2008/ISO 60559:2011 binary128 decimal number number decimal arbitrarily precise signed decimal number","title":"Portable Data Formats"},{"location":"representations/#json","text":"JSON has become the de facto format within RESTful APIs, by providing a compact, human-readable format for accessing data, which can help minimalize the bandwidth required. One of the advantages to REST is that it is not limited to a single format, and the choice of format should be based on what is most suitable for the clients. DO prefer JSON over XML to encode structured representations, whenever possible. DO include a self link to the resource in each representation. DO add a property to indicate the language, if an object in the representation is localized. DO pretty print the representation by default, as this will help when using a browser to access the public API. Together with compression the additional white-space characters are negligible. DO use gzip compression, if applicable. DO prefer application/json over more specialized and custom media type like application/example.booking+json . DO use consistent property names. DO use camelCase for property names. DO use a subset of us-ascii for property names. The first character must be a letter, an underscore or a dollar sign, and subsequent characters can be a letter, an underscore, a dollar sign, or a number. DO pluralize arrays to indicate they contain multiple values prefer to pluralize array names. This implies that object names should in turn be singular. DO use consistent property values DO use string to represent enumerations. CONSIDER using common field names and semantics to achieve consistency across the API portfolio, as there is very little utility for clients in having different names or value types for these fields across APIs. Candidates are, but not limited to: id , relation_id , type , created , modified/updated . CONSIDER including entity identifiers for each of the application domain entities that make up the resource. CONSIDER the use of an envelope by default. Only use envelopes in exceptional cases. CONSIDER follow RFC-7159 by having (if possible) a serialized object as the top-level structure, since it would allow for future extension. DO NOT return an array as the top-level structure, as this may expose security vunerabilities. Instead use an envelope, like: { list: [...]} . DO NOT use null for boolean property values must not be null. A boolean is essentially a closed enumeration of two values, true and false. If the content has a meaningful null value, strongly prefer to replace the boolean with enumeration of named values or statuses. DO NOT return null for empty array values should not be null. Empty array values can unambiguously be represented as the the empty list, []. Example { \"name\" : \"John\" , \"id\" : \"urn:example:user:1234\" , \"link\" : { \"rel\" : \"self\" , \"href\" : \"https://www.example.org/person/john\" }, \"address\" : { \"id\" : \"urn:example:address:4567\" , \"link\" : { \"rel\" : \"self\" , \"href\" : \"https://www.example.org/person/john/address\" } }, \"content\" : { \"text\" : [{ \"value\" : \"...\" }, { \"lang\" : \"en-GB\" , \"value\" : \"...\" }, { \"lang\" : \"en-US\" , \"value\" : \"...\" }] } }","title":"JSON"},{"location":"representations/#json-collections","text":"DO return an empty collection, if the query does not match any resources. DO include Pagination Links .","title":"JSON Collections"},{"location":"representations/#xml","text":"Suggested topics: Atom resources, AtomPub Service, category documents, AtomPub for feed and entry resources, media resources","title":"XML"},{"location":"representations/#html","text":"DO provide HTML representations, for resources that are expected to be consumed by end users. CONSIDER using microformats or RDFx to annotate data within the markup. AVOID avoid designing HTML representations for machine clients.","title":"HTML"},{"location":"representations/#binary-data","text":"DO use multipart media types such as multipart/mixed , multipart/related , or multipart/alternative . CONSIDER providing a link to fetch the binary data as a separate resource as an alternative. Creating and parsing multipart messages in some programming languages may be cumbersome and complex. AVOID encoding binary data within textual formats using Base64 encoding.","title":"Binary Data"},{"location":"representations/#pagination","text":"Access to lists of data items must support pagination for best client side batch processing and iteration experience. This holds true for all lists that are (potentially) larger than just a few hundred entries. It's almost always a bad idea to return every resource in a database. There are two overall pagination techniques: Offset/limit based pagination, where a numeric offset identifies the first page entry and a limit signifies how many entries are returned. Cursor based (or key based) pagination, where a unique key element identifies the first page entry. Offset/limit-based vs. cursor-based pagination Cursor-based pagination is usually better and more efficient when compared to offset/limit-based pagination. Especially when it comes to high-data volumes and/or storage in NoSQL databases, so there may be a tendency to prefer cursor-based pagination, however, before choosing cursor-based pagination, consider the following: Feature Offset/limit Cursor Comments Usability + - Offset/limit-based pagination is more wellknown, so it may be easier to use for most API clients. Framework support + - Offset/limit-based pagination is more wellknown, so it may have better framework support. Total count + - Cursor-based pagination may not work if you need the total count of results and/or backward iteration support. Use case: Jump to a certain page + - If jumping to a particular page in a range (e.g., 51 of 100) is really a required use case, cursor-based navigation is not feasible. Performance - + Efficient server-side processing using offset/limit-based pagination is hardly feasible for higher data list volumes, especially if they do not reside in the database's main memory. Sharded or NoSQL databases - + Not feasable for offset/limit-based pagination for most database implementations. Tip The technical conception of pagination should also consider user experience related issues. Jumping to a specific page is far less used than navigation via next/previous page links. This favours cursor -based over offset/limit -based pagination. When providing paginated collections: DO use offset/limit-based pagination when resource collections are backed by traditional relation databases. It is more common, well understood in leading databases, and easy for developers. DO use a cursor-based pagination strategy, when the dataset is very large and/or backed by a non-traditional relational data store, like a document database.","title":"Pagination"},{"location":"representations/#should-use-simple-hypertext-controls-for-pagination-and-self-references","text":"Hypertext controls for pagination inside collections and self-references should use a simple URI value in combination with their corresponding link relations (next, prev, first, last, self) instead of the extensible common hypertext control See Pagination for information how to best represent paginateable collections. Variability Both pagination strategies suffer when data changes, which may lead to anomalies in result pages. Offset/limit-based pagination may create duplicates or lead to missing entries if rows are inserted or deleted between two subsequent paging requests. When using cursor-based pagination, paging cannot continue when the cursor entry has been deleted while fetching two page","title":"SHOULD:: Use Simple Hypertext Controls for Pagination and Self-References"},{"location":"representations/#links","text":"Use pagination links where applicable. DO add a self link to the collection resource. DO add link to the next page, if the collection is paginated and has a next page. DO a link to the previous page, if the collection is paginated and has a previous page. DO keep collections homogeneous by include only the homogeneous aspects of its member resources. CONSIDER adding an indicator of the size of the collection, either embedded in the collection representation or using a custom HTTP header, like X-Total-Count . CONSIDER using the HTTP Link header to supply pagination links.","title":"Links"},{"location":"representations/#must-not-use-link-headers-with-json-entities","text":"We don't allow the use of the Link Header defined by RFC 5988 in conjunction with JSON media types. We prefer links directly embedded in JSON payloads to the uncommon link header syntax. Warning Although the size of the collection is useful for building user interfaces, avoid computing the exact size of the collection. It may be expensive to compute, volatile, or even confidential. Providing a hint is usually good enough. You should avoid providing a total count in your API unless there's a clear need to do so. Very often, there are systems and performance implications to supporting full counts, especially as datasets grow and requests become complex queries or filters that drive full scans (e.g., your database might need to look at all candidate items to count them). While this is an implementation detail relative to the API, it's important to consider your ability to support serving counts over the life of a service.","title":"MUST: Not Use Link Headers with JSON entities"},{"location":"representations/#representations_1","text":"Pagination links can be represented as simplified hypertext controls for pagination within the collection representation. In the following example the collections has an items: [ ... ] attribute holding the items of the current page, as well as pagination links (highlighted below). The collection may also contain additional metadata about the collection (e.g. offset, limit) when necessary. JSON representation containing pagination links { \"self\" : \"https://example.org/articles/authors?offset=4&limit=5\" , \"first\" : \"https://example.org/articles/authors?offset=0&limit=5\" , \"next\" : \"https://example.org/articles/authors?offset=9&limit=5\" , \"prev\" : \"https://example.org/articles/authors?offset=0&limit=5\" , \"last\" : \"https://example.org/articles/authors?offset=100&limit=5\" , \"offset\" : 10 , \"limit\" : 5 , \"items\" : [ { \"href\" : \"https://example.org/authors/123e4567-e89b-12d3-a456-426655440000\" , \"id\" : \"123e4567-e89b-12d3-a456-426655440000\" , \"name\" : \"Kent Beck\" }, { \"href\" : \"https://example.org/authors/987e2343-e89b-12d3-a456-426655440000\" , \"id\" : \"987e2343-e89b-12d3-a456-426655440000\" , \"name\" : \"Martin Fowler\" }, ... ] } An alternative approach may be to using HTTP Link header introduced by RFC 5988 2 : HTTP header Using the HTTP Link header for pagination links: # Request GET /articles/authors?offset=4&limit=5 # Response Link: <https://example.org/articles/authors?offset=9&limit=5>; rel=\"next\", <https://example.org/articles/authors?offset=100&limit=5>; rel=\"last\", <https://example.org/articles/authors?offset=0&limit=5>; rel=\"first\", <https://example.org/articles/authors?offset=0&limit=5>; rel=\"prev\" Or using custom HTTP headers: # Request GET projects/1/stories?offset=1300&limit=300 # Response X-Tracker-Pagination-Total: 1543 X-Tracker-Pagination-Limit: 300 X-Tracker-Pagination-Offset: 1300 X-Tracker-Pagination-Returned: 243","title":"Representations"},{"location":"representations/#error-representations","text":"Communicate errors through standard HTTP status codes along with details supplied in the response body. Errors are a key element for providing context and visibility, and status codes enable clients to quickly ascertain the status of their request. While well-formed and descriptive error details will tell the client what happened, why it happened, and how to fix it. DO provide error documentation. DO include a Date header with a value indicating the date-time at which the error occurred. DO include a body in the representation formatted and localized using content negotiation or in human-readable HTML or plain text, unless the request method is HEAD . DO provide an identifier or a link that can be used to refer to that error, if you are logging errors on the server side for later tracking or analysis. DO include a brief message describing the error condition DO include a longer description with information on how to fix the error condition, if applicable DO describe any action that the client can take to correct the error or to help the server debug and fix the error, if appropriate. CONSIDER using Problem Detail. The RFC 7807 3 defines the media type application/problem+json. Operations should return that (together with a suitable status code) when any problem occurred during processing and you can give more details than the status code itself can supply, whether it be caused by the client or the server (i.e. both for 4xx or 5xx errors). CONSIDER adding a field breakdown, providing detailed errors, along with a fixed top-level error description, for PUT , PATCH and POST requests. CONSIDER including a link to that document via a Link header or a link in the body, if information to correct or debug the error is available as a separate human-readable document. Also consider tracking the hits to these pages to see what areas tend to be more troublesome for your users \u2013 allowing you to provide even better documentation and/or build a better API. AVOID including details such as stack traces, errors from database connections failures, etc. AVOID using generic or non-descriptive error messages as they are often one of the biggest hinderances to API integration, as developers may struggle for hours trying to figure out why the call is failing, even misinterpreting the intent of the error message altogether. DO NOT return 2xx and include a message body that describes an error condition. Doing so prevents HTTP-aware software from detecting errors. Error example { \"error\" : { \"code\" : \"e3526\" , \"message\" : \"Missing UserID\" , \"description\" : \"A UserID is required to edit a user.\" , \"link\" : \"https://docs.mysite.com/errors/e3526/\" } } Validation error example { \"code\" : 1024 , \"message\" : \"Validation Failed\" , \"errors\" : [ { \"code\" : 5432 , \"field\" : \"first_name\" , \"message\" : \"First name cannot have fancy characters\" }, { \"code\" : 5622 , \"field\" : \"password\" , \"message\" : \"Password cannot be blank\" } ] }","title":"Error Representations"},{"location":"representations/#http-status-codes","text":"Always return meaningful HTTP Status Codes! By using meaningful status codes, developers can quickly see what is happening with the application and do a \"quick check\" for errors without having to rely on the body's response. DO return 2xx when the request was received, understood, and accepted. DO return 3xx when the client needs to take further action. Include the necessary information in order for the client to complete the request. DO return a representation with a 4xx status code, for errors due to client inputs. DO return a representation with a 5xx status code, for errors due to server implementation or its current state. DO use the most specific HTTP status code for your concrete resource request processing status or error situation. DO provide good documentation in the API definition when using HTTP status codes that are less commonly used and not listed below. DO NOT invent new HTTP status codes; only use standardized HTTP status codes and consistent with its intended semantics.","title":"HTTP Status Codes"},{"location":"representations/#2xx","text":"For successful responses: DO return 200 OK when the request completed without issues. DO return 201 Created when a resource was successfully created. Always set the Location header with the URI of the newly created resource. Return either an empty response or the created resource. DO return 202 Accepted when a request was successful and will be processed asynchronously. DO return 204 No content when there is no response body. DO return 207 Multi-Status when the response body contains multiple status informations for different parts of a batch/bulk request.","title":"2xx"},{"location":"representations/#3xx","text":"For any action required by the client: DO return 301 Moved Permanently when this and all future requests should be directed to the URI specified in the Location header. DO return 303 See Other when the response to the request can be found under the URI specified in the Location header using a GET method. DO return 304 Not Modified when the resource has not been modified since the date or version passed via request headers If-Modified-Since or If-None-Match .","title":"3xx"},{"location":"representations/#4xx","text":"For client errors: DO return 400 Bad Request when your server cannot decipher client requests because of syntactical errors. DO return 401 Unauthorized when the client is not authorized to access the resource, but may be able to gain access after authentication. If your server will not let the client access the resource even after authentication return 403 Forbidden instead. When returning this error code, include a WWW-Authenticate header field with the authentication method to use. DO return 403 Forbidden when your serer will not let the client gain access (authenticated or not). DO return 404 Not Found when the resource is not found. If possible, return a reason in the message body. DO return 405 Not Allowed when an HTTP method is not allowed. Return an Allow header with methods that are valid for the resource. DO return 406 Not Acceptable when the resource can only generate content not acceptable according to the Accept headers sent in the request. DO return 408 Request Timeout when the server times out waiting for the resource. DO return 409 Conflict when the request conflicts with the current state of the resource. Include a body explaining the reason. DO return 410 Gone when the resource used to exist, but it does not anymore. You may not be able to return this unless you tracked deleted resources, then return 404 Not Found . DO return 412 Precondition Failed for conditional requests, when If-Match and/or ETag preconditions fail. DO return 413 Request Entity Too Large when the body of a POST or PUT is too large. If possible, specify what is allowed in the body and provide alternatives. DO return 415 Unsupported Media Type when a client sends the message body in a form that the server does not understand. DO return 423 Locked when using pessimistic locking. DO return 428 Precondition Required when the server requires the request to be conditional (e.g. to make sure that the \"lost update problem\" is avoided). DO return 429 Too Many Requests when the client does not consider rate limiting and sent too many requests. Include headers to indicate rate limits.","title":"4xx"},{"location":"representations/#5xx","text":"For server errors: DO return 500 Internal Server Error when your code on the server side failed due to come implementation bug. DO return 501 Not Implemented to indicate that a future feature may become available. DO return 503 Service Unavailable when the server cannot fulfill the request either for some specific interval or undetermined amount of time. If possible, include a Retry-After response header with either a date or a number of seconds as a hint.","title":"5xx"},{"location":"representations/#entity-identifiers-in-representations","text":"When an API is part of a larger API portfolio or system, information may cross several system boundaries, and entity identifiers can be used to uniformly identify, cross-reference or transform data. Especially when a lot of different technologies are envolved (RPC, SOAP, asynchronous messaging, stored procedures, etc.) and/or third-party applications, the only common denominator may very well be entity identifiers. CONSIDER consider formatting identifiers as URNs, for each of the application domain entities included in the representation of a resource, to maintain uniqueness of identifiers. CONSIDER using strings rather than number types for identifiers, as this gives more flexibility to evolve the identifier naming scheme. Accordingly, if used as identifiers, UUIDs should not be qualified using a format property. CONSIDER using UUIDs as entity identifiers for scaling in high frequency and near real time use cases, as they can be generated without collisions in a distributed, non-coordinated way and without additional server roundtrips. CONSIDER limiting the use of UUIDs as entity identifiers when it is not strictly necessary, if it possible to come up with a better naming scheme, if the id volume is low, or if the ids have widespread steering functionality. The Internet Assigned Number Authority ( IANA ) media type registry. \u21a9 Web Linking . \u21a9 Problem Details for HTTP APIs . \u21a9","title":"Entity Identifiers in Representations"},{"location":"resource-identifiers/","text":"Unique Resource Identifiers (URIs) \u00b6 An important aspect of API design is to think about how to design unique resource identifiers, and as such, URIs should be treated as opaque resource identifiers 1 . The opacity of URIs helps reduce coupling between servers and clients. This has nothing to do with readability or hackability, both of which may be extremely important aspects for developers consuming the API, where: readable URIs help developers understand something about the resource. hackable URIs are manipulated by altering/removing portions of the path or query, and can help developers locate other resources. Important Clients should not be concerned with the design of URIs, nor should clients try to pick apart URIs in order to gather information from them. Instead URIs should be discovered through links (and the submission of forms). For instance, clients must not use the fact that a URI ends with .xml to infer that it resolves to an XML representation, instead it must rely on the Content-Type header of the response. Designing URIs \u00b6 When done successfully the result, of a thoughtful URI design, can be the most important design affordance 2 of your API. So when designing URIs: DO design URIs to last a long time - Cool URIs don't change . DO design URIs based on stable concepts, identifiers, and information. URIs cannot be permanent if the concepts or identifiers used cannot be permanent for business, technical, or security reasons. DO keep your base URIs simple and intuitive. DO use lowercase for URIs. DO use domains and subdomains to logically group or partition resources for localization, distribution, or to enforce various monitoring or security policies. DO use forward-slash ( / ) in the path segment to indicate a hierarchical relationship between resources. DO use the hyphen ( - ) or ( _ ) characters to improve the readability of long path segments. Pick one or the other for consistency . DO use ampersand ( & ) to separate query parameters. DO use the URI only to determine which resource should process a request. CONSIDER providing URIs at runtime using links in the body of representations or headers, whenever appropriate. CONSIDER using /api as the first path segment, when the API also supports non-public APIs, e.g., for specific operational support functions. Otherwise, consider forgoing the /api prefix. CONSIDER using comma ( , ) and semi-colon ( ; ) to indicate nonhierarchical elements in the path segment. The semi-colon ( ; ) convention is used to identify matrix parameters. Not all libraries recognize these as separators and may require custom coding to extract these parameters . CONSIDER using URI templates, if it is impractical to supply all the possible URIs in the representation (e.g., ad hoc searching). CONSIDER disregarding opacity to protect against tampering using digitally signed URIs, or to protect sensitive information by encrypting parts of the URI. AVOID including file extensions, instead rely on the media types. AVOID using trailing forward slash, as some frameworks may incorrectly remove or add such slashes during URI normalization. AVOID expecting clients to construct URIs. AVOID unnecessary query strings in URIs. AVOID leaking implementation details to clients, by keeping the creating of URIs on the server, as those details will become part of the public interface. DO NOT use an URI as a generic gateway, by tunneling repeated state changes over POST using the same URI. DO NOT use custom headers to overload URIs. URIs for Queries \u00b6 Queries usually involve filtering, sorting and projections. When providing query support for these and other actions: DO use snake_case ( never camelCase ) for Query Parameters. DO use query parameters to let clients specify filter conditions, sort fields, and projections. DO treat query parameters as optional with sensible defaults. DO document each parameter. CONSIDER using q (e.g. used by browser tab completion) as the default query parameter. CONSIDER using a format query parameter, if standard content negotiation is not possible. AVOID ad hoc queries that use general-purpose query languages such as SQL or XPath . AVOID Range requests for implementing queries. Partial Responses \u00b6 CONSIDER using a fields query parameter for projections (partial responses), like https://www.example.org/customers?fields=name,gender,birthday or https://example.org/customer?fields=(firstName,user(email)) and ! to negate field selection. Depending on the use case and payload size, it can reduce network bandwidth and reduce filtering on clients. CONSIDER using a view query parameter for predefined projections, like https://www.example.org/customers?view=summary CONSIDER supporting aliases for commonly used queries (it may also improve cacheability). For instance, GET /tickets/recently_closed CONSIDER using embed to allow for resource expansion. Embedding related resources can help reduce the number of requests. Sorting \u00b6 CONSIDER using a generic sort parameter to describe sorting rules. To accommodate more complex sorting requirements, let the sort parameter take a list of comma-separated fields, each with a possible unary negative to imply descending sort order. Like: GET /tickets?sort=-priority,created_at Pagination \u00b6 CONSIDER using limit and offset , like https://www.example.org/authors?offset=50&limit=25 , when resource collections are backed by traditional relation databases, as these ties into the general implementation on the data store. Opt for cursor and a cursor-based pagination strategy, when the dataset is very large and/or backed by a non-traditional relational data store, like a document database. For more information see Pagination . CONSIDER using sensible defaults for pagination, e.g. limit=10 and offset=0. The pagination defaults are dependent on your data size. Opaque URIs != Unreadable URIs \u21a9 a design property that communicates how something should be used without requiring documentation \u21a9","title":"Resource Identifiers"},{"location":"resource-identifiers/#unique-resource-identifiers-uris","text":"An important aspect of API design is to think about how to design unique resource identifiers, and as such, URIs should be treated as opaque resource identifiers 1 . The opacity of URIs helps reduce coupling between servers and clients. This has nothing to do with readability or hackability, both of which may be extremely important aspects for developers consuming the API, where: readable URIs help developers understand something about the resource. hackable URIs are manipulated by altering/removing portions of the path or query, and can help developers locate other resources. Important Clients should not be concerned with the design of URIs, nor should clients try to pick apart URIs in order to gather information from them. Instead URIs should be discovered through links (and the submission of forms). For instance, clients must not use the fact that a URI ends with .xml to infer that it resolves to an XML representation, instead it must rely on the Content-Type header of the response.","title":"Unique Resource Identifiers (URIs)"},{"location":"resource-identifiers/#designing-uris","text":"When done successfully the result, of a thoughtful URI design, can be the most important design affordance 2 of your API. So when designing URIs: DO design URIs to last a long time - Cool URIs don't change . DO design URIs based on stable concepts, identifiers, and information. URIs cannot be permanent if the concepts or identifiers used cannot be permanent for business, technical, or security reasons. DO keep your base URIs simple and intuitive. DO use lowercase for URIs. DO use domains and subdomains to logically group or partition resources for localization, distribution, or to enforce various monitoring or security policies. DO use forward-slash ( / ) in the path segment to indicate a hierarchical relationship between resources. DO use the hyphen ( - ) or ( _ ) characters to improve the readability of long path segments. Pick one or the other for consistency . DO use ampersand ( & ) to separate query parameters. DO use the URI only to determine which resource should process a request. CONSIDER providing URIs at runtime using links in the body of representations or headers, whenever appropriate. CONSIDER using /api as the first path segment, when the API also supports non-public APIs, e.g., for specific operational support functions. Otherwise, consider forgoing the /api prefix. CONSIDER using comma ( , ) and semi-colon ( ; ) to indicate nonhierarchical elements in the path segment. The semi-colon ( ; ) convention is used to identify matrix parameters. Not all libraries recognize these as separators and may require custom coding to extract these parameters . CONSIDER using URI templates, if it is impractical to supply all the possible URIs in the representation (e.g., ad hoc searching). CONSIDER disregarding opacity to protect against tampering using digitally signed URIs, or to protect sensitive information by encrypting parts of the URI. AVOID including file extensions, instead rely on the media types. AVOID using trailing forward slash, as some frameworks may incorrectly remove or add such slashes during URI normalization. AVOID expecting clients to construct URIs. AVOID unnecessary query strings in URIs. AVOID leaking implementation details to clients, by keeping the creating of URIs on the server, as those details will become part of the public interface. DO NOT use an URI as a generic gateway, by tunneling repeated state changes over POST using the same URI. DO NOT use custom headers to overload URIs.","title":"Designing URIs"},{"location":"resource-identifiers/#uris-for-queries","text":"Queries usually involve filtering, sorting and projections. When providing query support for these and other actions: DO use snake_case ( never camelCase ) for Query Parameters. DO use query parameters to let clients specify filter conditions, sort fields, and projections. DO treat query parameters as optional with sensible defaults. DO document each parameter. CONSIDER using q (e.g. used by browser tab completion) as the default query parameter. CONSIDER using a format query parameter, if standard content negotiation is not possible. AVOID ad hoc queries that use general-purpose query languages such as SQL or XPath . AVOID Range requests for implementing queries.","title":"URIs for Queries"},{"location":"resource-identifiers/#partial-responses","text":"CONSIDER using a fields query parameter for projections (partial responses), like https://www.example.org/customers?fields=name,gender,birthday or https://example.org/customer?fields=(firstName,user(email)) and ! to negate field selection. Depending on the use case and payload size, it can reduce network bandwidth and reduce filtering on clients. CONSIDER using a view query parameter for predefined projections, like https://www.example.org/customers?view=summary CONSIDER supporting aliases for commonly used queries (it may also improve cacheability). For instance, GET /tickets/recently_closed CONSIDER using embed to allow for resource expansion. Embedding related resources can help reduce the number of requests.","title":"Partial Responses"},{"location":"resource-identifiers/#sorting","text":"CONSIDER using a generic sort parameter to describe sorting rules. To accommodate more complex sorting requirements, let the sort parameter take a list of comma-separated fields, each with a possible unary negative to imply descending sort order. Like: GET /tickets?sort=-priority,created_at","title":"Sorting"},{"location":"resource-identifiers/#pagination","text":"CONSIDER using limit and offset , like https://www.example.org/authors?offset=50&limit=25 , when resource collections are backed by traditional relation databases, as these ties into the general implementation on the data store. Opt for cursor and a cursor-based pagination strategy, when the dataset is very large and/or backed by a non-traditional relational data store, like a document database. For more information see Pagination . CONSIDER using sensible defaults for pagination, e.g. limit=10 and offset=0. The pagination defaults are dependent on your data size. Opaque URIs != Unreadable URIs \u21a9 a design property that communicates how something should be used without requiring documentation \u21a9","title":"Pagination"},{"location":"resources/","text":"Resources \u00b6 A resource is an abstract entity that is identified by a URI, and can be data, processing logic, files, or basically anything else that can be named. Identifying and Naming Resources \u00b6 Most often when identifying resources, the use of nouns instead of verbs will be obvious, however, sometimes it is necessary to veer from this naming convention, when exposing certain services through the public API. There is no single approach that will work for all the situations, so care should be taking when identifying resources and finding the right resource granularity, in order for API consumers to get the desired functionality, the API to behave correctly, and for maintainability. Note Taking business needs, maintainability, extensibility, and perhaps some sort of cost-benefit analysis or ROI, into consideration may be pertinent. In order to identify and name resources: DO analyze use cases to find domain nouns that can be operated using create , read , update , or delete operations. Designate each noun as a resource. Use POST , GET , PUT , or DELETE operations respectively, on each resource. DO design resources to suit client's usage patterns and not design them based on what exists in a database or the object model. Think from the client's perspective . DO use network efficiency, size of representations, and client convenience to guide resource granularity. DO think about frequency of change and cacheability. Try to separate immutable (less frequently changing) resources from less cacheable resources. CONSIDER the level of abstraction, as it is not always meaningful for developers, if the information conveyed in URIs are too abstract. CONSIDER limiting the number of resources to between 12 4 and 24 8 to keep maintenance and service evolution manageable. Avoid mixing different business functionalities in same API. AVOID using verbs in your URIs, instead use HTTP verbs to operate on them. AVOID blindly applying the same techniques for identifying resources, as for object-oriented design and database modeling. AVOID bluntly mapping domain entities into resources, as this may lead to resources that are inefficient and inconvenient to use and also leak irrelevant implementation details out to your public API. DO NOT limit yourself to identifying resources based on domain nouns alone, you are likely to find that the fixed set of methods in HTTP is quite a limitation. E.g., using a root-level \"Me\" endpoint is perfectly valid. Collection Resources \u00b6 Resources almost always have relationships to other resources, and collection resources can give the ability to refer to a group of a resources as one, to perform queries on the collection, or use the collection as a factory to create new resources. Tip A collection resource does not necessarily imply hierarchical containment. A resource may be part of more than one collection resource. A collection resource should provide sufficient filter mechanisms, as well as pagination. So when organizing resources into collections: DO pluralizing collection resource names. To keep things simple, pragmatic and consistent always use plural to avoid odd pluralization (person/people). Using a mixed model, in which you use singular for some resources and plural for others will make it harder for developers to reason about the API. DO provide a way of searching the collection for it members, if applicable. DO provide a filtered view of the collection, if applicable. DO provide a paginated view of a collection, when appropriate. DO embed commonly requested relations alongside the resource, for client convenience. DO organize resources according to relationship using path segments ( / ), if a relation can only exist within another resource. Sub-resources should be referenced by their name and identifier in the path segments: /{resource}/{resource_id}/{sub_resource}/{sub_resource_id} . CONSIDER providing a link, if a relation can exist independently of the resource. However, if the relation is commonly requested it might be better to always embed the relation's representation, or perhaps offer a functionality to automatically embed the relation's representation and save the client a second round-trip ( GET /bookings?embed=departures,vehicles ). AVOID sub-sub-resource levels, etc. Try to simplify the association between resources. Composite Resources \u00b6 At times it may prove convenient to identify new composite resources. DO identify new resources that aggregate other resources to reduce the number of client/server round-trips, based on client usage patterns, performance, and latency requirements. CONSIDER a snapshot page to summarize information, using an URI of the form https://www.example.org/customer/1234/snapshot . CONSIDER , if requests for composites are rare, or if network latency is an issue, whether caching may be a better option. Processing Functions \u00b6 Having the public API expose means to support business processes, like, calculate, translate, and convert, is not uncommon depending on the domain. These may often not map well to nouns in our domain, but usually are in the form of verbs. By using processing functions to abstract specific services the clients is able to perform tasks such computation or data validation. DO treat the processing function as a resource, as most often it will be difficult to find appropriate nouns. So when validating a vehicle registration number: https://www.example.org/vehicles/validate?regnum=ZY12345 DO use GET to fetch a representation containing the output of the processing function. DO use query parameters to supply inputs to the processing function. DO document these resources, and make use of verbs clear. Controller Resources \u00b6 A controller is a resource that can atomically make changes to resources. It can help abstract complex business operations, which increase the separation of concerns and reduces coupling between clients and servers, which in turn may improve network efficiency. DO designate a controller resource for each distinct operation. DO use POST to submit a request to trigger the operation. DO , if the output of the operation is the creation of a new resource, return 201 Created with a Location header referring to the URI of the newly created resource. DO , if the outcome is the modification of one or more existing resource, return 303 See Other with a Location to a URI that clients can use to fetch a representation of those modifications. DO , if the server cannot provide a single URI to all the modified resources, return 200 OK with a representation in the body that clients can use to learn about the outcome. DO handle errors as described in Errors . AVOID tunneling at all costs. Instead, use a distinct resource (such as a controller) for each operation. Tunneling occurs whenever the client is using the same method on a single URI for different actions. Tunneling reduces protocol-level visibility, because the visible parts of requests such as the request URI, the HTTP method used, headers, and media types do not unambiguously describe the operation.","title":"Resources"},{"location":"resources/#resources","text":"A resource is an abstract entity that is identified by a URI, and can be data, processing logic, files, or basically anything else that can be named.","title":"Resources"},{"location":"resources/#identifying-and-naming-resources","text":"Most often when identifying resources, the use of nouns instead of verbs will be obvious, however, sometimes it is necessary to veer from this naming convention, when exposing certain services through the public API. There is no single approach that will work for all the situations, so care should be taking when identifying resources and finding the right resource granularity, in order for API consumers to get the desired functionality, the API to behave correctly, and for maintainability. Note Taking business needs, maintainability, extensibility, and perhaps some sort of cost-benefit analysis or ROI, into consideration may be pertinent. In order to identify and name resources: DO analyze use cases to find domain nouns that can be operated using create , read , update , or delete operations. Designate each noun as a resource. Use POST , GET , PUT , or DELETE operations respectively, on each resource. DO design resources to suit client's usage patterns and not design them based on what exists in a database or the object model. Think from the client's perspective . DO use network efficiency, size of representations, and client convenience to guide resource granularity. DO think about frequency of change and cacheability. Try to separate immutable (less frequently changing) resources from less cacheable resources. CONSIDER the level of abstraction, as it is not always meaningful for developers, if the information conveyed in URIs are too abstract. CONSIDER limiting the number of resources to between 12 4 and 24 8 to keep maintenance and service evolution manageable. Avoid mixing different business functionalities in same API. AVOID using verbs in your URIs, instead use HTTP verbs to operate on them. AVOID blindly applying the same techniques for identifying resources, as for object-oriented design and database modeling. AVOID bluntly mapping domain entities into resources, as this may lead to resources that are inefficient and inconvenient to use and also leak irrelevant implementation details out to your public API. DO NOT limit yourself to identifying resources based on domain nouns alone, you are likely to find that the fixed set of methods in HTTP is quite a limitation. E.g., using a root-level \"Me\" endpoint is perfectly valid.","title":"Identifying and Naming Resources"},{"location":"resources/#collection-resources","text":"Resources almost always have relationships to other resources, and collection resources can give the ability to refer to a group of a resources as one, to perform queries on the collection, or use the collection as a factory to create new resources. Tip A collection resource does not necessarily imply hierarchical containment. A resource may be part of more than one collection resource. A collection resource should provide sufficient filter mechanisms, as well as pagination. So when organizing resources into collections: DO pluralizing collection resource names. To keep things simple, pragmatic and consistent always use plural to avoid odd pluralization (person/people). Using a mixed model, in which you use singular for some resources and plural for others will make it harder for developers to reason about the API. DO provide a way of searching the collection for it members, if applicable. DO provide a filtered view of the collection, if applicable. DO provide a paginated view of a collection, when appropriate. DO embed commonly requested relations alongside the resource, for client convenience. DO organize resources according to relationship using path segments ( / ), if a relation can only exist within another resource. Sub-resources should be referenced by their name and identifier in the path segments: /{resource}/{resource_id}/{sub_resource}/{sub_resource_id} . CONSIDER providing a link, if a relation can exist independently of the resource. However, if the relation is commonly requested it might be better to always embed the relation's representation, or perhaps offer a functionality to automatically embed the relation's representation and save the client a second round-trip ( GET /bookings?embed=departures,vehicles ). AVOID sub-sub-resource levels, etc. Try to simplify the association between resources.","title":"Collection Resources"},{"location":"resources/#composite-resources","text":"At times it may prove convenient to identify new composite resources. DO identify new resources that aggregate other resources to reduce the number of client/server round-trips, based on client usage patterns, performance, and latency requirements. CONSIDER a snapshot page to summarize information, using an URI of the form https://www.example.org/customer/1234/snapshot . CONSIDER , if requests for composites are rare, or if network latency is an issue, whether caching may be a better option.","title":"Composite Resources"},{"location":"resources/#processing-functions","text":"Having the public API expose means to support business processes, like, calculate, translate, and convert, is not uncommon depending on the domain. These may often not map well to nouns in our domain, but usually are in the form of verbs. By using processing functions to abstract specific services the clients is able to perform tasks such computation or data validation. DO treat the processing function as a resource, as most often it will be difficult to find appropriate nouns. So when validating a vehicle registration number: https://www.example.org/vehicles/validate?regnum=ZY12345 DO use GET to fetch a representation containing the output of the processing function. DO use query parameters to supply inputs to the processing function. DO document these resources, and make use of verbs clear.","title":"Processing Functions"},{"location":"resources/#controller-resources","text":"A controller is a resource that can atomically make changes to resources. It can help abstract complex business operations, which increase the separation of concerns and reduces coupling between clients and servers, which in turn may improve network efficiency. DO designate a controller resource for each distinct operation. DO use POST to submit a request to trigger the operation. DO , if the output of the operation is the creation of a new resource, return 201 Created with a Location header referring to the URI of the newly created resource. DO , if the outcome is the modification of one or more existing resource, return 303 See Other with a Location to a URI that clients can use to fetch a representation of those modifications. DO , if the server cannot provide a single URI to all the modified resources, return 200 OK with a representation in the body that clients can use to learn about the outcome. DO handle errors as described in Errors . AVOID tunneling at all costs. Instead, use a distinct resource (such as a controller) for each operation. Tunneling occurs whenever the client is using the same method on a single URI for different actions. Tunneling reduces protocol-level visibility, because the visible parts of requests such as the request URI, the HTTP method used, headers, and media types do not unambiguously describe the operation.","title":"Controller Resources"},{"location":"rest/","text":"REpresentational State Transfer (REST) \u00b6 We will focus our discussion on REST. Especially on how to achieve a more RESTful web API, by leveraging the capabilities of HTTP. For more information about REST itself, please refer to the Wikipedia article on REST or What is REST? . Some of the aspects of designing RESTful web APIs include: Unique identification of resources by using URIs. Operation on resources by using the available HTTP methods . A choice of formats and media types that allow clients to specify representation formats they can render, and for servers to honor those (or indicate if it cannot). Linking between resources to indicate relationships (e.g., hypermedia links). Why REST? REST imposes architectural constraints on your API design, but instead of regarding them as limitations, see them as support for building great APIs, i.e. APIs that are simple, clear, clean and approachable. Richardson Maturity Model \u00b6 Today, most web APIs are not truly RESTful APIs, instead they tend to be of a varying degree of RESTful. Therefore, the Richardson Maturity Model is often used to describe what it takes to make a well-designed RESTful API: Level 0 : Uses HTTP as a transport mechanism for RPC. Level 1 : Uses URIs for individual resources, but does not use HTTP methods, media types, or linking between resources. Level 2 : Uses HTTP methods and headers for interactions with resources, and returns appropriate HTTP status codes. Level 3 : Uses hypermedia controls for discoverability, which also helps making an API more self-documenting. So a good RESTful API should Use unique URIs to expose resources for clients to use. Use the full spectrum of HTTP, to perform operations on those resources, and allow different representations of content. Provide relational links for resources, to inform the client what can be done next. What's in the Box? \u00b6 In these guidelines we will offer advise on how to achieve this, when implementing clients as well as servers. Specifically we will focus on: How to identify resources by using URIs How to use the available HTTP methods What representation formats to you expose and what to do when the server cannot fulfill a request for a given representation How to report errors, using HTTP status codes and representations How to handle security, including HTTP authentication, OAuth2, and API tokens How to handle hypermedia linking How to document your API Anything else? Before we get Started \u00b6 Because REST is an architectural style and not a strict standard, it allows for a lot of flexibly. So before jumping straight into designing an API, it can be helpful to think about: the target audience, i.e. customers, third-party services, or other developers looking to take advantage of your services for their own customers. ease of life for the consuming developers use cases technologies used products/services to expose other services to interact with As long as the API only has one consumer, one could argue that it's entire job is to service that consumer. Make it RESTful, but be pragmatic! Aiming at a level above 2, according to Richardson's Maturity Model, may simply be impractical.","title":"REST"},{"location":"rest/#representational-state-transfer-rest","text":"We will focus our discussion on REST. Especially on how to achieve a more RESTful web API, by leveraging the capabilities of HTTP. For more information about REST itself, please refer to the Wikipedia article on REST or What is REST? . Some of the aspects of designing RESTful web APIs include: Unique identification of resources by using URIs. Operation on resources by using the available HTTP methods . A choice of formats and media types that allow clients to specify representation formats they can render, and for servers to honor those (or indicate if it cannot). Linking between resources to indicate relationships (e.g., hypermedia links). Why REST? REST imposes architectural constraints on your API design, but instead of regarding them as limitations, see them as support for building great APIs, i.e. APIs that are simple, clear, clean and approachable.","title":"REpresentational State Transfer (REST)"},{"location":"rest/#richardson-maturity-model","text":"Today, most web APIs are not truly RESTful APIs, instead they tend to be of a varying degree of RESTful. Therefore, the Richardson Maturity Model is often used to describe what it takes to make a well-designed RESTful API: Level 0 : Uses HTTP as a transport mechanism for RPC. Level 1 : Uses URIs for individual resources, but does not use HTTP methods, media types, or linking between resources. Level 2 : Uses HTTP methods and headers for interactions with resources, and returns appropriate HTTP status codes. Level 3 : Uses hypermedia controls for discoverability, which also helps making an API more self-documenting. So a good RESTful API should Use unique URIs to expose resources for clients to use. Use the full spectrum of HTTP, to perform operations on those resources, and allow different representations of content. Provide relational links for resources, to inform the client what can be done next.","title":"Richardson Maturity Model"},{"location":"rest/#whats-in-the-box","text":"In these guidelines we will offer advise on how to achieve this, when implementing clients as well as servers. Specifically we will focus on: How to identify resources by using URIs How to use the available HTTP methods What representation formats to you expose and what to do when the server cannot fulfill a request for a given representation How to report errors, using HTTP status codes and representations How to handle security, including HTTP authentication, OAuth2, and API tokens How to handle hypermedia linking How to document your API Anything else?","title":"What's in the Box?"},{"location":"rest/#before-we-get-started","text":"Because REST is an architectural style and not a strict standard, it allows for a lot of flexibly. So before jumping straight into designing an API, it can be helpful to think about: the target audience, i.e. customers, third-party services, or other developers looking to take advantage of your services for their own customers. ease of life for the consuming developers use cases technologies used products/services to expose other services to interact with As long as the API only has one consumer, one could argue that it's entire job is to service that consumer. Make it RESTful, but be pragmatic! Aiming at a level above 2, according to Richardson's Maturity Model, may simply be impractical.","title":"Before we get Started"},{"location":"security/","text":"Security \u00b6 MUST: Secure Endpoints with OAuth 2.0 \u00b6 Out-of-band authentication CORS Use Access and Refresh Tokens Access Tokens Refresh Tokens Logging In Renewing a Token Validating a Token Terminating a Session Keep JWTs Small Always use SSL. No exceptions. One thing to watch out for is non-SSL access to API URLs. Do not redirect these to their SSL counterparts. Throw a hard error instead! A RESTful API should be stateless. This means that request authentication should not depend on cookies or sessions. Instead, each request should come with some sort authentication credentials. An API Manager should also be designed to handle security, not just by validating a boarding pass (API key) and directing them to their appropriate gate (permissions, SLA tier), but your API Manager should watch out for other dangerous threats, such as malicious IPs, DDoS, content threats (such as with JSON or XML), and others. It should also be designed to work with your current user validation systems such as OAuth2 to help protect your user's sensitive data (such as their username and password) within your API. Keep in mind that even the most righteous of applications are still prone to being hacked \u2013 and if they have access to your user's sensitive data, that means that hackers might get access to it too. It's always better to never have your users expose their credentials through the API (such as with basic auth) but instead rely on your website to handle logins and then return back an access token as OAuth2 does. But... Security is Hard \u00b6 One of my favorite talks, by Anthony Ferrara sums this up very nicely... Don't do it. Leave it for the Experts. Granted, his talk was specifically on encryption, but there's a lot of careful thought, planning, and considerations that need to be taken when building an API Management tool. You can certainly build it yourself, and handling API keys or doing IP white-listing/ black-listing is fairly easy to do. However, like an airport, what you don't see is all of the stuff happening in the background, all the things being monitored carefully, security measures implemented by experts with years of experience in risk mitigation. For that reason, as tempting as it might be to build one yourself, I would strongly encourage you to take a look at a professional API Management company- such as MuleSoft (of course, I might be a little biased). Authentication \u00b6 There are many schools of thought. My colleagues at Apigee and I don't always agree on how to handle authentication - but overall here's my take. Let's look at these three top services. See how each of these services handles things differently: PayPal Permissions Service API Facebook OAuth 2.0 Twitter OAuth 1.0a Note that PayPal's proprietary three-legged permissions API was in place long before OAuth was conceived. What should you do? Use the latest and greatest OAuth - OAuth 2.0 (as of this writing). It means that Web or mobile apps that expose APIs don't have to share passwords. It allows the API provider to revoke tokens for an individual user, for an entire app, without requiring the user to change their original password. This is critical if a mobile device is compromised or if a rogue app is discovered. Above all, OAuth 2.0 will mean improved security and better end-user and consumer experiences with Web and mobile apps. Don't do something like OAuth, but different. It will be frustrating for app developers if they can't use an OAuth library in their language because of your variation. Security \u00b6 Securing may require: ensure that only authenticated users access resources ensure the condidentiality and integrity of information right from the moment it is collected until the time it is stored and later presented to authorized entities or users. prevent unauthorized or malicious clients from abusing resources and data. maintain privacy, and follow the laws of the land that govern various security aspects. How to use Basic Authentication to Authenticate Clients return 401 Authorization Required along with a WWW-Authenticate: Basic realm=\"Some name\" . On the client, concatenate the client identifier (a username if the client is making a request on behalf of a user) and the shared secret (password) as <identifier>:<secret> and then compute the Base64 encoding on this text. Include the value in the Authorization: Basic <Base64 encoded value . On the server decode the text and verify the values. Do not use basic authentication unless using TLS to connect to the server. How to use Three-Legged OAuth How to use Two-Legged OAuth How to Deal with Sensitive Information in URIs How to Maintain the Confidentiality and Integrity of Representations use TLS and make resource accessible over a server configured to serve request only using HTTPS. Authentication is Key \u00b6 By providing your API users with a unique API token, or API key you can tell exactly who is making calls to your API. Along with having quick access to determining potential malicious users which can immediately be removed, you can also set permissions and SLAs for users depending on their individual needs. This means that you can set a default SLA for most users, giving them say only 4 calls per second, but for silver partners they get 10 calls per second, for gold partners 100 calls per second, etc. This means that you can not only identify abuse quickly, but also help prevent it by limiting their access to certain aspects of your API, and by limiting the number of calls they can make to your API.","title":"Security"},{"location":"security/#security","text":"","title":"Security"},{"location":"security/#must-secure-endpoints-with-oauth-20","text":"Out-of-band authentication CORS Use Access and Refresh Tokens Access Tokens Refresh Tokens Logging In Renewing a Token Validating a Token Terminating a Session Keep JWTs Small Always use SSL. No exceptions. One thing to watch out for is non-SSL access to API URLs. Do not redirect these to their SSL counterparts. Throw a hard error instead! A RESTful API should be stateless. This means that request authentication should not depend on cookies or sessions. Instead, each request should come with some sort authentication credentials. An API Manager should also be designed to handle security, not just by validating a boarding pass (API key) and directing them to their appropriate gate (permissions, SLA tier), but your API Manager should watch out for other dangerous threats, such as malicious IPs, DDoS, content threats (such as with JSON or XML), and others. It should also be designed to work with your current user validation systems such as OAuth2 to help protect your user's sensitive data (such as their username and password) within your API. Keep in mind that even the most righteous of applications are still prone to being hacked \u2013 and if they have access to your user's sensitive data, that means that hackers might get access to it too. It's always better to never have your users expose their credentials through the API (such as with basic auth) but instead rely on your website to handle logins and then return back an access token as OAuth2 does.","title":"MUST: Secure Endpoints with OAuth 2.0"},{"location":"security/#but-security-is-hard","text":"One of my favorite talks, by Anthony Ferrara sums this up very nicely... Don't do it. Leave it for the Experts. Granted, his talk was specifically on encryption, but there's a lot of careful thought, planning, and considerations that need to be taken when building an API Management tool. You can certainly build it yourself, and handling API keys or doing IP white-listing/ black-listing is fairly easy to do. However, like an airport, what you don't see is all of the stuff happening in the background, all the things being monitored carefully, security measures implemented by experts with years of experience in risk mitigation. For that reason, as tempting as it might be to build one yourself, I would strongly encourage you to take a look at a professional API Management company- such as MuleSoft (of course, I might be a little biased).","title":"But... Security is Hard"},{"location":"security/#authentication","text":"There are many schools of thought. My colleagues at Apigee and I don't always agree on how to handle authentication - but overall here's my take. Let's look at these three top services. See how each of these services handles things differently: PayPal Permissions Service API Facebook OAuth 2.0 Twitter OAuth 1.0a Note that PayPal's proprietary three-legged permissions API was in place long before OAuth was conceived. What should you do? Use the latest and greatest OAuth - OAuth 2.0 (as of this writing). It means that Web or mobile apps that expose APIs don't have to share passwords. It allows the API provider to revoke tokens for an individual user, for an entire app, without requiring the user to change their original password. This is critical if a mobile device is compromised or if a rogue app is discovered. Above all, OAuth 2.0 will mean improved security and better end-user and consumer experiences with Web and mobile apps. Don't do something like OAuth, but different. It will be frustrating for app developers if they can't use an OAuth library in their language because of your variation.","title":"Authentication"},{"location":"security/#security_1","text":"Securing may require: ensure that only authenticated users access resources ensure the condidentiality and integrity of information right from the moment it is collected until the time it is stored and later presented to authorized entities or users. prevent unauthorized or malicious clients from abusing resources and data. maintain privacy, and follow the laws of the land that govern various security aspects. How to use Basic Authentication to Authenticate Clients return 401 Authorization Required along with a WWW-Authenticate: Basic realm=\"Some name\" . On the client, concatenate the client identifier (a username if the client is making a request on behalf of a user) and the shared secret (password) as <identifier>:<secret> and then compute the Base64 encoding on this text. Include the value in the Authorization: Basic <Base64 encoded value . On the server decode the text and verify the values. Do not use basic authentication unless using TLS to connect to the server. How to use Three-Legged OAuth How to use Two-Legged OAuth How to Deal with Sensitive Information in URIs How to Maintain the Confidentiality and Integrity of Representations use TLS and make resource accessible over a server configured to serve request only using HTTPS.","title":"Security"},{"location":"security/#authentication-is-key","text":"By providing your API users with a unique API token, or API key you can tell exactly who is making calls to your API. Along with having quick access to determining potential malicious users which can immediately be removed, you can also set permissions and SLAs for users depending on their individual needs. This means that you can set a default SLA for most users, giving them say only 4 calls per second, but for silver partners they get 10 calls per second, for gold partners 100 calls per second, etc. This means that you can not only identify abuse quickly, but also help prevent it by limiting their access to certain aspects of your API, and by limiting the number of calls they can make to your API.","title":"Authentication is Key"},{"location":"throttling/","text":"Throttling \u00b6 Throttling and rate limiting is not necessarily a bad thing. By throttling your API and setting up different SLA tiers you are able to help prevent abuse \u2013 often times completely accidental. This means that your API is able to operate optimally for all of your users, instead of having one infinite loop bring it crashing down for everyone. You may have partners that need more calls than others, or who the limits do not make sense for. By setting up SLA tiers based on your standard API user's needs, and then creating partner tiers, you can easily give the the permissions they need, while limiting the standard user to prevent abuse. The API key, or unique identifier for an user's application also helps you identify who your heavier users are \u2013 letting you get in contact with them to make sure their needs are being met, while also learning more about how they are using your API. Rate limiting (Twitter-style) X-Rate-Limit-Limit - The number of allowed requests in the current period X-Rate-Limit-Remaining - The number of remaining requests in the current period X-Rate-Limit-Reset - The number of seconds left in the current period MUST: Use 429 with Headers for Rate Limits \u00b6 APIs that wish to manage the request rate of clients must use the '429 Too Many Requests' response code if the client exceeded the request rate and therefore the request can't be fulfilled. Such responses must also contain header information providing further details to the client. There are two approaches a service can take for header information: Return a 'Retry-After' header indicating how long the client ought to wait before making a follow-up request. The Retry-After header can contain a HTTP date value to retry after or the number of seconds to delay. Either is acceptable but APIs should prefer to use a delay in seconds. Return a trio of 'X-RateLimit' headers. These headers (described below) allow a server to express a service level in the form of a number of allowing requests within a given window of time and when the window is reset. The 'X-RateLimit' headers are: X-RateLimit-Limit: The maximum number of requests that the client is allowed to make in this window. X-RateLimit-Remaining: The number of requests allowed in the current window. X-RateLimit-Reset: The relative time in seconds when the rate limit window will be reset. The reason to allow both approaches is that APIs can have different needs. Retry-After is often sufficient for general load handling and request throttling scenarios and notably, does not strictly require the concept of a calling entity such as a tenant or named account. In turn this allows resource owners to minimise the amount of state they have to carry with respect to client requests. The 'X-RateLimit' headers are suitable for scenarios where clients are associated with pre-existing account or tenancy structures. 'X-RateLimit' headers are generally returned on every request and not just on a 429, which implies the service implementing the API is carrying sufficient state to track the number of requests made within a given window for each named entity.","title":"Throttling"},{"location":"throttling/#throttling","text":"Throttling and rate limiting is not necessarily a bad thing. By throttling your API and setting up different SLA tiers you are able to help prevent abuse \u2013 often times completely accidental. This means that your API is able to operate optimally for all of your users, instead of having one infinite loop bring it crashing down for everyone. You may have partners that need more calls than others, or who the limits do not make sense for. By setting up SLA tiers based on your standard API user's needs, and then creating partner tiers, you can easily give the the permissions they need, while limiting the standard user to prevent abuse. The API key, or unique identifier for an user's application also helps you identify who your heavier users are \u2013 letting you get in contact with them to make sure their needs are being met, while also learning more about how they are using your API. Rate limiting (Twitter-style) X-Rate-Limit-Limit - The number of allowed requests in the current period X-Rate-Limit-Remaining - The number of remaining requests in the current period X-Rate-Limit-Reset - The number of seconds left in the current period","title":"Throttling"},{"location":"throttling/#must-use-429-with-headers-for-rate-limits","text":"APIs that wish to manage the request rate of clients must use the '429 Too Many Requests' response code if the client exceeded the request rate and therefore the request can't be fulfilled. Such responses must also contain header information providing further details to the client. There are two approaches a service can take for header information: Return a 'Retry-After' header indicating how long the client ought to wait before making a follow-up request. The Retry-After header can contain a HTTP date value to retry after or the number of seconds to delay. Either is acceptable but APIs should prefer to use a delay in seconds. Return a trio of 'X-RateLimit' headers. These headers (described below) allow a server to express a service level in the form of a number of allowing requests within a given window of time and when the window is reset. The 'X-RateLimit' headers are: X-RateLimit-Limit: The maximum number of requests that the client is allowed to make in this window. X-RateLimit-Remaining: The number of requests allowed in the current window. X-RateLimit-Reset: The relative time in seconds when the rate limit window will be reset. The reason to allow both approaches is that APIs can have different needs. Retry-After is often sufficient for general load handling and request throttling scenarios and notably, does not strictly require the concept of a calling entity such as a tenant or named account. In turn this allows resource owners to minimise the amount of state they have to carry with respect to client requests. The 'X-RateLimit' headers are suitable for scenarios where clients are associated with pre-existing account or tenancy structures. 'X-RateLimit' headers are generally returned on every request and not just on a 429, which implies the service implementing the API is carrying sufficient state to track the number of requests made within a given window for each named entity.","title":"MUST: Use 429 with Headers for Rate Limits"},{"location":"versioning-and-extensibility/","text":"Versioning and Extensibility \u00b6 versioning Use API Versioning what is a breaking change breaking changes http://www.jenitennison.com/2009/07/22/versioning-uris.html A lot of things that we want to talk about (make RDF assertions about) are non-information resources. We give them URIs to name them, so that we can talk about them unambiguously, and we give them HTTP URIs so that we have a way of finding information resources (documents) that give us information about them. non-information resource URIs must not include information that is likely to change non-information resource URIs must not include unnecessary hierarchy Always version your API. Versioning helps you iterate faster and prevents invalid requests from hitting updated endpoints. It also helps smooth over any major API version transitions as you can continue to offer old API versions for a period of time. However, the version needs to be in the URL to ensure browser explorability of the resources across versions Well documented and announced multi-month deprecation schedules can be an acceptable practice for many APIs. https://stripe.com/docs/api#versioning Like building an application it is important to keep your API standardized and extensible. It's easy to want to add \"quick fixes\" to make customers happy, but everything you do should be carefully thought out in order to make sure your API continues to serve not only your needs, but also your clients. Remember, when you create an API you are creating a contract- your users are depending on your API not just to make their application work, but in order to make a living and feed their families. When you break things, or when you break backwards compatibility you are taking their time and resources to fix their application instead of adding features and keeping their customers happy. The industry solution to this has been versioning, however, versioning is merely a Band-Aid, a temporary solution to make migration to the new system less stressful for your clients, but increasingly difficult on you. Keep in mind, when you have multiple versions of your API you end up supporting and maintaining those versions. One of the greatest challenges in regards to versioning is getting developers to migrate from one version to another, all while keeping your support staff from going insane. This doesn't mean that you shouldn't plan for versioning. Rather it means that you should plan to incorporate a version identifier (either in the URI or in the content-header of the response), but work under the mindset that you will only version your API if... You have backwards incompatible PLATFORM changes \u2013 in other words you completely change the UI or way your platform works You find that your API is no longer extendable \u2013 which is exactly what we are trying to avoid here You find that your spec no longer meets your developer's needs \u2013 for example, they are demanding REST instead of SOAP You should NOT version your API just because: You added additional endpoints You added additional data in the response You changed technologies \u2013 your API should be decoupled from your technology stack You changed your applications services or code \u2013 your API should be decoupled from your service layer To clarify on the last two, it shouldn't matter what technologies you are using, or how your services work. The API should interact with both, but be decoupled enough that changing something in the background does not effect the API adversely. Any changes you make to your API in regards to the technology or service layer should be as seamless and transparent as possible. After all, the less you can break backwards compatibility, the happier you, and your customers will be. And this can only happen if you go into building your API with a long-term, flexibility, and extensibility focus. Extensibility and Versioning \u00b6 Managing change in any distributed client/server environment can be hard. In these environments, clients count on servers to honor their contracts. When a change is backward compatible, you need not upgrade clients at the same time as you modify the server. Forward compatibility may be important when you have several clients and servers upgraded at different points in time. In this case, some newer clients may be interacting with older servers. The purpose of forward compatibility is to ensure that newer clients can continue to use the older servers without disruption albeit with reduced functionality. The characteristic that lets you maintain compatibility is extensibility . Extensibility is a design process to account for future changes. As a transfer protocol, HTTP is extensible, but that does not mean that APIs built over HTTP are automatically extensible. It takes discipline, careful planning, and defensive coding practices. A one-time simultaneous upgrade of all server and clients is not a realistic task. You need to plan for a gradual rollout of upgrades to servers and clients to maintain the availability of the overall system. Note that both clients and server need to take the appropriate steps to operate smoothly under change. For the server, the goal is to keep the clients from breaking. For the clients, the objective is not fail when new unknown data or links appear in representations. How to Maintain URI Compatibility ( url regression ) \u00b6 Keeps URIs permanent. Treat URIs containing the same query parameters but in a different order as the same When you add new parameters to URIs, continue to honor existing parameters, and treat new parameters as optional. When changing data formats for query parameters, continue to honor existing formats. If that is not viable, introduce format changes via new query parameters or new URIs. Treat query parameters in URIs as optional except when need for concurrency and security. DO use rewrite rules on the server to shield clients from implementation-level changes. DO use 301 Moved Permanently with the new URI in the Location header, when URIs must change to honor old URIs. DO monitor request traffic for redirection. DO maintain redirection services until you are confident the majority of clients have updated their stored links to point to the new URI. DO communicate an appropriate end-of-life policy for old URIs, when you cannot monitor the old URIs. DO convert 301 Moved Permanently to 410 Gone or 404 Not Found once the traffic has fallen of or the preset time interval has passed. How to Maintain Compatibility of XML and JSON Representations \u00b6 When making changes to JSON preserve the hierarchical structure so that clients can continue to follow the same structure to extract data. Make new data elements in requests optional to maintain compatibility with existing clients. Clients that do not send new data fields must be able to continue to function. Do not remove or rename any data fields from representaions in reponse bodies. Example: Clients that do not understand new fields may not store them locally. If this causes the server to assume the user has no email address, introduce a new version of the resource that contains the email. How to Maintain Compatibility of Links \u00b6 Avoid removing links Do not change the value of the rel and href attributes of links. When introducing new resources, use links to provide URIs of those resources to clients. When to Version \u00b6 Consider versioning when the server cannot maintain compatibility. Also consider versioning if some clients require behavior or functionality different from other clients. Versioning may introduce new problems: Data stored by the clients for one version may not automatically work with the data from a different version. Clients may have to port resource data stored locally before migrating to the new version. Version changes may involve new business rules and new application flow, which requires code changes in clients. Maintaining multiple versions of resources at the same time is not trivial. When you use links to convey URIs to clients, clients may store them locally. When you assign new URIs, clients will have to upgrade those URIs along with other stored data of resources. How to Version RESTful Web Services \u00b6 Add new resources with new URIs when there is a change in the behavior of resource or a change in the information contained in representations. Use easily detectable patterns such as v1 or v2 in subdomain names, path segments, or query parameters to distinguish URIs by their version. Avoid treating each version as a new representation with a new media type of the same resource. Versioning involves versioning resources with new URIs. This is because HTTP dictates everything except URIs of resources and their representations. Although you can add custom HTTP methods and headers, such additions may impair interoperability. Avoid introducing new media types for each version since it leads to media type proliferation, which reduce interoperability. Compatibility \u00b6 MUST: Don't Break Backward Compatibility \u00b6 Change APIs, but keep all consumers running. Consumers usually have independent release life-cycles, focus on stability, and avoid changes that do not provide additional value. APIs are contracts between service providers and service consumers that cannot be broken via unilateral decisions. There are two techniques to change APIs without breaking them: follow rules for compatible extensions introduce new API versions and still support older versions We strongly encourage using compatible API extensions and discourage versioning. The below guideline rules for service providers and consumers enable us (having Postel's Law in mind) to make compatible changes without versioning. Hint: Please note that the compatibility guarantees are for the \"on the wire\" format. Binary or source compatibility of code generated from an API definition is not covered by these rules. If client implementations update their generation process to a new version of the API definition, it has to be expected that code changes are necessary. SHOULD:: Prefer Compatible Extensions \u00b6 API designers should apply the following rules to evolve RESTful APIs for services in a backward-compatible way: Add only optional, never mandatory fields. Never change the semantic of fields (e.g. changing the semantic from customer-number to customer-id, as both are different unique customer keys) Input fields may have (complex) constraints being validated via server-side business logic. - - Never change the validation logic to be more restrictive and make sure that all constraints a clearly defined in description. Enum ranges can be reduced when used as input parameters, only if the server is ready to accept and handle old range values too. Enum range can be reduced when used as output parameters. Enum ranges cannot not be extended when used for output parameters \u2014 clients may not be prepared to handle it. However, enum ranges can be extended when used for input parameters. Support redirection in case an URL has to change (301 Moved Permanently). MUST: Prepare Clients To Not Crash On Compatible API Extensions \u00b6 Service clients should apply the robustness principle: Be conservative with API requests and data passed as input, e.g. avoid to exploit definition eficits like passing megabytes for strings with unspecified maximum length. Be tolerant in processing and reading data of API responses, more specifically... Service clients must be prepared for compatible API extensions of service providers: Be tolerant with unknown fields in the payload (see also Fowler's \"TolerantReader\" post), i.e. ignore new fields but do not eliminate them from payload if needed for subsequent PUT requests. Be prepared to handle HTTP status codes not explicitly specified in endpoint definitions. Note also, that status codes are extensible. Default handling is how you would treat the corresponding x00 code (see RFC7231 Section 6). Follow the redirect when the server returns HTTP status 301 Moved Permanently. SHOULD:: Design APIs Conservatively \u00b6 Designers of service provider APIs should be conservative and accurate in what they accept from clients: Unknown input fields in payload or URL should not be ignored; servers should provide error feedback to clients via an HTTP 400 response code. Be accurate in defining input data constraints (like formats, ranges, lengths etc.) \u2014 and check constraints and return dedicated error information in case of violations. Prefer being more specific and restrictive (if compliant to functional requirements), e.g. by defining length range of strings. It may simplify implementation while providing freedom for further evolution as compatible extensions. Not ignoring unknown input fields is a specific deviation from Postel's Law (e.g. see also The Robustness Principle Reconsidered) and a strong recommendation. Servers might want to take different approach but should be aware of the following problems and be explicit in what is supported: Ignoring unknown input fields is actually not an option for PUT, since it becomes asymmetric with subsequent GET response and HTTP is clear about the PUT \"replace\" semantics and default round-trip expectations (see RFC7231 Section 4.3.4). Note, accepting (i.e. not ignoring) unknown input fields and returning it in subsequent GET responses is a different situation and compliant to PUT semantics. Certain client errors cannot be recognized by servers, e.g. attribute name typing errors will be ignored without server error feedback. The server cannot differentiate between the client intentionally providing an additional field versus the client sending a mistakenly named field, when the client's actual intent was to provide an optional input field. Future extensions of the input data structure might be in conflict with already ignored fields and, hence, will not be compatible, i.e. break clients that already use this field but with different type. In specific situations, where a (known) input field is not needed anymore, it either can stay in the API definition with \"not used anymore\" description or can be removed from the API definition as long as the server ignores this specific parameter. MUST: Always Return JSON Objects As Top-Level Data Structures To Support Extensibility \u00b6 In a response body, you must always return a JSON objects (and not e.g. an array) as a top level data structure to support future extensibility. JSON objects support compatible extension by additional attributes. This allows you to easily extend your response and e.g. add pagination later, without breaking backwards compatibility. SHOULD:: Avoid Versioning \u00b6 When changing your RESTful APIs, do so in a compatible way and avoid generating additional API versions. Multiple versions can significantly complicate understanding, testing, maintaining, evolving, operating and releasing our systems (supplementary reading). If changing an API can't be done in a compatible way, then proceed in one of these three ways: create a new resource (variant) in addition to the old resource variant create a new service endpoint \u2014 i.e. a new application with a new API (with a new domain name) create a new API version supported in parallel with the old API by the same microservice As we discourage versioning by all means because of the manifold disadvantages, we suggest to only use the first two approaches. MUST: Use Media Type Versioning \u00b6 When API versioning is unavoidable, you have to design your multi-version RESTful APIs using media type versioning (instead of URI versioning, see below). Media type versioning is less tightly coupled since it supports content negotiation and hence reduces complexity of release management. Media type versioning: Here, version information and media type are provided together via the HTTP Content-Type header \u2014 e.g. application/x.zalando.cart+json;version=2. For incompatible changes, a new media type version for the resource is created. To generate the new representation version, consumer and producer can do content negotiation using the HTTP Content-Type and Accept headers. Note: This versioning only applies to the request and response content schema, not to URI or method semantics. In this example, a client wants only the new version of the response: Accept: application/x.zalando.cart+json;version=2 A server responding to this, as well as a client sending a request with content should use the Content-Type header, declaring that one is sending the new version: Content-Type: application/x.zalando.cart+json;version=2 Using header versioning should: include versions in request and response headers to increase visibility include Content-Type in the Vary header to enable proxy caches to differ between versions Hint: OpenAPI currently doesn't support content negotiation, though a comment in this issue mentions a workaround (using a fragment identifier that gets stripped off). Another way would be to document just the new version, but let the server accept the old one (with the previous content-type). Until an incompatible change is necessary, it is recommended to stay with the standard application/json media type. MUST: Do Not Use URI Versioning \u00b6 With URI versioning a (major) version number is included in the path, e.g. /v1/customers. The consumer has to wait until the provider has been released and deployed. If the consumer also supports hypermedia links \u2014 even in their APIs \u2014 to drive workflows (HATEOAS), this quickly becomes complex. So does coordinating version upgrades \u2014 especially with hyperlinked service dependencies \u2014 when using URL versioning. To avoid this tighter coupling and complexer release management we do not use URI versioning, and go instead with media type versioning and content negotiation (see above). The API will inevitably need to be modified in ways that will impact customer client code. Not all customer clients will be able to make the necessary changes in their code right away, so the API should maintain older versions for some time after releasing the new version, usually a minimum of 90 days. In the \u2018When to Version\u2019 section should you mention APIs that use enum types? My company\u2019s API reference documentation has attributes with an enum type and we list all values. We ran into a problem when a requirement added an enum value to an existing attribute and it broke a partner app. The partner had coded to the defined enum values in the documentation and their app wasn\u2019t expecting new values with future releases. I corrected the problem by removing the new enum value and I defined any request to add a new enum value to an existing attribute requires a new API version. Last, going forward new attributes with an enum type will be defined as String in the documentation and we will indicate the values are not limited to the ones defined. Versioning Content Types The use of content negotiation with custom MIME types allows for finer grained versioning at the resource level without the need to create a plethora of new endpoints. New versions must be communicated to developers through existing channels \u2013 email, developer blogs, etc. When a content version is no longer supported, the body of the HTTP error should include a list of supported content types. Versioning URIs The use of both hyper links and content negotiation should all but eliminate the need to version at the URI level. However, there may be instances where the entire structure of the API must be changed, particularly when moving from one API style to another, such when moving from an RPC-type style to NARWHL. To prepare for these possibilities, it\u2019s recommended that a version be embedded within each API endpoint. The version can either be embedded at the root for all endpoints of a given API, such as: http://api.example.com/v1 Or within the fully qualified domain name for the endpoint: http://apiv1.example.com The version need not be numeric, nor specified using the \u201cv[x]\u201d syntax. Alternatives include dates, project names, seasons or other identifiers that are meaningful enough to the team producing the APIs and flexible enough to change as the versions change. Subbu Allamaraju, revisits one of the recurring debates in the REST community; the standard media types vs. custom media types and tries to determine the best practices when using them. He starts with the stating dichotomous views on the use of media types. Opinion 1: Web services must use standard media types to be RESTful. Opinion 2: Custom media types are necessary to keep interactions visible, and to serve as contracts. The first opinion which, if adhered to strictly, per Roy Fieldings thesis, \u201cthe use of media types such as application/vnd.example.myway+xml is not RESTful\u201d. Subbu believes that understanding the impact of such media type usage in the real world is more important than following the thesis to the letter. There are however comments that suggest that this interpretation of the thesis might be up for debate as well. To the contrary, the second opinion, he says, leads to visibility of the messages at the protocol level via the use of custom media types. [\u2026] For instance, how can anyone know if a representation that uses application/xml media type describes a purchase order, or a photo album? If the web service uses media types likeapplication/vnd.example.po and application/vnd.example.album, then any one can interpret the semantics of the representation without parsing the body of the representation. Per this line of thinking, a media type is an identifier for message semantics, and message recipients use media types to trigger processing code. \u201cSo what is the right thing to do?\u201d He asks, as he puts forth his idea, in a effort to democratically determine the best practices If the sender is formatting representations using standard extensible formats such as XML or JSON, use standard media types such as application/xml and application/json. Mint new media types when you invent new formats. If you are just looking for a way to communicate application level semantics for XML and JSON messages, use something else (e.g. XML namespaces and conventions). If the goal is versioning, use version identifiers in URIs. Giving java-like examples, He asserts that though its possible to peek into the messages to see how a request can be processed, it compromises visibility or opacity as the case may be. Media types such as application/xml and application/json are good enough for XML and JSON message processing in code. [\u2026] URI based approaches are guaranteed to work across the stack. Ignoring real-world interoperability for the sake of \"architectual purity\" or \"RESTful contracts\" may eventually back fire. Via the post is the solution presented by Subbu found the right balance between architectural purity and interoperable real-world solutions? Be sure to visit the original post to weigh in your opinion. Tips for versioning \u00b6 Versioning is one of the most important considerations when designing your Web API. Never release an API without a version and make the version mandatory. Let's see how three top API providers handle versioning. Twilio /2010-04-01/Accounts/ salesforce.com /services/data/v20.0/sobjects/Account Facebook ?v=1.0 Twilio uses a timestamp in the URL (note the European format). At compilation time, the developer includes the timestamp of the application when the code was compiled. That timestamp goes in all the HTTP requests. When a request arrives, Twilio does a look up. Based on the timestamp they identify the API that was valid when this code was created and route accordingly. It's a very clever and interesting approach, although we think it is a bit complex. For example, it can be confusing to understand whether the timestamp is the compilation time or the timestamp when the API was released. Salesforce.com uses v20.0, placed somewhere in the middle of the URL. We like the use of the v. notation. However, we don't like using the .0 because it implies that the interface might be changing more frequently than it should. The logic behind an interface can change rapidly but the interface itself shouldn't change frequently. Facebook also uses the v. notation but makes the version an optional parameter. This is problematic because as soon as Facebook forced the API up to the next version, all the apps that didn't include the version number broke and had to be pulled back and version number added. How to think about version numbers in a pragmatic way with REST? \u00b6 Never release an API without a version. Make the version mandatory. Specify the version with a 'v' prefix. Move it all the way to the left in the URL so that it has the highest scope (e.g. /v1/dogs). Use a simple ordinal number. Don't use the dot notation like v1.2 because it implies a granularity of versioning that doesn't work well with APIs--it's an interface not an implementation. Stick with v1, v2, and so on. How many versions should you maintain? Maintain at least one version back. For how long should you maintain a version? Give developers at least one cycle to react before obsoleting a version. Sometimes that's 6 months; sometimes it's 2 years. It depends on your developers' development platform, application type, and application users. For example, mobile apps take longer to rev' than Web apps. Should version and format be in URLs or headers? There is a strong school of thought about putting format and version in the header. Sometimes people are forced to put the version in the header because they have multiple inter-dependent APIs. That is often a symptom of a bigger problem, namely, they are usually exposing their internal mess instead of creating one, usable API facade on top. That's not to say that putting the version in the header is a symptom of a problematic API design. It's not! In fact, using headers is more correct for many reasons: it leverages existing HTTP standards, it's intellectually consistent with Fielding's vision, it solves some hard realworld problems related to inter-dependent APIs, and more. However, we think the reason most of the popular APIs do not use it is because it's less fun to hack in a browser. Simple rules we follow: If it changes the logic you write to handle the response, put it in the URL so you can see it easily. If it doesn't change the logic for each response, like OAuth information, put it in the header. The code we would write to handle the responses would be very different. There's no question the header is more correct and it is still a very strong API design. Deprecation \u00b6 Sometimes it is necessary to phase out an API endpoint (or version). I.e. this may be necessary if a field is no longer supported in the result or a whole business functionality behind an endpoint has to be shut down. There are many other reasons as well. MUST: Obtain Approval of Clients \u00b6 Before shutting down an API (or version of an API) the producer must make sure, that all clients have given their consent to shut down the endpoint. Producers should help consumers to migrate to a potential new endpoint (i.e. by providing a migration manual). After all clients are migrated, the producer may shut down the deprecated API. MUST: External Partners Must Agree on Deprecation Timespan \u00b6 If the API is consumed by any external partner, the producer must define a reasonable timespan that the API will be maintained after the producer has announced deprecation. The external partner (client) must agree to this minimum after-deprecation-lifespan before he starts using the API. MUST: Reflect Deprecation in API Definition \u00b6 API deprecation must be part of the OpenAPI definition. If a method on a path, a whole path or even a whole API endpoint (multiple paths) should be deprecated, the producers must set deprecated=true on each method / path element that will be deprecated (OpenAPI 2.0 only allows you to define deprecation on this level). If deprecation should happen on a more fine grained level (i.e. query parameter, payload etc.), the producer should set deprecated=true on the affected method / path element and add further explanation to the description section. If deprecated is set to true, the producer must describe what clients should use instead and when the API will be shut down in the description section of the API definition. MUST: Monitor Usage of Deprecated APIs \u00b6 Owners of APIs used in production must monitor usage of deprecated APIs until the API can be shut down in order to align deprecation and avoid uncontrolled breaking effects. See also the general rule on API usage monitoring SHOULD:: Add a Warning Header to Responses \u00b6 During deprecation phase, the producer should add a Warning header (see RFC 7234 - Warning header) field. When adding the Warning header, the warn-code must be 299 and the warn-text should be in form of \"The path/operation/parameter/... {name} is deprecated and will be removed by {date}. Please see {link} for details.\" with a link to a documentation describing why the API is no longer supported in the current form and what clients should do about it. Adding the Warning header is not sufficient to gain client consent to shut down an API. SHOULD:: Add Monitoring for Warning Header \u00b6 Clients should monitor the Warning header in HTTP responses to see if an API will be deprecated in future. MUST: Not Start Using Deprecated APIs \u00b6 Clients must not start using deprecated parts of an API.","title":"Versioning and Extensibility"},{"location":"versioning-and-extensibility/#versioning-and-extensibility","text":"versioning Use API Versioning what is a breaking change breaking changes http://www.jenitennison.com/2009/07/22/versioning-uris.html A lot of things that we want to talk about (make RDF assertions about) are non-information resources. We give them URIs to name them, so that we can talk about them unambiguously, and we give them HTTP URIs so that we have a way of finding information resources (documents) that give us information about them. non-information resource URIs must not include information that is likely to change non-information resource URIs must not include unnecessary hierarchy Always version your API. Versioning helps you iterate faster and prevents invalid requests from hitting updated endpoints. It also helps smooth over any major API version transitions as you can continue to offer old API versions for a period of time. However, the version needs to be in the URL to ensure browser explorability of the resources across versions Well documented and announced multi-month deprecation schedules can be an acceptable practice for many APIs. https://stripe.com/docs/api#versioning Like building an application it is important to keep your API standardized and extensible. It's easy to want to add \"quick fixes\" to make customers happy, but everything you do should be carefully thought out in order to make sure your API continues to serve not only your needs, but also your clients. Remember, when you create an API you are creating a contract- your users are depending on your API not just to make their application work, but in order to make a living and feed their families. When you break things, or when you break backwards compatibility you are taking their time and resources to fix their application instead of adding features and keeping their customers happy. The industry solution to this has been versioning, however, versioning is merely a Band-Aid, a temporary solution to make migration to the new system less stressful for your clients, but increasingly difficult on you. Keep in mind, when you have multiple versions of your API you end up supporting and maintaining those versions. One of the greatest challenges in regards to versioning is getting developers to migrate from one version to another, all while keeping your support staff from going insane. This doesn't mean that you shouldn't plan for versioning. Rather it means that you should plan to incorporate a version identifier (either in the URI or in the content-header of the response), but work under the mindset that you will only version your API if... You have backwards incompatible PLATFORM changes \u2013 in other words you completely change the UI or way your platform works You find that your API is no longer extendable \u2013 which is exactly what we are trying to avoid here You find that your spec no longer meets your developer's needs \u2013 for example, they are demanding REST instead of SOAP You should NOT version your API just because: You added additional endpoints You added additional data in the response You changed technologies \u2013 your API should be decoupled from your technology stack You changed your applications services or code \u2013 your API should be decoupled from your service layer To clarify on the last two, it shouldn't matter what technologies you are using, or how your services work. The API should interact with both, but be decoupled enough that changing something in the background does not effect the API adversely. Any changes you make to your API in regards to the technology or service layer should be as seamless and transparent as possible. After all, the less you can break backwards compatibility, the happier you, and your customers will be. And this can only happen if you go into building your API with a long-term, flexibility, and extensibility focus.","title":"Versioning and Extensibility"},{"location":"versioning-and-extensibility/#extensibility-and-versioning","text":"Managing change in any distributed client/server environment can be hard. In these environments, clients count on servers to honor their contracts. When a change is backward compatible, you need not upgrade clients at the same time as you modify the server. Forward compatibility may be important when you have several clients and servers upgraded at different points in time. In this case, some newer clients may be interacting with older servers. The purpose of forward compatibility is to ensure that newer clients can continue to use the older servers without disruption albeit with reduced functionality. The characteristic that lets you maintain compatibility is extensibility . Extensibility is a design process to account for future changes. As a transfer protocol, HTTP is extensible, but that does not mean that APIs built over HTTP are automatically extensible. It takes discipline, careful planning, and defensive coding practices. A one-time simultaneous upgrade of all server and clients is not a realistic task. You need to plan for a gradual rollout of upgrades to servers and clients to maintain the availability of the overall system. Note that both clients and server need to take the appropriate steps to operate smoothly under change. For the server, the goal is to keep the clients from breaking. For the clients, the objective is not fail when new unknown data or links appear in representations.","title":"Extensibility and Versioning"},{"location":"versioning-and-extensibility/#how-to-maintain-uri-compatibility-url-regression","text":"Keeps URIs permanent. Treat URIs containing the same query parameters but in a different order as the same When you add new parameters to URIs, continue to honor existing parameters, and treat new parameters as optional. When changing data formats for query parameters, continue to honor existing formats. If that is not viable, introduce format changes via new query parameters or new URIs. Treat query parameters in URIs as optional except when need for concurrency and security. DO use rewrite rules on the server to shield clients from implementation-level changes. DO use 301 Moved Permanently with the new URI in the Location header, when URIs must change to honor old URIs. DO monitor request traffic for redirection. DO maintain redirection services until you are confident the majority of clients have updated their stored links to point to the new URI. DO communicate an appropriate end-of-life policy for old URIs, when you cannot monitor the old URIs. DO convert 301 Moved Permanently to 410 Gone or 404 Not Found once the traffic has fallen of or the preset time interval has passed.","title":"How to Maintain URI Compatibility (url regression)"},{"location":"versioning-and-extensibility/#how-to-maintain-compatibility-of-xml-and-json-representations","text":"When making changes to JSON preserve the hierarchical structure so that clients can continue to follow the same structure to extract data. Make new data elements in requests optional to maintain compatibility with existing clients. Clients that do not send new data fields must be able to continue to function. Do not remove or rename any data fields from representaions in reponse bodies. Example: Clients that do not understand new fields may not store them locally. If this causes the server to assume the user has no email address, introduce a new version of the resource that contains the email.","title":"How to Maintain Compatibility of XML and JSON Representations"},{"location":"versioning-and-extensibility/#how-to-maintain-compatibility-of-links","text":"Avoid removing links Do not change the value of the rel and href attributes of links. When introducing new resources, use links to provide URIs of those resources to clients.","title":"How to Maintain Compatibility of Links"},{"location":"versioning-and-extensibility/#when-to-version","text":"Consider versioning when the server cannot maintain compatibility. Also consider versioning if some clients require behavior or functionality different from other clients. Versioning may introduce new problems: Data stored by the clients for one version may not automatically work with the data from a different version. Clients may have to port resource data stored locally before migrating to the new version. Version changes may involve new business rules and new application flow, which requires code changes in clients. Maintaining multiple versions of resources at the same time is not trivial. When you use links to convey URIs to clients, clients may store them locally. When you assign new URIs, clients will have to upgrade those URIs along with other stored data of resources.","title":"When to Version"},{"location":"versioning-and-extensibility/#how-to-version-restful-web-services","text":"Add new resources with new URIs when there is a change in the behavior of resource or a change in the information contained in representations. Use easily detectable patterns such as v1 or v2 in subdomain names, path segments, or query parameters to distinguish URIs by their version. Avoid treating each version as a new representation with a new media type of the same resource. Versioning involves versioning resources with new URIs. This is because HTTP dictates everything except URIs of resources and their representations. Although you can add custom HTTP methods and headers, such additions may impair interoperability. Avoid introducing new media types for each version since it leads to media type proliferation, which reduce interoperability.","title":"How to Version RESTful Web Services"},{"location":"versioning-and-extensibility/#compatibility","text":"","title":"Compatibility"},{"location":"versioning-and-extensibility/#must-dont-break-backward-compatibility","text":"Change APIs, but keep all consumers running. Consumers usually have independent release life-cycles, focus on stability, and avoid changes that do not provide additional value. APIs are contracts between service providers and service consumers that cannot be broken via unilateral decisions. There are two techniques to change APIs without breaking them: follow rules for compatible extensions introduce new API versions and still support older versions We strongly encourage using compatible API extensions and discourage versioning. The below guideline rules for service providers and consumers enable us (having Postel's Law in mind) to make compatible changes without versioning. Hint: Please note that the compatibility guarantees are for the \"on the wire\" format. Binary or source compatibility of code generated from an API definition is not covered by these rules. If client implementations update their generation process to a new version of the API definition, it has to be expected that code changes are necessary.","title":"MUST: Don't Break Backward Compatibility"},{"location":"versioning-and-extensibility/#should-prefer-compatible-extensions","text":"API designers should apply the following rules to evolve RESTful APIs for services in a backward-compatible way: Add only optional, never mandatory fields. Never change the semantic of fields (e.g. changing the semantic from customer-number to customer-id, as both are different unique customer keys) Input fields may have (complex) constraints being validated via server-side business logic. - - Never change the validation logic to be more restrictive and make sure that all constraints a clearly defined in description. Enum ranges can be reduced when used as input parameters, only if the server is ready to accept and handle old range values too. Enum range can be reduced when used as output parameters. Enum ranges cannot not be extended when used for output parameters \u2014 clients may not be prepared to handle it. However, enum ranges can be extended when used for input parameters. Support redirection in case an URL has to change (301 Moved Permanently).","title":"SHOULD:: Prefer Compatible Extensions"},{"location":"versioning-and-extensibility/#must-prepare-clients-to-not-crash-on-compatible-api-extensions","text":"Service clients should apply the robustness principle: Be conservative with API requests and data passed as input, e.g. avoid to exploit definition eficits like passing megabytes for strings with unspecified maximum length. Be tolerant in processing and reading data of API responses, more specifically... Service clients must be prepared for compatible API extensions of service providers: Be tolerant with unknown fields in the payload (see also Fowler's \"TolerantReader\" post), i.e. ignore new fields but do not eliminate them from payload if needed for subsequent PUT requests. Be prepared to handle HTTP status codes not explicitly specified in endpoint definitions. Note also, that status codes are extensible. Default handling is how you would treat the corresponding x00 code (see RFC7231 Section 6). Follow the redirect when the server returns HTTP status 301 Moved Permanently.","title":"MUST: Prepare Clients To Not Crash On Compatible API Extensions"},{"location":"versioning-and-extensibility/#should-design-apis-conservatively","text":"Designers of service provider APIs should be conservative and accurate in what they accept from clients: Unknown input fields in payload or URL should not be ignored; servers should provide error feedback to clients via an HTTP 400 response code. Be accurate in defining input data constraints (like formats, ranges, lengths etc.) \u2014 and check constraints and return dedicated error information in case of violations. Prefer being more specific and restrictive (if compliant to functional requirements), e.g. by defining length range of strings. It may simplify implementation while providing freedom for further evolution as compatible extensions. Not ignoring unknown input fields is a specific deviation from Postel's Law (e.g. see also The Robustness Principle Reconsidered) and a strong recommendation. Servers might want to take different approach but should be aware of the following problems and be explicit in what is supported: Ignoring unknown input fields is actually not an option for PUT, since it becomes asymmetric with subsequent GET response and HTTP is clear about the PUT \"replace\" semantics and default round-trip expectations (see RFC7231 Section 4.3.4). Note, accepting (i.e. not ignoring) unknown input fields and returning it in subsequent GET responses is a different situation and compliant to PUT semantics. Certain client errors cannot be recognized by servers, e.g. attribute name typing errors will be ignored without server error feedback. The server cannot differentiate between the client intentionally providing an additional field versus the client sending a mistakenly named field, when the client's actual intent was to provide an optional input field. Future extensions of the input data structure might be in conflict with already ignored fields and, hence, will not be compatible, i.e. break clients that already use this field but with different type. In specific situations, where a (known) input field is not needed anymore, it either can stay in the API definition with \"not used anymore\" description or can be removed from the API definition as long as the server ignores this specific parameter.","title":"SHOULD:: Design APIs Conservatively"},{"location":"versioning-and-extensibility/#must-always-return-json-objects-as-top-level-data-structures-to-support-extensibility","text":"In a response body, you must always return a JSON objects (and not e.g. an array) as a top level data structure to support future extensibility. JSON objects support compatible extension by additional attributes. This allows you to easily extend your response and e.g. add pagination later, without breaking backwards compatibility.","title":"MUST: Always Return JSON Objects As Top-Level Data Structures To Support Extensibility"},{"location":"versioning-and-extensibility/#should-avoid-versioning","text":"When changing your RESTful APIs, do so in a compatible way and avoid generating additional API versions. Multiple versions can significantly complicate understanding, testing, maintaining, evolving, operating and releasing our systems (supplementary reading). If changing an API can't be done in a compatible way, then proceed in one of these three ways: create a new resource (variant) in addition to the old resource variant create a new service endpoint \u2014 i.e. a new application with a new API (with a new domain name) create a new API version supported in parallel with the old API by the same microservice As we discourage versioning by all means because of the manifold disadvantages, we suggest to only use the first two approaches.","title":"SHOULD:: Avoid Versioning"},{"location":"versioning-and-extensibility/#must-use-media-type-versioning","text":"When API versioning is unavoidable, you have to design your multi-version RESTful APIs using media type versioning (instead of URI versioning, see below). Media type versioning is less tightly coupled since it supports content negotiation and hence reduces complexity of release management. Media type versioning: Here, version information and media type are provided together via the HTTP Content-Type header \u2014 e.g. application/x.zalando.cart+json;version=2. For incompatible changes, a new media type version for the resource is created. To generate the new representation version, consumer and producer can do content negotiation using the HTTP Content-Type and Accept headers. Note: This versioning only applies to the request and response content schema, not to URI or method semantics. In this example, a client wants only the new version of the response: Accept: application/x.zalando.cart+json;version=2 A server responding to this, as well as a client sending a request with content should use the Content-Type header, declaring that one is sending the new version: Content-Type: application/x.zalando.cart+json;version=2 Using header versioning should: include versions in request and response headers to increase visibility include Content-Type in the Vary header to enable proxy caches to differ between versions Hint: OpenAPI currently doesn't support content negotiation, though a comment in this issue mentions a workaround (using a fragment identifier that gets stripped off). Another way would be to document just the new version, but let the server accept the old one (with the previous content-type). Until an incompatible change is necessary, it is recommended to stay with the standard application/json media type.","title":"MUST: Use Media Type Versioning"},{"location":"versioning-and-extensibility/#must-do-not-use-uri-versioning","text":"With URI versioning a (major) version number is included in the path, e.g. /v1/customers. The consumer has to wait until the provider has been released and deployed. If the consumer also supports hypermedia links \u2014 even in their APIs \u2014 to drive workflows (HATEOAS), this quickly becomes complex. So does coordinating version upgrades \u2014 especially with hyperlinked service dependencies \u2014 when using URL versioning. To avoid this tighter coupling and complexer release management we do not use URI versioning, and go instead with media type versioning and content negotiation (see above). The API will inevitably need to be modified in ways that will impact customer client code. Not all customer clients will be able to make the necessary changes in their code right away, so the API should maintain older versions for some time after releasing the new version, usually a minimum of 90 days. In the \u2018When to Version\u2019 section should you mention APIs that use enum types? My company\u2019s API reference documentation has attributes with an enum type and we list all values. We ran into a problem when a requirement added an enum value to an existing attribute and it broke a partner app. The partner had coded to the defined enum values in the documentation and their app wasn\u2019t expecting new values with future releases. I corrected the problem by removing the new enum value and I defined any request to add a new enum value to an existing attribute requires a new API version. Last, going forward new attributes with an enum type will be defined as String in the documentation and we will indicate the values are not limited to the ones defined. Versioning Content Types The use of content negotiation with custom MIME types allows for finer grained versioning at the resource level without the need to create a plethora of new endpoints. New versions must be communicated to developers through existing channels \u2013 email, developer blogs, etc. When a content version is no longer supported, the body of the HTTP error should include a list of supported content types. Versioning URIs The use of both hyper links and content negotiation should all but eliminate the need to version at the URI level. However, there may be instances where the entire structure of the API must be changed, particularly when moving from one API style to another, such when moving from an RPC-type style to NARWHL. To prepare for these possibilities, it\u2019s recommended that a version be embedded within each API endpoint. The version can either be embedded at the root for all endpoints of a given API, such as: http://api.example.com/v1 Or within the fully qualified domain name for the endpoint: http://apiv1.example.com The version need not be numeric, nor specified using the \u201cv[x]\u201d syntax. Alternatives include dates, project names, seasons or other identifiers that are meaningful enough to the team producing the APIs and flexible enough to change as the versions change. Subbu Allamaraju, revisits one of the recurring debates in the REST community; the standard media types vs. custom media types and tries to determine the best practices when using them. He starts with the stating dichotomous views on the use of media types. Opinion 1: Web services must use standard media types to be RESTful. Opinion 2: Custom media types are necessary to keep interactions visible, and to serve as contracts. The first opinion which, if adhered to strictly, per Roy Fieldings thesis, \u201cthe use of media types such as application/vnd.example.myway+xml is not RESTful\u201d. Subbu believes that understanding the impact of such media type usage in the real world is more important than following the thesis to the letter. There are however comments that suggest that this interpretation of the thesis might be up for debate as well. To the contrary, the second opinion, he says, leads to visibility of the messages at the protocol level via the use of custom media types. [\u2026] For instance, how can anyone know if a representation that uses application/xml media type describes a purchase order, or a photo album? If the web service uses media types likeapplication/vnd.example.po and application/vnd.example.album, then any one can interpret the semantics of the representation without parsing the body of the representation. Per this line of thinking, a media type is an identifier for message semantics, and message recipients use media types to trigger processing code. \u201cSo what is the right thing to do?\u201d He asks, as he puts forth his idea, in a effort to democratically determine the best practices If the sender is formatting representations using standard extensible formats such as XML or JSON, use standard media types such as application/xml and application/json. Mint new media types when you invent new formats. If you are just looking for a way to communicate application level semantics for XML and JSON messages, use something else (e.g. XML namespaces and conventions). If the goal is versioning, use version identifiers in URIs. Giving java-like examples, He asserts that though its possible to peek into the messages to see how a request can be processed, it compromises visibility or opacity as the case may be. Media types such as application/xml and application/json are good enough for XML and JSON message processing in code. [\u2026] URI based approaches are guaranteed to work across the stack. Ignoring real-world interoperability for the sake of \"architectual purity\" or \"RESTful contracts\" may eventually back fire. Via the post is the solution presented by Subbu found the right balance between architectural purity and interoperable real-world solutions? Be sure to visit the original post to weigh in your opinion.","title":"MUST: Do Not Use URI Versioning"},{"location":"versioning-and-extensibility/#tips-for-versioning","text":"Versioning is one of the most important considerations when designing your Web API. Never release an API without a version and make the version mandatory. Let's see how three top API providers handle versioning. Twilio /2010-04-01/Accounts/ salesforce.com /services/data/v20.0/sobjects/Account Facebook ?v=1.0 Twilio uses a timestamp in the URL (note the European format). At compilation time, the developer includes the timestamp of the application when the code was compiled. That timestamp goes in all the HTTP requests. When a request arrives, Twilio does a look up. Based on the timestamp they identify the API that was valid when this code was created and route accordingly. It's a very clever and interesting approach, although we think it is a bit complex. For example, it can be confusing to understand whether the timestamp is the compilation time or the timestamp when the API was released. Salesforce.com uses v20.0, placed somewhere in the middle of the URL. We like the use of the v. notation. However, we don't like using the .0 because it implies that the interface might be changing more frequently than it should. The logic behind an interface can change rapidly but the interface itself shouldn't change frequently. Facebook also uses the v. notation but makes the version an optional parameter. This is problematic because as soon as Facebook forced the API up to the next version, all the apps that didn't include the version number broke and had to be pulled back and version number added.","title":"Tips for versioning"},{"location":"versioning-and-extensibility/#how-to-think-about-version-numbers-in-a-pragmatic-way-with-rest","text":"Never release an API without a version. Make the version mandatory. Specify the version with a 'v' prefix. Move it all the way to the left in the URL so that it has the highest scope (e.g. /v1/dogs). Use a simple ordinal number. Don't use the dot notation like v1.2 because it implies a granularity of versioning that doesn't work well with APIs--it's an interface not an implementation. Stick with v1, v2, and so on. How many versions should you maintain? Maintain at least one version back. For how long should you maintain a version? Give developers at least one cycle to react before obsoleting a version. Sometimes that's 6 months; sometimes it's 2 years. It depends on your developers' development platform, application type, and application users. For example, mobile apps take longer to rev' than Web apps. Should version and format be in URLs or headers? There is a strong school of thought about putting format and version in the header. Sometimes people are forced to put the version in the header because they have multiple inter-dependent APIs. That is often a symptom of a bigger problem, namely, they are usually exposing their internal mess instead of creating one, usable API facade on top. That's not to say that putting the version in the header is a symptom of a problematic API design. It's not! In fact, using headers is more correct for many reasons: it leverages existing HTTP standards, it's intellectually consistent with Fielding's vision, it solves some hard realworld problems related to inter-dependent APIs, and more. However, we think the reason most of the popular APIs do not use it is because it's less fun to hack in a browser. Simple rules we follow: If it changes the logic you write to handle the response, put it in the URL so you can see it easily. If it doesn't change the logic for each response, like OAuth information, put it in the header. The code we would write to handle the responses would be very different. There's no question the header is more correct and it is still a very strong API design.","title":"How to think about version numbers in a pragmatic way with REST?"},{"location":"versioning-and-extensibility/#deprecation","text":"Sometimes it is necessary to phase out an API endpoint (or version). I.e. this may be necessary if a field is no longer supported in the result or a whole business functionality behind an endpoint has to be shut down. There are many other reasons as well.","title":"Deprecation"},{"location":"versioning-and-extensibility/#must-obtain-approval-of-clients","text":"Before shutting down an API (or version of an API) the producer must make sure, that all clients have given their consent to shut down the endpoint. Producers should help consumers to migrate to a potential new endpoint (i.e. by providing a migration manual). After all clients are migrated, the producer may shut down the deprecated API.","title":"MUST: Obtain Approval of Clients"},{"location":"versioning-and-extensibility/#must-external-partners-must-agree-on-deprecation-timespan","text":"If the API is consumed by any external partner, the producer must define a reasonable timespan that the API will be maintained after the producer has announced deprecation. The external partner (client) must agree to this minimum after-deprecation-lifespan before he starts using the API.","title":"MUST: External Partners Must Agree on Deprecation Timespan"},{"location":"versioning-and-extensibility/#must-reflect-deprecation-in-api-definition","text":"API deprecation must be part of the OpenAPI definition. If a method on a path, a whole path or even a whole API endpoint (multiple paths) should be deprecated, the producers must set deprecated=true on each method / path element that will be deprecated (OpenAPI 2.0 only allows you to define deprecation on this level). If deprecation should happen on a more fine grained level (i.e. query parameter, payload etc.), the producer should set deprecated=true on the affected method / path element and add further explanation to the description section. If deprecated is set to true, the producer must describe what clients should use instead and when the API will be shut down in the description section of the API definition.","title":"MUST: Reflect Deprecation in API Definition"},{"location":"versioning-and-extensibility/#must-monitor-usage-of-deprecated-apis","text":"Owners of APIs used in production must monitor usage of deprecated APIs until the API can be shut down in order to align deprecation and avoid uncontrolled breaking effects. See also the general rule on API usage monitoring","title":"MUST: Monitor Usage of Deprecated APIs"},{"location":"versioning-and-extensibility/#should-add-a-warning-header-to-responses","text":"During deprecation phase, the producer should add a Warning header (see RFC 7234 - Warning header) field. When adding the Warning header, the warn-code must be 299 and the warn-text should be in form of \"The path/operation/parameter/... {name} is deprecated and will be removed by {date}. Please see {link} for details.\" with a link to a documentation describing why the API is no longer supported in the current form and what clients should do about it. Adding the Warning header is not sufficient to gain client consent to shut down an API.","title":"SHOULD:: Add a Warning Header to Responses"},{"location":"versioning-and-extensibility/#should-add-monitoring-for-warning-header","text":"Clients should monitor the Warning header in HTTP responses to see if an API will be deprecated in future.","title":"SHOULD:: Add Monitoring for Warning Header"},{"location":"versioning-and-extensibility/#must-not-start-using-deprecated-apis","text":"Clients must not start using deprecated parts of an API.","title":"MUST: Not Start Using Deprecated APIs"}]}